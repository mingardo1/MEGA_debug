<!DOCTYPE html>
    <html lang="en">
              <head>
                <meta charset="utf-8">
                <title>278</title>
                    <style>
                        #top {
                            height: 48vh;
                            overflow-y: auto;
                        }
                        #bottom {
                            height: 48vh;
                            overflow-y: auto;
                        }
                        abbr {
                          /* Here is the delay */
                          transition-delay:0s;
                        }
                    </style>
              </head>
              <body>
                <span style="height: 4vh">
                    278
                    <a href="277.html">prev</a>
                    <a href="279.html">next</a>
                    <a href="278_chunks.html">chunks</a>
                    <a href="index.html">index</a>
                    DTStack/flinkStreamSQL_fe7880a68ed3272ba6192b7dd47bceadb83b633d_hbase/hbase-sink/src/main/java/com/dtstack/flink/sql/sink/hbase/HbaseOutputFormat.java
                    <textarea rows=1 onclick='navigator.clipboard.writeText(this.value)'>cd C:\studies\se\mega\git-analyzer-plus\notebooks\debug
del /Q *
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;fe7880a68ed3272ba6192b7dd47bceadb83b633d:hbase/hbase-sink/src/main/java/com/dtstack/flink/sql/sink/hbase/HbaseOutputFormat.java&quot; &gt; committed.java
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;fe7880a68ed3272ba6192b7dd47bceadb83b633d^1:hbase/hbase-sink/src/main/java/com/dtstack/flink/sql/sink/hbase/HbaseOutputFormat.java&quot; &gt; ours.java
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;fe7880a68ed3272ba6192b7dd47bceadb83b633d^2:hbase/hbase-sink/src/main/java/com/dtstack/flink/sql/sink/hbase/HbaseOutputFormat.java&quot; &gt; theirs.java
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;b97ef2c376f3f65f38136ac572cc33ea3e4cddbf:hbase/hbase-sink/src/main/java/com/dtstack/flink/sql/sink/hbase/HbaseOutputFormat.java&quot; &gt; base.java
copy ours.java 1ours.java
copy ours.java 2ours.java
copy theirs.java 1theirs.java
copy theirs.java 2theirs.java
copy base.java 1base.java
copy base.java 2base.java
&quot;C:\Program Files\Java\jdk1.8.0_241\bin\java.exe&quot; -Dfile.encoding=UTF-8 -jar &quot;C:\studies\se\jFSTMerge\build\libs\jFSTMerge-all.jar&quot; C:\studies\se\mega\git-analyzer-plus\notebooks\debug\1ours.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\1base.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\1theirs.java -o C:\studies\se\mega\git-analyzer-plus\notebooks\debug\jfstmerge.java --show-base
&quot;C:\Program Files\Eclipse Adoptium\jdk-17.0.11.9-hotspot\bin\java.exe&quot; -Dfile.encoding=UTF-8 -jar &quot;C:\studies\se\spork\target\spork-0.5.0-SNAPSHOT.jar&quot; C:\studies\se\mega\git-analyzer-plus\notebooks\debug\2ours.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\2base.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\2theirs.java -o C:\studies\se\mega\git-analyzer-plus\notebooks\debug\spork.java
del /Q 1*.java
del /Q 2*.java
del /Q jfstmerge.java.merge
</textarea>
                    {strict: [[b], [bj], [b], [j]], subset: [[b], [bj], [b], [j]]}
                </span>
                <div id="top">

                    <table>
                        <tr>
                            <th>line based (standard git)</th>
                            <th>jfstmerge</th>
                            <th>spork</th>
                        </tr>
                        <tr>
                            <td><pre>   1 /*
   2  * Licensed to the Apache Software Foundation (ASF) under one
   3  * or more contributor license agreements.  See the NOTICE file
   4  * distributed with this work for additional information
   5  * regarding copyright ownership.  The ASF licenses this file
   6  * to you under the Apache License, Version 2.0 (the
   7  * &quot;License&quot;); you may not use this file except in compliance
   8  * with the License.  You may obtain a copy of the License at
   9  *
  10  *     http://www.apache.org/licenses/LICENSE-2.0
  11  *
  12  * Unless required by applicable law or agreed to in writing, software
  13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15  * See the License for the specific language governing permissions and
  16  * limitations under the License.
  17  */
  18 
  19 
  20 package com.dtstack.flink.sql.sink.hbase;
  21 
  22 &lt;&lt;&lt;&lt;&lt;&lt;&lt; GitAnalyzerPlus_ours
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  23 import com.dtstack.flink.sql.dirtyManager.manager.DirtyDataManager;</span>
  24 ||||||| GitAnalyzerPlus_base
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  25 import com.dtstack.flink.sql.factory.DTThreadFactory;</span>
  26 =======
  27 &gt;&gt;&gt;&gt;&gt;&gt;&gt; GitAnalyzerPlus_theirs
  28 import com.dtstack.flink.sql.exception.ExceptionTrace;
  29 import com.dtstack.flink.sql.factory.DTThreadFactory;
  30 import com.dtstack.flink.sql.outputformat.AbstractDtRichOutputFormat;
  31 import com.google.common.collect.Maps;
  32 import org.apache.commons.lang3.StringUtils;
  33 import org.apache.flink.api.java.tuple.Tuple2;
  34 import org.apache.flink.configuration.Configuration;
  35 import org.apache.flink.types.Row;
  36 import org.apache.flink.util.Preconditions;
  37 import org.apache.hadoop.hbase.AuthUtil;
  38 import org.apache.hadoop.hbase.ChoreService;
  39 import org.apache.hadoop.hbase.HBaseConfiguration;
  40 import org.apache.hadoop.hbase.ScheduledChore;
  41 import org.apache.hadoop.hbase.TableName;
  42 import org.apache.hadoop.hbase.client.Connection;
  43 import org.apache.hadoop.hbase.client.ConnectionFactory;
  44 import org.apache.hadoop.hbase.client.Put;
  45 import org.apache.hadoop.hbase.client.Table;
  46 import org.apache.hadoop.security.UserGroupInformation;
  47 import org.slf4j.Logger;
  48 import org.slf4j.LoggerFactory;
  49 
  50 import java.io.File;
  51 import java.io.IOException;
  52 import java.security.PrivilegedAction;
  53 import java.util.ArrayList;
  54 import java.util.LinkedList;
  55 import java.util.List;
  56 import java.util.Map;
  57 import java.util.concurrent.ScheduledExecutorService;
  58 import java.util.concurrent.ScheduledFuture;
  59 import java.util.concurrent.ScheduledThreadPoolExecutor;
  60 import java.util.concurrent.TimeUnit;
  61 
  62 /**
  63  * @author: jingzhen@dtstack.com
  64  * date: 2017-6-29
  65  */
  66 public class HbaseOutputFormat extends AbstractDtRichOutputFormat&lt;Tuple2&lt;Boolean, Row&gt;&gt; {
  67 
  68     private static final Logger LOG = LoggerFactory.getLogger(HbaseOutputFormat.class);
  69     private String host;
  70     private String zkParent;
  71     private String rowkey;
  72     private String tableName;
  73     private String[] columnNames;
  74     private Map&lt;String, String&gt; columnNameFamily;
  75     private boolean kerberosAuthEnable;
  76     private String regionserverKeytabFile;
  77     private String regionserverPrincipal;
  78     private String securityKrb5Conf;
  79     private String zookeeperSaslClient;
  80     private String clientPrincipal;
  81     private String clientKeytabFile;
  82     private String[] families;
  83     private String[] qualifiers;
  84     private transient org.apache.hadoop.conf.Configuration conf;
  85     private transient Connection conn;
  86     private transient Table table;
  87     private transient ChoreService choreService;
  88     private transient List&lt;Row&gt; records;
  89     private transient volatile boolean closed = false;
  90     /**
  91      * 批量写入的参数
  92      */
  93     private Integer batchSize;
  94     private Long batchWaitInterval;
  95     /**
  96      * 定时任务
  97      */
  98     private transient ScheduledExecutorService scheduler;
  99     private transient ScheduledFuture&lt;?&gt; scheduledFuture;
 100 
 101     /**
 102      * 脏数据管理
 103      */
 104     private DirtyDataManager dirtyDataManager;
 105 
 106     private HbaseOutputFormat() {
 107     }
 108 
 109     public static HbaseOutputFormatBuilder buildHbaseOutputFormat() {
 110         return new HbaseOutputFormatBuilder();
 111     }
 112 
 113     @Override
 114     public void configure(Configuration parameters) {
 115         // 这里不要做耗时较长的操作，否则会导致AKKA通信超时
 116         // DO NOTHING
 117     }
 118 
 119     @Override
 120     public void open(int taskNumber, int numTasks) throws IOException {
 121         LOG.warn(&quot;---open---&quot;);
 122         records = new ArrayList&lt;&gt;();
 123         conf = HBaseConfiguration.create();
 124         openConn();
 125         table = conn.getTable(TableName.valueOf(tableName));
 126         LOG.warn(&quot;---open end(get table from hbase) ---&quot;);
 127         initMetric();
 128     }
 129 
 130     private void openConn() {
 131         try {
 132             if (kerberosAuthEnable) {
 133                 LOG.info(&quot;open kerberos conn&quot;);
 134                 openKerberosConn();
 135             } else {
 136                 LOG.info(&quot;open conn&quot;);
 137                 conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 138                 conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 139                 conn = ConnectionFactory.createConnection(conf);
 140             }
 141         } catch (Exception e) {
 142             throw new RuntimeException(e);
 143         }
 144         initScheduledTask(batchWaitInterval);
 145     }
 146 
 147     /**
 148      * 初始化定时写入任务
 149      *
 150      * @param batchWaitInterval 定时任务时间
 151      */
 152     private void initScheduledTask(Long batchWaitInterval) {
 153         try {
 154             if (batchWaitInterval &gt; 0) {
 155                 this.scheduler = new ScheduledThreadPoolExecutor(
 156                         1,
 157                         new DTThreadFactory(&quot;hbase-batch-flusher&quot;)
 158                 );
 159 
 160                 this.scheduledFuture = this.scheduler.scheduleWithFixedDelay(
 161                         () -&gt; {
 162                             synchronized (HbaseOutputFormat.this) {
 163                                 if (!records.isEmpty()) {
 164                                     dealBatchOperation(records);
 165                                 }
 166                             }
 167                         }, batchWaitInterval, batchWaitInterval, TimeUnit.MILLISECONDS
 168                 );
 169             }
 170         } catch (Exception e) {
 171             LOG.error(&quot;init schedule task failed !&quot;);
 172             throw new RuntimeException(e);
 173         }
 174     }
 175 
 176     private void openKerberosConn() throws Exception {
 177         conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 178         conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 179 
 180         LOG.info(&quot;kerberos config:{}&quot;, this.conf.toString());
 181         Preconditions.checkArgument(!StringUtils.isEmpty(clientPrincipal), &quot; clientPrincipal not null!&quot;);
<abbr title=" 182         Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;);"> 182         Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;🔵</abbr>
 183 
 184         fillSyncKerberosConfig(conf, regionserverPrincipal, zookeeperSaslClient, securityKrb5Conf);
 185 
 186         clientKeytabFile = System.getProperty(&quot;user.dir&quot;) + File.separator + clientKeytabFile;
<abbr title=" 187         clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal;"> 187         clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal🔵</abbr>
 188 
 189         conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KEYTAB_FILE, clientKeytabFile);
 190         conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KERBEROS_PRINCIPAL, clientPrincipal);
 191 
<abbr title=" 192         UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clientKeytabFile);"> 192         UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrinci🔵</abbr>
 193         org.apache.hadoop.conf.Configuration finalConf = conf;
 194         conn = userGroupInformation.doAs((PrivilegedAction&lt;Connection&gt;) () -&gt; {
 195             try {
 196                 ScheduledChore authChore = AuthUtil.getAuthChore(finalConf);
 197                 if (authChore != null) {
 198                     choreService = new ChoreService(&quot;hbaseKerberosSink&quot;);
 199                     choreService.scheduleChore(authChore);
 200                 }
 201 
 202                 return ConnectionFactory.createConnection(finalConf);
 203             } catch (IOException e) {
 204                 LOG.error(&quot;Get connection fail with config:{}&quot;, finalConf);
 205                 throw new RuntimeException(e);
 206             }
 207         });
 208     }
 209 
 210     @Override
 211     public void writeRecord(Tuple2&lt;Boolean, Row&gt; record) {
 212         if (record.f0) {
 213             if (this.batchSize &gt; 1) {
 214                 writeBatchRecord(record.f1);
 215             } else {
 216                 dealInsert(record.f1);
 217             }
 218         }
 219     }
 220 
 221     public void writeBatchRecord(Row row) {
 222         records.add(row);
 223         // 数据累计到batchSize之后开始处理
 224         if (records.size() == this.batchSize) {
 225             dealBatchOperation(records);
 226         }
 227     }
 228 
 229     protected synchronized void dealBatchOperation(List&lt;Row&gt; records) {
 230         // A null in the result array means that the call for that action failed, even after retries.
 231         Object[] results = new Object[records.size()];
 232         try {
 233             List&lt;Put&gt; puts = new ArrayList&lt;&gt;();
 234             for (Row record : records) {
 235                 puts.add(getPutByRow(record));
 236             }
 237             table.batch(puts, results);
 238 
 239             // 打印结果
 240             if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {
 241                 // 只打印最后一条数据
 242                 LOG.info(records.get(records.size() - 1).toString());
 243             }
 244         } catch (IOException | InterruptedException e) {
 245             // ignore exception
 246         } finally {
 247             // 判断数据是否插入成功
 248             for (int i = 0; i &lt; results.length; i++) {
 249                 if (results[i] instanceof Exception) {
 250 &lt;&lt;&lt;&lt;&lt;&lt;&lt; GitAnalyzerPlus_ours
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0"> 251                     dirtyDataManager.execute();</span>
 252 ||||||| GitAnalyzerPlus_base
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 253                         LOG.error(&quot;Get dirty data: {}&quot;, records.get(i).toString());</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 254                         LOG.error(&quot;Error cause: &quot; + results[i]);</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 255                     }</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 256                     // 脏数据记录</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 257                     outDirtyRecords.inc();</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 258                 } else {</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 259                     // 输出结果条数记录</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 260                     outRecords.inc();</span>
 261 =======
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 262                     if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 263                         LOG.error(&quot;Get dirty data: {}&quot;, records.get(i).toString());</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"><abbr title=" 264                         LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause((Exception) results[i]));"> 264                         LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause((Exception) results🔵</abbr></span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 265                     }</span>
 266 &gt;&gt;&gt;&gt;&gt;&gt;&gt; GitAnalyzerPlus_theirs
 267                     // 脏数据记录
 268                     dirtyDataManager.collectDirtyData(
 269                         records.get(i).toString(),
 270                         ExceptionTrace.traceOriginalCause((Exception) results[i])
 271                     );
 272                     outDirtyRecords.inc();
 273                 } else {
 274                     // 输出结果条数记录
 275                     outRecords.inc();
 276                 }
 277             }
 278             // 添加完数据之后数据清空records
 279             records.clear();
 280         }
 281     }
 282 
 283     protected void dealInsert(Row record) {
 284         Put put = getPutByRow(record);
 285         if (put == null || put.isEmpty()) {
 286             // 记录脏数据
 287             outDirtyRecords.inc();
 288             return;
 289         }
 290 
 291         try {
 292             table.put(put);
 293 &lt;&lt;&lt;&lt;&lt;&lt;&lt; GitAnalyzerPlus_ours
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0"> 294             if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0"> 295                 LOG.info(record.toString());</span>
 296 ||||||| GitAnalyzerPlus_base
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 297             if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0 || LOG.isDebugEnabled()) {</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 298                 LOG.error(&quot;record insert failed ..{}&quot;, record.toString());</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 299                 LOG.error(&quot;&quot;, e);</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 300             }</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 301             outDirtyRecords.inc();</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 302         }</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 303 </span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 304         if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 305             LOG.info(record.toString());</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 306         }</span>
 307 =======
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 308         } catch (Exception e) {</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 309             if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0 || LOG.isDebugEnabled()) {</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 310                 LOG.error(&quot;Get dirty data: {}&quot;, record.toString());</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 311                 LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause(e));</span>
 312 &gt;&gt;&gt;&gt;&gt;&gt;&gt; GitAnalyzerPlus_theirs
 313             }
 314         } catch (Exception e) {
 315             dirtyDataManager.collectDirtyData(
 316                 record.toString(),
 317                 ExceptionTrace.traceOriginalCause(e));
 318             outDirtyRecords.inc();
 319         }
 320 
 321         outRecords.inc();
 322     }
 323 
 324     private Put getPutByRow(Row record) {
 325         String rowKey = buildRowKey(record);
 326         if (StringUtils.isEmpty(rowKey)) {
 327             return null;
 328         }
 329         Put put = new Put(rowKey.getBytes());
 330         for (int i = 0; i &lt; record.getArity(); ++i) {
 331             Object fieldVal = record.getField(i);
 332             if (fieldVal != null) {
 333                 byte[] val = fieldVal.toString().getBytes();
 334                 byte[] cf = families[i].getBytes();
 335                 byte[] qualifier = qualifiers[i].getBytes();
 336 
 337                 put.addColumn(cf, qualifier, val);
 338             }
 339         }
 340         return put;
 341     }
 342 
 343     private String buildRowKey(Row record) {
 344         String rowKeyValues = getRowKeyValues(record);
 345         // all rowkey not null
 346         if (StringUtils.isBlank(rowKeyValues)) {
 347             LOG.error(&quot;row key value must not null,record is ..{}&quot;, record);
 348             outDirtyRecords.inc();
 349             return &quot;&quot;;
 350         }
 351         return rowKeyValues;
 352     }
 353 
 354     private String getRowKeyValues(Row record) {
 355         Map&lt;String, Object&gt; row = rowConvertMap(record);
 356         RowKeyBuilder rowKeyBuilder = new RowKeyBuilder();
 357         rowKeyBuilder.init(rowkey);
 358         return rowKeyBuilder.getRowKey(row);
 359     }
 360 
 361     private Map&lt;String, Object&gt; rowConvertMap(Row record) {
 362         Map&lt;String, Object&gt; rowValue = Maps.newHashMap();
 363         for (int i = 0; i &lt; columnNames.length; i++) {
 364             rowValue.put(columnNames[i], record.getField(i));
 365         }
 366         return rowValue;
 367     }
 368 
 369     @Override
 370     public synchronized void close() throws IOException {
 371         if (closed) {
 372             return;
 373         }
 374 
 375         closed = true;
 376         if (!records.isEmpty()) {
 377             dealBatchOperation(records);
 378         }
 379 
 380         if (scheduledFuture != null) {
 381             scheduledFuture.cancel(false);
 382             if (scheduler != null) {
 383                 scheduler.shutdownNow();
 384             }
 385         }
 386 
 387         if (conn != null) {
 388             conn.close();
 389             conn = null;
 390         }
 391 
 392         if (dirtyDataManager != null) {
 393             dirtyDataManager.close();
 394         }
 395     }
 396 
 397     private void fillSyncKerberosConfig(org.apache.hadoop.conf.Configuration config,
 398                                         String regionserverPrincipal,
 399                                         String zookeeperSaslClient,
 400                                         String securityKrb5Conf) {
 401         if (StringUtils.isEmpty(regionserverPrincipal)) {
<abbr title=" 402             throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos&quot;);"> 402             throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is🔵</abbr>
 403         }
 404         config.set(HbaseConfigUtils.KEY_HBASE_MASTER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 405         config.set(HbaseConfigUtils.KEY_HBASE_REGIONSERVER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 406         config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHORIZATION, &quot;true&quot;);
 407         config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHENTICATION, &quot;kerberos&quot;);
 408 
 409 
 410         if (!StringUtils.isEmpty(zookeeperSaslClient)) {
 411             System.setProperty(HbaseConfigUtils.KEY_ZOOKEEPER_SASL_CLIENT, zookeeperSaslClient);
 412         }
 413 
 414         if (!StringUtils.isEmpty(securityKrb5Conf)) {
 415             String krb5ConfPath = System.getProperty(&quot;user.dir&quot;) + File.separator + securityKrb5Conf;
 416             LOG.info(&quot;krb5ConfPath:{}&quot;, krb5ConfPath);
 417             System.setProperty(HbaseConfigUtils.KEY_JAVA_SECURITY_KRB5_CONF, krb5ConfPath);
 418         }
 419     }
 420 
 421     @Override
 422     public String toString() {
 423         return &quot;HbaseOutputFormat kerberos{&quot; +
 424                 &quot;kerberosAuthEnable=&quot; + kerberosAuthEnable +
 425                 &quot;, regionserverKeytabFile=&#x27;&quot; + regionserverKeytabFile + &#x27;\&#x27;&#x27; +
 426                 &quot;, regionserverPrincipal=&#x27;&quot; + regionserverPrincipal + &#x27;\&#x27;&#x27; +
 427                 &quot;, securityKrb5Conf=&#x27;&quot; + securityKrb5Conf + &#x27;\&#x27;&#x27; +
 428                 &quot;, zookeeperSaslClient=&#x27;&quot; + zookeeperSaslClient + &#x27;\&#x27;&#x27; +
 429                 &quot;, clientPrincipal=&#x27;&quot; + clientPrincipal + &#x27;\&#x27;&#x27; +
 430                 &quot;, clientKeytabFile=&#x27;&quot; + clientKeytabFile + &#x27;\&#x27;&#x27; +
 431                 &quot;, batchSize=&#x27;&quot; + batchSize + &#x27;\&#x27;&#x27; +
 432                 &quot;, batchWaitInterval=&#x27;&quot; + batchWaitInterval + &#x27;\&#x27;&#x27; +
 433                 &#x27;}&#x27;;
 434     }
 435 
 436     public static class HbaseOutputFormatBuilder {
 437 
 438         private final HbaseOutputFormat format;
 439 
 440         private HbaseOutputFormatBuilder() {
 441             format = new HbaseOutputFormat();
 442         }
 443 
 444         public HbaseOutputFormatBuilder setHost(String host) {
 445             format.host = host;
 446             return this;
 447         }
 448 
 449         public HbaseOutputFormatBuilder setZkParent(String parent) {
 450             format.zkParent = parent;
 451             return this;
 452         }
 453 
 454 
 455         public HbaseOutputFormatBuilder setTable(String tableName) {
 456             format.tableName = tableName;
 457             return this;
 458         }
 459 
 460         public HbaseOutputFormatBuilder setRowkey(String rowkey) {
 461             format.rowkey = rowkey;
 462             return this;
 463         }
 464 
 465         public HbaseOutputFormatBuilder setColumnNames(String[] columnNames) {
 466             format.columnNames = columnNames;
 467             return this;
 468         }
 469 
 470         public HbaseOutputFormatBuilder setColumnNameFamily(Map&lt;String, String&gt; columnNameFamily) {
 471             format.columnNameFamily = columnNameFamily;
 472             return this;
 473         }
 474 
 475         public HbaseOutputFormatBuilder setKerberosAuthEnable(boolean kerberosAuthEnable) {
 476             format.kerberosAuthEnable = kerberosAuthEnable;
 477             return this;
 478         }
 479 
 480         public HbaseOutputFormatBuilder setRegionserverKeytabFile(String regionserverKeytabFile) {
 481             format.regionserverKeytabFile = regionserverKeytabFile;
 482             return this;
 483         }
 484 
 485         public HbaseOutputFormatBuilder setRegionserverPrincipal(String regionserverPrincipal) {
 486             format.regionserverPrincipal = regionserverPrincipal;
 487             return this;
 488         }
 489 
 490         public HbaseOutputFormatBuilder setSecurityKrb5Conf(String securityKrb5Conf) {
 491             format.securityKrb5Conf = securityKrb5Conf;
 492             return this;
 493         }
 494 
 495         public HbaseOutputFormatBuilder setZookeeperSaslClient(String zookeeperSaslClient) {
 496             format.zookeeperSaslClient = zookeeperSaslClient;
 497             return this;
 498         }
 499 
 500         public HbaseOutputFormatBuilder setClientPrincipal(String clientPrincipal) {
 501             format.clientPrincipal = clientPrincipal;
 502             return this;
 503         }
 504 
 505         public HbaseOutputFormatBuilder setClientKeytabFile(String clientKeytabFile) {
 506             format.clientKeytabFile = clientKeytabFile;
 507             return this;
 508         }
 509 
 510         public HbaseOutputFormatBuilder setDirtyManager(DirtyDataManager dirtyDataManager) {
 511             format.dirtyDataManager = dirtyDataManager;
 512             return this;
 513         }
 514 
 515         public HbaseOutputFormatBuilder setBatchSize(Integer batchSize) {
 516             format.batchSize = batchSize;
 517             return this;
 518         }
 519 
 520         public HbaseOutputFormatBuilder setBatchWaitInterval(Long batchWaitInterval) {
 521             format.batchWaitInterval = batchWaitInterval;
 522             return this;
 523         }
 524 
 525         public HbaseOutputFormat finish() {
 526             Preconditions.checkNotNull(format.host, &quot;zookeeperQuorum should be specified&quot;);
 527             Preconditions.checkNotNull(format.tableName, &quot;tableName should be specified&quot;);
 528             Preconditions.checkNotNull(format.columnNames, &quot;columnNames should be specified&quot;);
<abbr title=" 529             Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be zero&quot;);"> 529             Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be🔵</abbr>
 530 
 531             String[] families = new String[format.columnNames.length];
 532             String[] qualifiers = new String[format.columnNames.length];
 533 
 534             if (format.columnNameFamily != null) {
 535                 List&lt;String&gt; keyList = new LinkedList&lt;&gt;(format.columnNameFamily.keySet());
 536                 String[] columns = keyList.toArray(new String[0]);
 537                 for (int i = 0; i &lt; columns.length; ++i) {
 538                     String col = columns[i];
 539                     String[] part = col.split(&quot;:&quot;);
 540                     families[i] = part[0];
 541                     qualifiers[i] = part[1];
 542                 }
 543             }
 544             format.families = families;
 545             format.qualifiers = qualifiers;
 546 
 547             return format;
 548         }
 549     }
 550 }</pre></td>
                            <td><pre>   1 /*
   2  * Licensed to the Apache Software Foundation (ASF) under one
   3  * or more contributor license agreements.  See the NOTICE file
   4  * distributed with this work for additional information
   5  * regarding copyright ownership.  The ASF licenses this file
   6  * to you under the Apache License, Version 2.0 (the
   7  * &quot;License&quot;); you may not use this file except in compliance
   8  * with the License.  You may obtain a copy of the License at
   9  *
  10  *     http://www.apache.org/licenses/LICENSE-2.0
  11  *
  12  * Unless required by applicable law or agreed to in writing, software
  13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15  * See the License for the specific language governing permissions and
  16  * limitations under the License.
  17  */
  18 
  19 
  20 package com.dtstack.flink.sql.sink.hbase;
  21 
  22 import com.dtstack.flink.sql.dirtyManager.manager.DirtyDataManager;
  23 import com.dtstack.flink.sql.exception.ExceptionTrace;
  24 import com.dtstack.flink.sql.factory.DTThreadFactory;
  25 import com.dtstack.flink.sql.outputformat.AbstractDtRichOutputFormat;
  26 import com.google.common.collect.Maps;
  27 import org.apache.commons.lang3.StringUtils;
  28 import org.apache.flink.api.java.tuple.Tuple2;
  29 import org.apache.flink.configuration.Configuration;
  30 import org.apache.flink.types.Row;
  31 import org.apache.flink.util.Preconditions;
  32 import org.apache.hadoop.hbase.AuthUtil;
  33 import org.apache.hadoop.hbase.ChoreService;
  34 import org.apache.hadoop.hbase.HBaseConfiguration;
  35 import org.apache.hadoop.hbase.ScheduledChore;
  36 import org.apache.hadoop.hbase.TableName;
  37 import org.apache.hadoop.hbase.client.Connection;
  38 import org.apache.hadoop.hbase.client.ConnectionFactory;
  39 import org.apache.hadoop.hbase.client.Put;
  40 import org.apache.hadoop.hbase.client.Table;
  41 import org.apache.hadoop.security.UserGroupInformation;
  42 import org.slf4j.Logger;
  43 import org.slf4j.LoggerFactory;
  44 
  45 import java.io.File;
  46 import java.io.IOException;
  47 import java.security.PrivilegedAction;
  48 import java.util.ArrayList;
  49 import java.util.LinkedList;
  50 import java.util.List;
  51 import java.util.Map;
  52 import java.util.concurrent.ScheduledExecutorService;
  53 import java.util.concurrent.ScheduledFuture;
  54 import java.util.concurrent.ScheduledThreadPoolExecutor;
  55 import java.util.concurrent.TimeUnit;
  56 
  57 /**
  58  * @author: jingzhen@dtstack.com
  59  * date: 2017-6-29
  60  */
  61 public class HbaseOutputFormat extends AbstractDtRichOutputFormat&lt;Tuple2&lt;Boolean, Row&gt;&gt; {
  62 
  63     private static final Logger LOG = LoggerFactory.getLogger(HbaseOutputFormat.class);
  64     private String host;
  65     private String zkParent;
  66     private String rowkey;
  67     private String tableName;
  68     private String[] columnNames;
  69     private Map&lt;String, String&gt; columnNameFamily;
  70     private boolean kerberosAuthEnable;
  71     private String regionserverKeytabFile;
  72     private String regionserverPrincipal;
  73     private String securityKrb5Conf;
  74     private String zookeeperSaslClient;
  75     private String clientPrincipal;
  76     private String clientKeytabFile;
  77     private String[] families;
  78     private String[] qualifiers;
  79     private transient org.apache.hadoop.conf.Configuration conf;
  80     private transient Connection conn;
  81     private transient Table table;
  82     private transient ChoreService choreService;
  83     private transient List&lt;Row&gt; records;
  84     private transient volatile boolean closed = false;
  85     /**
  86      * 批量写入的参数
  87      */
  88     private Integer batchSize;
  89     private Long batchWaitInterval;
  90     /**
  91      * 定时任务
  92      */
  93     private transient ScheduledExecutorService scheduler;
  94     private transient ScheduledFuture&lt;?&gt; scheduledFuture;
  95 
  96     /**
  97      * 脏数据管理
  98      */
  99     private DirtyDataManager dirtyDataManager;
 100 
 101     private HbaseOutputFormat() {
 102     }
 103 
 104     public static HbaseOutputFormatBuilder buildHbaseOutputFormat() {
 105         return new HbaseOutputFormatBuilder();
 106     }
 107 
 108     @Override
 109     public void configure(Configuration parameters) {
 110         // 这里不要做耗时较长的操作，否则会导致AKKA通信超时
 111         // DO NOTHING
 112     }
 113 
 114     @Override
 115     public void open(int taskNumber, int numTasks) throws IOException {
 116         LOG.warn(&quot;---open---&quot;);
 117         records = new ArrayList&lt;&gt;();
 118         conf = HBaseConfiguration.create();
 119         openConn();
 120         table = conn.getTable(TableName.valueOf(tableName));
 121         LOG.warn(&quot;---open end(get table from hbase) ---&quot;);
 122         initMetric();
 123     }
 124 
 125     private void openConn() {
 126         try {
 127             if (kerberosAuthEnable) {
 128                 LOG.info(&quot;open kerberos conn&quot;);
 129                 openKerberosConn();
 130             } else {
 131                 LOG.info(&quot;open conn&quot;);
 132                 conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 133                 conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 134                 conn = ConnectionFactory.createConnection(conf);
 135             }
 136         } catch (Exception e) {
 137             throw new RuntimeException(e);
 138         }
 139         initScheduledTask(batchWaitInterval);
 140     }
 141 
 142     /**
 143      * 初始化定时写入任务
 144      *
 145      * @param batchWaitInterval 定时任务时间
 146      */
 147     private void initScheduledTask(Long batchWaitInterval) {
 148         try {
 149             if (batchWaitInterval &gt; 0) {
 150                 this.scheduler = new ScheduledThreadPoolExecutor(
 151                         1,
 152                         new DTThreadFactory(&quot;hbase-batch-flusher&quot;)
 153                 );
 154 
 155                 this.scheduledFuture = this.scheduler.scheduleWithFixedDelay(
 156                         () -&gt; {
 157                             synchronized (HbaseOutputFormat.this) {
 158                                 if (!records.isEmpty()) {
 159                                     dealBatchOperation(records);
 160                                 }
 161                             }
 162                         }, batchWaitInterval, batchWaitInterval, TimeUnit.MILLISECONDS
 163                 );
 164             }
 165         } catch (Exception e) {
 166             LOG.error(&quot;init schedule task failed !&quot;);
 167             throw new RuntimeException(e);
 168         }
 169     }
 170 
 171     private void openKerberosConn() throws Exception {
 172         conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 173         conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 174 
 175         LOG.info(&quot;kerberos config:{}&quot;, this.conf.toString());
 176         Preconditions.checkArgument(!StringUtils.isEmpty(clientPrincipal), &quot; clientPrincipal not null!&quot;);
<abbr title=" 177         Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;);"> 177         Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;🔵</abbr>
 178 
 179         fillSyncKerberosConfig(conf, regionserverPrincipal, zookeeperSaslClient, securityKrb5Conf);
 180 
 181         clientKeytabFile = System.getProperty(&quot;user.dir&quot;) + File.separator + clientKeytabFile;
<abbr title=" 182         clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal;"> 182         clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal🔵</abbr>
 183 
 184         conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KEYTAB_FILE, clientKeytabFile);
 185         conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KERBEROS_PRINCIPAL, clientPrincipal);
 186 
<abbr title=" 187         UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clientKeytabFile);"> 187         UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrinci🔵</abbr>
 188         org.apache.hadoop.conf.Configuration finalConf = conf;
 189         conn = userGroupInformation.doAs((PrivilegedAction&lt;Connection&gt;) () -&gt; {
 190             try {
 191                 ScheduledChore authChore = AuthUtil.getAuthChore(finalConf);
 192                 if (authChore != null) {
 193                     choreService = new ChoreService(&quot;hbaseKerberosSink&quot;);
 194                     choreService.scheduleChore(authChore);
 195                 }
 196 
 197                 return ConnectionFactory.createConnection(finalConf);
 198             } catch (IOException e) {
 199                 LOG.error(&quot;Get connection fail with config:{}&quot;, finalConf);
 200                 throw new RuntimeException(e);
 201             }
 202         });
 203     }
 204 
 205     @Override
 206     public void writeRecord(Tuple2&lt;Boolean, Row&gt; record) {
 207         if (record.f0) {
 208             if (this.batchSize &gt; 1) {
 209                 writeBatchRecord(record.f1);
 210             } else {
 211                 dealInsert(record.f1);
 212             }
 213         }
 214     }
 215 
 216     public void writeBatchRecord(Row row) {
 217         records.add(row);
 218         // 数据累计到batchSize之后开始处理
 219         if (records.size() == this.batchSize) {
 220             dealBatchOperation(records);
 221         }
 222     }
 223 
 224     protected synchronized void dealBatchOperation(List&lt;Row&gt; records) {
 225         // A null in the result array means that the call for that action failed, even after retries.
 226         Object[] results = new Object[records.size()];
 227         try {
 228             List&lt;Put&gt; puts = new ArrayList&lt;&gt;();
 229             for (Row record : records) {
 230                 puts.add(getPutByRow(record));
 231             }
 232             table.batch(puts, results);
 233 
 234             // 打印结果
 235             if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {
 236                 // 只打印最后一条数据
 237                 LOG.info(records.get(records.size() - 1).toString());
 238             }
 239         } catch (IOException | InterruptedException e) {
 240             // ignore exception
 241         } finally {
 242             // 判断数据是否插入成功
 243             for (int i = 0; i &lt; results.length; i++) {
 244                 if (results[i] instanceof Exception) {
 245 &lt;&lt;&lt;&lt;&lt;&lt;&lt; MINE
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0"> 246                     dirtyDataManager.execute();</span>
 247 ||||||| BASE
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 248                     if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 249                         LOG.error(&quot;Get dirty data: {}&quot;, records.get(i).toString());</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 250                         LOG.error(&quot;Error cause: &quot; + results[i]);</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 251                     }</span>
 252 =======
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 253                     if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 254                         LOG.error(&quot;Get dirty data: {}&quot;, records.get(i).toString());</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"><abbr title=" 255                         LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause((Exception) results[i]));"> 255                         LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause((Exception) results🔵</abbr></span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 256                     }</span>
 257 &gt;&gt;&gt;&gt;&gt;&gt;&gt; YOURS
 258                     // 脏数据记录
 259                     dirtyDataManager.collectDirtyData(
 260                         records.get(i).toString(),
 261                         ExceptionTrace.traceOriginalCause((Exception) results[i])
 262                     );
 263                     outDirtyRecords.inc();
 264                 } else {
 265                     // 输出结果条数记录
 266                     outRecords.inc();
 267                 }
 268             }
 269             // 添加完数据之后数据清空records
 270             records.clear();
 271         }
 272     }
 273 
 274     protected void dealInsert(Row record) {
 275         Put put = getPutByRow(record);
 276         if (put == null || put.isEmpty()) {
 277             // 记录脏数据
 278             outDirtyRecords.inc();
 279             return;
 280         }
 281 
 282         try {
 283             table.put(put);
 284 &lt;&lt;&lt;&lt;&lt;&lt;&lt; MINE
 285 ||||||| BASE
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 286         } catch (Exception e) {</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 287             if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0 || LOG.isDebugEnabled()) {</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 288                 LOG.error(&quot;record insert failed ..{}&quot;, record.toString());</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 289                 LOG.error(&quot;&quot;, e);</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 290             }</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 291             outDirtyRecords.inc();</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 292         }</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 293 </span>
 294 =======
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 295         } catch (Exception e) {</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 296             if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0 || LOG.isDebugEnabled()) {</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 297                 LOG.error(&quot;Get dirty data: {}&quot;, record.toString());</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 298                 LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause(e));</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 299             }</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 300             outDirtyRecords.inc();</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 301         }</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 302 </span>
 303 &gt;&gt;&gt;&gt;&gt;&gt;&gt; YOURS
 304         if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {
 305             LOG.info(record.toString());
 306         }
 307         } catch (Exception e) {
 308             dirtyDataManager.collectDirtyData(
 309                 record.toString(),
 310                 ExceptionTrace.traceOriginalCause(e));
 311             outDirtyRecords.inc();
 312         }
 313 
 314         outRecords.inc();
 315     }
 316 
 317     private Put getPutByRow(Row record) {
 318         String rowKey = buildRowKey(record);
 319         if (StringUtils.isEmpty(rowKey)) {
 320             return null;
 321         }
 322         Put put = new Put(rowKey.getBytes());
 323         for (int i = 0; i &lt; record.getArity(); ++i) {
 324             Object fieldVal = record.getField(i);
 325             if (fieldVal != null) {
 326                 byte[] val = fieldVal.toString().getBytes();
 327                 byte[] cf = families[i].getBytes();
 328                 byte[] qualifier = qualifiers[i].getBytes();
 329 
 330                 put.addColumn(cf, qualifier, val);
 331             }
 332         }
 333         return put;
 334     }
 335 
 336     private String buildRowKey(Row record) {
 337         String rowKeyValues = getRowKeyValues(record);
 338         // all rowkey not null
 339         if (StringUtils.isBlank(rowKeyValues)) {
 340             LOG.error(&quot;row key value must not null,record is ..{}&quot;, record);
 341             outDirtyRecords.inc();
 342             return &quot;&quot;;
 343         }
 344         return rowKeyValues;
 345     }
 346 
 347     private String getRowKeyValues(Row record) {
 348         Map&lt;String, Object&gt; row = rowConvertMap(record);
 349         RowKeyBuilder rowKeyBuilder = new RowKeyBuilder();
 350         rowKeyBuilder.init(rowkey);
 351         return rowKeyBuilder.getRowKey(row);
 352     }
 353 
 354     private Map&lt;String, Object&gt; rowConvertMap(Row record) {
 355         Map&lt;String, Object&gt; rowValue = Maps.newHashMap();
 356         for (int i = 0; i &lt; columnNames.length; i++) {
 357             rowValue.put(columnNames[i], record.getField(i));
 358         }
 359         return rowValue;
 360     }
 361 
 362     @Override
 363     public synchronized void close() throws IOException {
 364         if (closed) {
 365             return;
 366         }
 367 
 368         closed = true;
 369         if (!records.isEmpty()) {
 370             dealBatchOperation(records);
 371         }
 372 
 373         if (scheduledFuture != null) {
 374             scheduledFuture.cancel(false);
 375             if (scheduler != null) {
 376                 scheduler.shutdownNow();
 377             }
 378         }
 379 
 380         if (conn != null) {
 381             conn.close();
 382             conn = null;
 383         }
 384 
 385         if (dirtyDataManager != null) {
 386             dirtyDataManager.close();
 387         }
 388     }
 389 
 390     private void fillSyncKerberosConfig(org.apache.hadoop.conf.Configuration config,
 391                                         String regionserverPrincipal,
 392                                         String zookeeperSaslClient,
 393                                         String securityKrb5Conf) {
 394         if (StringUtils.isEmpty(regionserverPrincipal)) {
<abbr title=" 395             throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos&quot;);"> 395             throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is🔵</abbr>
 396         }
 397         config.set(HbaseConfigUtils.KEY_HBASE_MASTER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 398         config.set(HbaseConfigUtils.KEY_HBASE_REGIONSERVER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 399         config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHORIZATION, &quot;true&quot;);
 400         config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHENTICATION, &quot;kerberos&quot;);
 401 
 402 
 403         if (!StringUtils.isEmpty(zookeeperSaslClient)) {
 404             System.setProperty(HbaseConfigUtils.KEY_ZOOKEEPER_SASL_CLIENT, zookeeperSaslClient);
 405         }
 406 
 407         if (!StringUtils.isEmpty(securityKrb5Conf)) {
 408             String krb5ConfPath = System.getProperty(&quot;user.dir&quot;) + File.separator + securityKrb5Conf;
 409             LOG.info(&quot;krb5ConfPath:{}&quot;, krb5ConfPath);
 410             System.setProperty(HbaseConfigUtils.KEY_JAVA_SECURITY_KRB5_CONF, krb5ConfPath);
 411         }
 412     }
 413 
 414     @Override
 415     public String toString() {
 416         return &quot;HbaseOutputFormat kerberos{&quot; +
 417                 &quot;kerberosAuthEnable=&quot; + kerberosAuthEnable +
 418                 &quot;, regionserverKeytabFile=&#x27;&quot; + regionserverKeytabFile + &#x27;\&#x27;&#x27; +
 419                 &quot;, regionserverPrincipal=&#x27;&quot; + regionserverPrincipal + &#x27;\&#x27;&#x27; +
 420                 &quot;, securityKrb5Conf=&#x27;&quot; + securityKrb5Conf + &#x27;\&#x27;&#x27; +
 421                 &quot;, zookeeperSaslClient=&#x27;&quot; + zookeeperSaslClient + &#x27;\&#x27;&#x27; +
 422                 &quot;, clientPrincipal=&#x27;&quot; + clientPrincipal + &#x27;\&#x27;&#x27; +
 423                 &quot;, clientKeytabFile=&#x27;&quot; + clientKeytabFile + &#x27;\&#x27;&#x27; +
 424                 &quot;, batchSize=&#x27;&quot; + batchSize + &#x27;\&#x27;&#x27; +
 425                 &quot;, batchWaitInterval=&#x27;&quot; + batchWaitInterval + &#x27;\&#x27;&#x27; +
 426                 &#x27;}&#x27;;
 427     }
 428 
 429     public static class HbaseOutputFormatBuilder {
 430 
 431         private final HbaseOutputFormat format;
 432 
 433         private HbaseOutputFormatBuilder() {
 434             format = new HbaseOutputFormat();
 435         }
 436 
 437         public HbaseOutputFormatBuilder setHost(String host) {
 438             format.host = host;
 439             return this;
 440         }
 441 
 442         public HbaseOutputFormatBuilder setZkParent(String parent) {
 443             format.zkParent = parent;
 444             return this;
 445         }
 446 
 447 
 448         public HbaseOutputFormatBuilder setTable(String tableName) {
 449             format.tableName = tableName;
 450             return this;
 451         }
 452 
 453         public HbaseOutputFormatBuilder setRowkey(String rowkey) {
 454             format.rowkey = rowkey;
 455             return this;
 456         }
 457 
 458         public HbaseOutputFormatBuilder setColumnNames(String[] columnNames) {
 459             format.columnNames = columnNames;
 460             return this;
 461         }
 462 
 463         public HbaseOutputFormatBuilder setColumnNameFamily(Map&lt;String, String&gt; columnNameFamily) {
 464             format.columnNameFamily = columnNameFamily;
 465             return this;
 466         }
 467 
 468         public HbaseOutputFormatBuilder setKerberosAuthEnable(boolean kerberosAuthEnable) {
 469             format.kerberosAuthEnable = kerberosAuthEnable;
 470             return this;
 471         }
 472 
 473         public HbaseOutputFormatBuilder setRegionserverKeytabFile(String regionserverKeytabFile) {
 474             format.regionserverKeytabFile = regionserverKeytabFile;
 475             return this;
 476         }
 477 
 478         public HbaseOutputFormatBuilder setRegionserverPrincipal(String regionserverPrincipal) {
 479             format.regionserverPrincipal = regionserverPrincipal;
 480             return this;
 481         }
 482 
 483         public HbaseOutputFormatBuilder setSecurityKrb5Conf(String securityKrb5Conf) {
 484             format.securityKrb5Conf = securityKrb5Conf;
 485             return this;
 486         }
 487 
 488         public HbaseOutputFormatBuilder setZookeeperSaslClient(String zookeeperSaslClient) {
 489             format.zookeeperSaslClient = zookeeperSaslClient;
 490             return this;
 491         }
 492 
 493         public HbaseOutputFormatBuilder setClientPrincipal(String clientPrincipal) {
 494             format.clientPrincipal = clientPrincipal;
 495             return this;
 496         }
 497 
 498         public HbaseOutputFormatBuilder setClientKeytabFile(String clientKeytabFile) {
 499             format.clientKeytabFile = clientKeytabFile;
 500             return this;
 501         }
 502 
 503         public HbaseOutputFormatBuilder setDirtyManager(DirtyDataManager dirtyDataManager) {
 504             format.dirtyDataManager = dirtyDataManager;
 505             return this;
 506         }
 507 
 508         public HbaseOutputFormatBuilder setBatchSize(Integer batchSize) {
 509             format.batchSize = batchSize;
 510             return this;
 511         }
 512 
 513         public HbaseOutputFormatBuilder setBatchWaitInterval(Long batchWaitInterval) {
 514             format.batchWaitInterval = batchWaitInterval;
 515             return this;
 516         }
 517 
 518         public HbaseOutputFormat finish() {
 519             Preconditions.checkNotNull(format.host, &quot;zookeeperQuorum should be specified&quot;);
 520             Preconditions.checkNotNull(format.tableName, &quot;tableName should be specified&quot;);
 521             Preconditions.checkNotNull(format.columnNames, &quot;columnNames should be specified&quot;);
<abbr title=" 522             Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be zero&quot;);"> 522             Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be🔵</abbr>
 523 
 524             String[] families = new String[format.columnNames.length];
 525             String[] qualifiers = new String[format.columnNames.length];
 526 
 527             if (format.columnNameFamily != null) {
 528                 List&lt;String&gt; keyList = new LinkedList&lt;&gt;(format.columnNameFamily.keySet());
 529                 String[] columns = keyList.toArray(new String[0]);
 530                 for (int i = 0; i &lt; columns.length; ++i) {
 531                     String col = columns[i];
 532                     String[] part = col.split(&quot;:&quot;);
 533                     families[i] = part[0];
 534                     qualifiers[i] = part[1];
 535                 }
 536             }
 537             format.families = families;
 538             format.qualifiers = qualifiers;
 539 
 540             return format;
 541         }
 542     }
 543 }
 
 
 
 
 
 </pre></td>
                            <td><pre>   1 /*
   2  * Licensed to the Apache Software Foundation (ASF) under one
   3  * or more contributor license agreements.  See the NOTICE file
   4  * distributed with this work for additional information
   5  * regarding copyright ownership.  The ASF licenses this file
   6  * to you under the Apache License, Version 2.0 (the
   7  * &quot;License&quot;); you may not use this file except in compliance
   8  * with the License.  You may obtain a copy of the License at
   9  *
  10  *     http://www.apache.org/licenses/LICENSE-2.0
  11  *
  12  * Unless required by applicable law or agreed to in writing, software
  13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15  * See the License for the specific language governing permissions and
  16  * limitations under the License.
  17  */
  18 package com.dtstack.flink.sql.sink.hbase;
  19 
  20 import com.dtstack.flink.sql.dirtyManager.manager.DirtyDataManager;
  21 import com.dtstack.flink.sql.exception.ExceptionTrace;
  22 import com.dtstack.flink.sql.factory.DTThreadFactory;
  23 import com.dtstack.flink.sql.outputformat.AbstractDtRichOutputFormat;
  24 import com.google.common.collect.Maps;
  25 import java.io.File;
  26 import java.io.IOException;
  27 import java.security.PrivilegedAction;
  28 import java.util.ArrayList;
  29 import java.util.LinkedList;
  30 import java.util.List;
  31 import java.util.Map;
  32 import java.util.concurrent.ScheduledExecutorService;
  33 import java.util.concurrent.ScheduledFuture;
  34 import java.util.concurrent.ScheduledThreadPoolExecutor;
  35 import java.util.concurrent.TimeUnit;
  36 import org.apache.commons.lang3.StringUtils;
  37 import org.apache.flink.api.java.tuple.Tuple2;
  38 import org.apache.flink.configuration.Configuration;
  39 import org.apache.flink.types.Row;
  40 import org.apache.flink.util.Preconditions;
  41 import org.apache.hadoop.hbase.AuthUtil;
  42 import org.apache.hadoop.hbase.ChoreService;
  43 import org.apache.hadoop.hbase.HBaseConfiguration;
  44 import org.apache.hadoop.hbase.ScheduledChore;
  45 import org.apache.hadoop.hbase.TableName;
  46 import org.apache.hadoop.hbase.client.Connection;
  47 import org.apache.hadoop.hbase.client.ConnectionFactory;
  48 import org.apache.hadoop.hbase.client.Put;
  49 import org.apache.hadoop.hbase.client.Table;
  50 import org.apache.hadoop.security.UserGroupInformation;
  51 import org.slf4j.Logger;
  52 import org.slf4j.LoggerFactory;
  53 
  54 
  55 /**
  56  * @author: jingzhen@dtstack.com
  57  * date: 2017-6-29
  58  */
  59 public class HbaseOutputFormat extends AbstractDtRichOutputFormat&lt;Tuple2&lt;Boolean, Row&gt;&gt; {
  60     private static final Logger LOG = LoggerFactory.getLogger(HbaseOutputFormat.class);
  61 
  62     private String host;
  63 
  64     private String zkParent;
  65 
  66     private String rowkey;
  67 
  68     private String tableName;
  69 
  70     private String[] columnNames;
  71 
  72     private Map&lt;String, String&gt; columnNameFamily;
  73 
  74     private boolean kerberosAuthEnable;
  75 
  76     private String regionserverKeytabFile;
  77 
  78     private String regionserverPrincipal;
  79 
  80     private String securityKrb5Conf;
  81 
  82     private String zookeeperSaslClient;
  83 
  84     private String clientPrincipal;
  85 
  86     private String clientKeytabFile;
  87 
  88     private String[] families;
  89 
  90     private String[] qualifiers;
  91 
  92     private transient org.apache.hadoop.conf.Configuration conf;
  93 
  94     private transient Connection conn;
  95 
  96     private transient Table table;
  97 
  98     private transient ChoreService choreService;
  99 
 100     private transient List&lt;Row&gt; records;
 101 
 102     private transient volatile boolean closed = false;
 103 
 104     /**
 105      * 批量写入的参数
 106      */
 107     private Integer batchSize;
 108 
 109     private Long batchWaitInterval;
 110 
 111     /**
 112      * 定时任务
 113      */
 114     private transient ScheduledExecutorService scheduler;
 115 
 116     private transient ScheduledFuture&lt;?&gt; scheduledFuture;
 117 
 118     /**
 119      * 脏数据管理
 120      */
 121     private DirtyDataManager dirtyDataManager;
 122 
 123     private HbaseOutputFormat() {
 124     }
 125 
 126     public static HbaseOutputFormatBuilder buildHbaseOutputFormat() {
 127         return new HbaseOutputFormatBuilder();
 128     }
 129 
 130     @Override
 131     public void configure(Configuration parameters) {
 132         // 这里不要做耗时较长的操作，否则会导致AKKA通信超时
 133         // DO NOTHING
 134     }
 135 
 136     @Override
 137     public void open(int taskNumber, int numTasks) throws IOException {
 138         LOG.warn(&quot;---open---&quot;);
 139         records = new ArrayList&lt;&gt;();
 140         conf = HBaseConfiguration.create();
 141         openConn();
 142         table = conn.getTable(TableName.valueOf(tableName));
 143         LOG.warn(&quot;---open end(get table from hbase) ---&quot;);
 144         initMetric();
 145     }
 146 
 147     private void openConn() {
 148         try {
 149             if (kerberosAuthEnable) {
 150                 LOG.info(&quot;open kerberos conn&quot;);
 151                 openKerberosConn();
 152             } else {
 153                 LOG.info(&quot;open conn&quot;);
 154                 conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 155                 conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 156                 conn = ConnectionFactory.createConnection(conf);
 157             }
 158         } catch (Exception e) {
 159             throw new RuntimeException(e);
 160         }
 161         initScheduledTask(batchWaitInterval);
 162     }
 163 
 164     /**
 165      * 初始化定时写入任务
 166      *
 167      * @param batchWaitInterval 定时任务时间
 168      */
 169     private void initScheduledTask(Long batchWaitInterval) {
 170         try {
 171             if (batchWaitInterval &gt; 0) {
 172                 this.scheduler = new ScheduledThreadPoolExecutor(
 173                         1,
 174                         new DTThreadFactory(&quot;hbase-batch-flusher&quot;)
 175                 );
 176 
 177                 this.scheduledFuture = this.scheduler.scheduleWithFixedDelay(
 178                         () -&gt; {
 179                             synchronized (HbaseOutputFormat.this) {
 180                                 if (!records.isEmpty()) {
 181                                     dealBatchOperation(records);
 182                                 }
 183                             }
 184                         }, batchWaitInterval, batchWaitInterval, TimeUnit.MILLISECONDS
 185                 );
 186             }
 187         } catch (Exception e) {
 188             LOG.error(&quot;init schedule task failed !&quot;);
 189             throw new RuntimeException(e);
 190         }
 191     }
 192 
 193     private void openKerberosConn() throws Exception {
 194         conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 195         conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 196 
 197         LOG.info(&quot;kerberos config:{}&quot;, this.conf.toString());
 198         Preconditions.checkArgument(!StringUtils.isEmpty(clientPrincipal), &quot; clientPrincipal not null!&quot;);
<abbr title=" 199         Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;);"> 199         Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;🔵</abbr>
 200 
 201         fillSyncKerberosConfig(conf, regionserverPrincipal, zookeeperSaslClient, securityKrb5Conf);
 202 
 203         clientKeytabFile = System.getProperty(&quot;user.dir&quot;) + File.separator + clientKeytabFile;
<abbr title=" 204         clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal;"> 204         clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal🔵</abbr>
 205 
 206         conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KEYTAB_FILE, clientKeytabFile);
 207         conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KERBEROS_PRINCIPAL, clientPrincipal);
 208 
<abbr title=" 209         UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clientKeytabFile);"> 209         UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrinci🔵</abbr>
 210         org.apache.hadoop.conf.Configuration finalConf = conf;
 211         conn = userGroupInformation.doAs((PrivilegedAction&lt;Connection&gt;) () -&gt; {
 212             try {
 213                 ScheduledChore authChore = AuthUtil.getAuthChore(finalConf);
 214                 if (authChore != null) {
 215                     choreService = new ChoreService(&quot;hbaseKerberosSink&quot;);
 216                     choreService.scheduleChore(authChore);
 217                 }
 218 
 219                 return ConnectionFactory.createConnection(finalConf);
 220             } catch (IOException e) {
 221                 LOG.error(&quot;Get connection fail with config:{}&quot;, finalConf);
 222                 throw new RuntimeException(e);
 223             }
 224         });
 225     }
 226 
 227     @Override
 228     public void writeRecord(Tuple2&lt;Boolean, Row&gt; record) {
 229         if (record.f0) {
 230             if (this.batchSize &gt; 1) {
 231                 writeBatchRecord(record.f1);
 232             } else {
 233                 dealInsert(record.f1);
 234             }
 235         }
 236     }
 237 
 238     public void writeBatchRecord(Row row) {
 239         records.add(row);
 240         // 数据累计到batchSize之后开始处理
 241         if (records.size() == this.batchSize) {
 242             dealBatchOperation(records);
 243         }
 244     }
 245 
 246     protected synchronized void dealBatchOperation(List&lt;Row&gt; records) {
 247         // A null in the result array means that the call for that action failed, even after retries.
 248         Object[] results = new Object[records.size()];
 249         try {
 250             List&lt;Put&gt; puts = new ArrayList&lt;&gt;();
 251             for (Row record : records) {
 252                 puts.add(getPutByRow(record));
 253             }
 254             table.batch(puts, results);
 255             // 打印结果
 256             if ((outRecords.getCount() % ROW_PRINT_FREQUENCY) == 0) {
 257                 // 只打印最后一条数据
 258                 LOG.info(records.get(records.size() - 1).toString());
 259             }
 260         } catch (IOException | java.lang.InterruptedException e) {
 261             // ignore exception
 262         } finally {
 263             // 判断数据是否插入成功
 264             for (int i = 0; i &lt; results.length; i++) {
 265                 if (results[i] instanceof Exception) {
 266                     dirtyDataManager.execute();
 267                     // 脏数据记录
<abbr title=" 268                     dirtyDataManager.collectDirtyData(records.get(i).toString(), ExceptionTrace.traceOriginalCause(((Exception) (results[i]))));"> 268                     dirtyDataManager.collectDirtyData(records.get(i).toString(), ExceptionTrace.traceOrig🔵</abbr>
 269                     outDirtyRecords.inc();
 270                 } else {
 271                     // 输出结果条数记录
 272                     outRecords.inc();
 273                 }
 274             }
 275             // 添加完数据之后数据清空records
 276             records.clear();
 277         }
 278     }
 279 
 280     protected void dealInsert(Row record) {
 281         Put put = getPutByRow(record);
 282         if ((put == null) || put.isEmpty()) {
 283             // 记录脏数据
 284             outDirtyRecords.inc();
 285             return;
 286         }
 287         try {
 288             table.put(put);
 289             if ((outRecords.getCount() % ROW_PRINT_FREQUENCY) == 0) {
 290                 LOG.info(record.toString());
 291             }
 292         } catch (java.lang.Exception e) {
 293             dirtyDataManager.collectDirtyData(record.toString(), ExceptionTrace.traceOriginalCause(e));
 294             outDirtyRecords.inc();
 295         }
 296         outRecords.inc();
 297     }
 298 
 299     private Put getPutByRow(Row record) {
 300         String rowKey = buildRowKey(record);
 301         if (StringUtils.isEmpty(rowKey)) {
 302             return null;
 303         }
 304         Put put = new Put(rowKey.getBytes());
 305         for (int i = 0; i &lt; record.getArity(); ++i) {
 306             Object fieldVal = record.getField(i);
 307             if (fieldVal != null) {
 308                 byte[] val = fieldVal.toString().getBytes();
 309                 byte[] cf = families[i].getBytes();
 310                 byte[] qualifier = qualifiers[i].getBytes();
 311 
 312                 put.addColumn(cf, qualifier, val);
 313             }
 314         }
 315         return put;
 316     }
 317 
 318     private String buildRowKey(Row record) {
 319         String rowKeyValues = getRowKeyValues(record);
 320         // all rowkey not null
 321         if (StringUtils.isBlank(rowKeyValues)) {
 322             LOG.error(&quot;row key value must not null,record is ..{}&quot;, record);
 323             outDirtyRecords.inc();
 324             return &quot;&quot;;
 325         }
 326         return rowKeyValues;
 327     }
 328 
 329     private String getRowKeyValues(Row record) {
 330         Map&lt;String, Object&gt; row = rowConvertMap(record);
 331         RowKeyBuilder rowKeyBuilder = new RowKeyBuilder();
 332         rowKeyBuilder.init(rowkey);
 333         return rowKeyBuilder.getRowKey(row);
 334     }
 335 
 336     private Map&lt;String, Object&gt; rowConvertMap(Row record) {
 337         Map&lt;String, Object&gt; rowValue = Maps.newHashMap();
 338         for (int i = 0; i &lt; columnNames.length; i++) {
 339             rowValue.put(columnNames[i], record.getField(i));
 340         }
 341         return rowValue;
 342     }
 343 
 344     @Override
 345     public synchronized void close() throws IOException {
 346         if (closed) {
 347             return;
 348         }
 349         closed = true;
 350         if (!records.isEmpty()) {
 351             dealBatchOperation(records);
 352         }
 353         if (scheduledFuture != null) {
 354             scheduledFuture.cancel(false);
 355             if (scheduler != null) {
 356                 scheduler.shutdownNow();
 357             }
 358         }
 359         if (conn != null) {
 360             conn.close();
 361             conn = null;
 362         }
 363         if (dirtyDataManager != null) {
 364             dirtyDataManager.close();
 365         }
 366     }
 367 
 368     private void fillSyncKerberosConfig(org.apache.hadoop.conf.Configuration config,
 369                                         String regionserverPrincipal,
 370                                         String zookeeperSaslClient,
 371                                         String securityKrb5Conf) {
 372         if (StringUtils.isEmpty(regionserverPrincipal)) {
<abbr title=" 373             throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos&quot;);"> 373             throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is🔵</abbr>
 374         }
 375         config.set(HbaseConfigUtils.KEY_HBASE_MASTER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 376         config.set(HbaseConfigUtils.KEY_HBASE_REGIONSERVER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 377         config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHORIZATION, &quot;true&quot;);
 378         config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHENTICATION, &quot;kerberos&quot;);
 379 
 380 
 381         if (!StringUtils.isEmpty(zookeeperSaslClient)) {
 382             System.setProperty(HbaseConfigUtils.KEY_ZOOKEEPER_SASL_CLIENT, zookeeperSaslClient);
 383         }
 384 
 385         if (!StringUtils.isEmpty(securityKrb5Conf)) {
 386             String krb5ConfPath = System.getProperty(&quot;user.dir&quot;) + File.separator + securityKrb5Conf;
 387             LOG.info(&quot;krb5ConfPath:{}&quot;, krb5ConfPath);
 388             System.setProperty(HbaseConfigUtils.KEY_JAVA_SECURITY_KRB5_CONF, krb5ConfPath);
 389         }
 390     }
 391 
 392     @Override
 393     public String toString() {
 394         return &quot;HbaseOutputFormat kerberos{&quot; +
 395                 &quot;kerberosAuthEnable=&quot; + kerberosAuthEnable +
 396                 &quot;, regionserverKeytabFile=&#x27;&quot; + regionserverKeytabFile + &#x27;\&#x27;&#x27; +
 397                 &quot;, regionserverPrincipal=&#x27;&quot; + regionserverPrincipal + &#x27;\&#x27;&#x27; +
 398                 &quot;, securityKrb5Conf=&#x27;&quot; + securityKrb5Conf + &#x27;\&#x27;&#x27; +
 399                 &quot;, zookeeperSaslClient=&#x27;&quot; + zookeeperSaslClient + &#x27;\&#x27;&#x27; +
 400                 &quot;, clientPrincipal=&#x27;&quot; + clientPrincipal + &#x27;\&#x27;&#x27; +
 401                 &quot;, clientKeytabFile=&#x27;&quot; + clientKeytabFile + &#x27;\&#x27;&#x27; +
 402                 &quot;, batchSize=&#x27;&quot; + batchSize + &#x27;\&#x27;&#x27; +
 403                 &quot;, batchWaitInterval=&#x27;&quot; + batchWaitInterval + &#x27;\&#x27;&#x27; +
 404                 &#x27;}&#x27;;
 405     }
 406 
 407     public static class HbaseOutputFormatBuilder {
 408         private final HbaseOutputFormat format;
 409 
 410         private HbaseOutputFormatBuilder() {
 411             format = new HbaseOutputFormat();
 412         }
 413 
 414         public HbaseOutputFormatBuilder setHost(String host) {
 415             format.host = host;
 416             return this;
 417         }
 418 
 419         public HbaseOutputFormatBuilder setZkParent(String parent) {
 420             format.zkParent = parent;
 421             return this;
 422         }
 423 
 424         public HbaseOutputFormatBuilder setTable(String tableName) {
 425             format.tableName = tableName;
 426             return this;
 427         }
 428 
 429         public HbaseOutputFormatBuilder setRowkey(String rowkey) {
 430             format.rowkey = rowkey;
 431             return this;
 432         }
 433 
 434         public HbaseOutputFormatBuilder setColumnNames(String[] columnNames) {
 435             format.columnNames = columnNames;
 436             return this;
 437         }
 438 
 439         public HbaseOutputFormatBuilder setColumnNameFamily(Map&lt;String, String&gt; columnNameFamily) {
 440             format.columnNameFamily = columnNameFamily;
 441             return this;
 442         }
 443 
 444         public HbaseOutputFormatBuilder setKerberosAuthEnable(boolean kerberosAuthEnable) {
 445             format.kerberosAuthEnable = kerberosAuthEnable;
 446             return this;
 447         }
 448 
 449         public HbaseOutputFormatBuilder setRegionserverKeytabFile(String regionserverKeytabFile) {
 450             format.regionserverKeytabFile = regionserverKeytabFile;
 451             return this;
 452         }
 453 
 454         public HbaseOutputFormatBuilder setRegionserverPrincipal(String regionserverPrincipal) {
 455             format.regionserverPrincipal = regionserverPrincipal;
 456             return this;
 457         }
 458 
 459         public HbaseOutputFormatBuilder setSecurityKrb5Conf(String securityKrb5Conf) {
 460             format.securityKrb5Conf = securityKrb5Conf;
 461             return this;
 462         }
 463 
 464         public HbaseOutputFormatBuilder setZookeeperSaslClient(String zookeeperSaslClient) {
 465             format.zookeeperSaslClient = zookeeperSaslClient;
 466             return this;
 467         }
 468 
 469         public HbaseOutputFormatBuilder setClientPrincipal(String clientPrincipal) {
 470             format.clientPrincipal = clientPrincipal;
 471             return this;
 472         }
 473 
 474         public HbaseOutputFormatBuilder setClientKeytabFile(String clientKeytabFile) {
 475             format.clientKeytabFile = clientKeytabFile;
 476             return this;
 477         }
 478 
 479         public HbaseOutputFormatBuilder setDirtyManager(DirtyDataManager dirtyDataManager) {
 480             format.dirtyDataManager = dirtyDataManager;
 481             return this;
 482         }
 483 
 484         public HbaseOutputFormatBuilder setBatchSize(Integer batchSize) {
 485             format.batchSize = batchSize;
 486             return this;
 487         }
 488 
 489         public HbaseOutputFormatBuilder setBatchWaitInterval(Long batchWaitInterval) {
 490             format.batchWaitInterval = batchWaitInterval;
 491             return this;
 492         }
 493 
 494         public HbaseOutputFormat finish() {
 495             Preconditions.checkNotNull(format.host, &quot;zookeeperQuorum should be specified&quot;);
 496             Preconditions.checkNotNull(format.tableName, &quot;tableName should be specified&quot;);
 497             Preconditions.checkNotNull(format.columnNames, &quot;columnNames should be specified&quot;);
<abbr title=" 498             Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be zero&quot;);"> 498             Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be🔵</abbr>
 499 
 500             String[] families = new String[format.columnNames.length];
 501             String[] qualifiers = new String[format.columnNames.length];
 502 
 503             if (format.columnNameFamily != null) {
 504                 List&lt;String&gt; keyList = new LinkedList&lt;&gt;(format.columnNameFamily.keySet());
 505                 String[] columns = keyList.toArray(new String[0]);
 506                 for (int i = 0; i &lt; columns.length; ++i) {
 507                     String col = columns[i];
 508                     String[] part = col.split(&quot;:&quot;);
 509                     families[i] = part[0];
 510                     qualifiers[i] = part[1];
 511                 }
 512             }
 513             format.families = families;
 514             format.qualifiers = qualifiers;
 515 
 516             return format;
 517         }
 518     }
 519 }
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </pre></td>
                        </tr>
                    </table>
                </div>
                <div id="bottom">
                    <table style="margin:auto">
                        <tr>
                            <th>ours vs. base</th>
                            <th>theirs vs. base</th>
                        </tr>
                        <tr>
                            <td><pre>   1  /*
   2   * Licensed to the Apache Software Foundation (ASF) under one
   3   * or more contributor license agreements.  See the NOTICE file
   4   * distributed with this work for additional information
   5   * regarding copyright ownership.  The ASF licenses this file
   6   * to you under the Apache License, Version 2.0 (the
   7   * &quot;License&quot;); you may not use this file except in compliance
   8   * with the License.  You may obtain a copy of the License at
   9   *
  10   *     http://www.apache.org/licenses/LICENSE-2.0
  11   *
  12   * Unless required by applicable law or agreed to in writing, software
  13   * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15   * See the License for the specific language governing permissions and
  16   * limitations under the License.
  17   */
  18  
  19  
  20  package com.dtstack.flink.sql.sink.hbase;
  21  
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  22 +import com.dtstack.flink.sql.dirtyManager.manager.DirtyDataManager;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  23 +import com.dtstack.flink.sql.exception.ExceptionTrace;</span>
  24  import com.dtstack.flink.sql.factory.DTThreadFactory;
  25  import com.dtstack.flink.sql.outputformat.AbstractDtRichOutputFormat;
  26  import com.google.common.collect.Maps;
  27  import org.apache.commons.lang3.StringUtils;
  28  import org.apache.flink.api.java.tuple.Tuple2;
  29  import org.apache.flink.configuration.Configuration;
  30  import org.apache.flink.types.Row;
  31  import org.apache.flink.util.Preconditions;
  32  import org.apache.hadoop.hbase.AuthUtil;
  33  import org.apache.hadoop.hbase.ChoreService;
  34  import org.apache.hadoop.hbase.HBaseConfiguration;
  35  import org.apache.hadoop.hbase.ScheduledChore;
  36  import org.apache.hadoop.hbase.TableName;
  37  import org.apache.hadoop.hbase.client.Connection;
  38  import org.apache.hadoop.hbase.client.ConnectionFactory;
  39  import org.apache.hadoop.hbase.client.Put;
  40  import org.apache.hadoop.hbase.client.Table;
  41  import org.apache.hadoop.security.UserGroupInformation;
  42  import org.slf4j.Logger;
  43  import org.slf4j.LoggerFactory;
  44  
  45  import java.io.File;
  46  import java.io.IOException;
  47  import java.security.PrivilegedAction;
  48  import java.util.ArrayList;
  49  import java.util.LinkedList;
  50  import java.util.List;
  51  import java.util.Map;
  52  import java.util.concurrent.ScheduledExecutorService;
  53  import java.util.concurrent.ScheduledFuture;
  54  import java.util.concurrent.ScheduledThreadPoolExecutor;
  55  import java.util.concurrent.TimeUnit;
  56  
  57  /**
  58   * @author: jingzhen@dtstack.com
  59   * date: 2017-6-29
  60   */
  61  public class HbaseOutputFormat extends AbstractDtRichOutputFormat&lt;Tuple2&lt;Boolean, Row&gt;&gt; {
  62  
  63      private static final Logger LOG = LoggerFactory.getLogger(HbaseOutputFormat.class);
  64      private String host;
  65      private String zkParent;
  66      private String rowkey;
  67      private String tableName;
  68      private String[] columnNames;
  69      private Map&lt;String, String&gt; columnNameFamily;
  70      private boolean kerberosAuthEnable;
  71      private String regionserverKeytabFile;
  72      private String regionserverPrincipal;
  73      private String securityKrb5Conf;
  74      private String zookeeperSaslClient;
  75      private String clientPrincipal;
  76      private String clientKeytabFile;
  77      private String[] families;
  78      private String[] qualifiers;
  79      private transient org.apache.hadoop.conf.Configuration conf;
  80      private transient Connection conn;
  81      private transient Table table;
  82      private transient ChoreService choreService;
  83      private transient List&lt;Row&gt; records;
  84      private transient volatile boolean closed = false;
  85      /**
  86       * 批量写入的参数
  87       */
  88      private Integer batchSize;
  89      private Long batchWaitInterval;
  90      /**
  91       * 定时任务
  92       */
  93      private transient ScheduledExecutorService scheduler;
  94      private transient ScheduledFuture&lt;?&gt; scheduledFuture;
  95  
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  96 +    /**</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  97 +     * 脏数据管理</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  98 +     */</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  99 +    private DirtyDataManager dirtyDataManager;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 100 +</span>
 101      private HbaseOutputFormat() {
 102      }
 103  
 104      public static HbaseOutputFormatBuilder buildHbaseOutputFormat() {
 105          return new HbaseOutputFormatBuilder();
 106      }
 107  
 108      @Override
 109      public void configure(Configuration parameters) {
 110          // 这里不要做耗时较长的操作，否则会导致AKKA通信超时
 111          // DO NOTHING
 112      }
 113  
 114      @Override
 115      public void open(int taskNumber, int numTasks) throws IOException {
 116          LOG.warn(&quot;---open---&quot;);
 117          records = new ArrayList&lt;&gt;();
 118          conf = HBaseConfiguration.create();
 119          openConn();
 120          table = conn.getTable(TableName.valueOf(tableName));
 121          LOG.warn(&quot;---open end(get table from hbase) ---&quot;);
 122          initMetric();
 123      }
 124  
 125      private void openConn() {
 126          try {
 127              if (kerberosAuthEnable) {
 128                  LOG.info(&quot;open kerberos conn&quot;);
 129                  openKerberosConn();
 130              } else {
 131                  LOG.info(&quot;open conn&quot;);
 132                  conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 133                  conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 134                  conn = ConnectionFactory.createConnection(conf);
 135              }
 136          } catch (Exception e) {
 137              throw new RuntimeException(e);
 138          }
 139          initScheduledTask(batchWaitInterval);
 140      }
 141  
 142      /**
 143       * 初始化定时写入任务
 144       *
 145       * @param batchWaitInterval 定时任务时间
 146       */
 147      private void initScheduledTask(Long batchWaitInterval) {
 148          try {
 149              if (batchWaitInterval &gt; 0) {
 150                  this.scheduler = new ScheduledThreadPoolExecutor(
 151                          1,
 152                          new DTThreadFactory(&quot;hbase-batch-flusher&quot;)
 153                  );
 154  
 155                  this.scheduledFuture = this.scheduler.scheduleWithFixedDelay(
 156                          () -&gt; {
 157                              synchronized (HbaseOutputFormat.this) {
 158                                  if (!records.isEmpty()) {
 159                                      dealBatchOperation(records);
 160                                  }
 161                              }
 162                          }, batchWaitInterval, batchWaitInterval, TimeUnit.MILLISECONDS
 163                  );
 164              }
 165          } catch (Exception e) {
 166              LOG.error(&quot;init schedule task failed !&quot;);
 167              throw new RuntimeException(e);
 168          }
 169      }
 170  
 171      private void openKerberosConn() throws Exception {
 172          conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 173          conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 174  
 175          LOG.info(&quot;kerberos config:{}&quot;, this.conf.toString());
 176          Preconditions.checkArgument(!StringUtils.isEmpty(clientPrincipal), &quot; clientPrincipal not null!&quot;);
 177          Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;);
 178  
 179          fillSyncKerberosConfig(conf, regionserverPrincipal, zookeeperSaslClient, securityKrb5Conf);
 180  
 181          clientKeytabFile = System.getProperty(&quot;user.dir&quot;) + File.separator + clientKeytabFile;
 182          clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal;
 183  
 184          conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KEYTAB_FILE, clientKeytabFile);
 185          conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KERBEROS_PRINCIPAL, clientPrincipal);
 186  
<abbr title=" 187          UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clientKeytabFile);"> 187          UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clie🔵</abbr>
 188          org.apache.hadoop.conf.Configuration finalConf = conf;
 189          conn = userGroupInformation.doAs((PrivilegedAction&lt;Connection&gt;) () -&gt; {
 190              try {
 191                  ScheduledChore authChore = AuthUtil.getAuthChore(finalConf);
 192                  if (authChore != null) {
 193                      choreService = new ChoreService(&quot;hbaseKerberosSink&quot;);
 194                      choreService.scheduleChore(authChore);
 195                  }
 196  
 197                  return ConnectionFactory.createConnection(finalConf);
 198              } catch (IOException e) {
 199                  LOG.error(&quot;Get connection fail with config:{}&quot;, finalConf);
 200                  throw new RuntimeException(e);
 201              }
 202          });
 203      }
 204  
 205      @Override
 206      public void writeRecord(Tuple2&lt;Boolean, Row&gt; record) {
 207          if (record.f0) {
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 208 -            if (this.batchSize != 0) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 209 +            if (this.batchSize &gt; 1) {</span>
 210                  writeBatchRecord(record.f1);
 211              } else {
 212                  dealInsert(record.f1);
 213              }
 214          }
 215      }
 216  
 217      public void writeBatchRecord(Row row) {
 218          records.add(row);
 219          // 数据累计到batchSize之后开始处理
 220          if (records.size() == this.batchSize) {
 221              dealBatchOperation(records);
 222          }
 223      }
 224  
 225      protected synchronized void dealBatchOperation(List&lt;Row&gt; records) {
 226          // A null in the result array means that the call for that action failed, even after retries.
 227          Object[] results = new Object[records.size()];
 228          try {
 229              List&lt;Put&gt; puts = new ArrayList&lt;&gt;();
 230              for (Row record : records) {
 231                  puts.add(getPutByRow(record));
 232              }
 233              table.batch(puts, results);
 234  
 235              // 打印结果
 236              if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {
 237                  // 只打印最后一条数据
 238                  LOG.info(records.get(records.size() - 1).toString());
 239              }
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 240 -        } catch (IOException | InterruptedException ignored) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 241 +        } catch (IOException | InterruptedException e) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 242 +            // ignore exception</span>
 243          } finally {
 244              // 判断数据是否插入成功
 245              for (int i = 0; i &lt; results.length; i++) {
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 246 -                if (results[i] != null) {</span>

<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 247 -                    if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 248 -                        LOG.error(&quot;Get dirty data: {}&quot;, records.get(i).toString());</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 249 -                        LOG.error(&quot;Error cause: &quot; + results[i]);</span>

<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 250 -                    }</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 251 +                if (results[i] instanceof Exception) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 252 +                    dirtyDataManager.execute();</span>
 253                      // 脏数据记录
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 254 +                    dirtyDataManager.collectDirtyData(</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 255 +                        records.get(i).toString(),</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 256 +                        ExceptionTrace.traceOriginalCause((Exception) results[i])</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 257 +                    );</span>
 258                      outDirtyRecords.inc();
 259                  } else {
 260                      // 输出结果条数记录
 261                      outRecords.inc();
 262                  }
 263              }
 264              // 添加完数据之后数据清空records
 265              records.clear();
 266          }
 267      }
 268  
 269      protected void dealInsert(Row record) {
 270          Put put = getPutByRow(record);
 271          if (put == null || put.isEmpty()) {
 272              // 记录脏数据
 273              outDirtyRecords.inc();
 274              return;
 275          }
 276  
 277          try {
 278              table.put(put);
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 279 +            if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 280 +                LOG.info(record.toString());</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 281 +            }</span>
 282          } catch (Exception e) {
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 283 -            if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0 || LOG.isDebugEnabled()) {</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 284 -                LOG.error(&quot;record insert failed ..{}&quot;, record.toString());</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 285 -                LOG.error(&quot;&quot;, e);</span>


<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 286 -            }</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 287 +            dirtyDataManager.collectDirtyData(</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 288 +                record.toString(),</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 289 +                ExceptionTrace.traceOriginalCause(e));</span>
 290              outDirtyRecords.inc();
 291          }
 292  
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 293 -        if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 294 -            LOG.info(record.toString());</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 295 -        }</span>
 296          outRecords.inc();
 297      }
 298  
 299      private Put getPutByRow(Row record) {
 300          String rowKey = buildRowKey(record);
 301          if (StringUtils.isEmpty(rowKey)) {
 302              return null;
 303          }
 304          Put put = new Put(rowKey.getBytes());
 305          for (int i = 0; i &lt; record.getArity(); ++i) {
 306              Object fieldVal = record.getField(i);
 307              if (fieldVal != null) {
 308                  byte[] val = fieldVal.toString().getBytes();
 309                  byte[] cf = families[i].getBytes();
 310                  byte[] qualifier = qualifiers[i].getBytes();
 311  
 312                  put.addColumn(cf, qualifier, val);
 313              }
 314          }
 315          return put;
 316      }
 317  
 318      private String buildRowKey(Row record) {
 319          String rowKeyValues = getRowKeyValues(record);
 320          // all rowkey not null
 321          if (StringUtils.isBlank(rowKeyValues)) {
 322              LOG.error(&quot;row key value must not null,record is ..{}&quot;, record);
 323              outDirtyRecords.inc();
 324              return &quot;&quot;;
 325          }
 326          return rowKeyValues;
 327      }
 328  
 329      private String getRowKeyValues(Row record) {
 330          Map&lt;String, Object&gt; row = rowConvertMap(record);
 331          RowKeyBuilder rowKeyBuilder = new RowKeyBuilder();
 332          rowKeyBuilder.init(rowkey);
 333          return rowKeyBuilder.getRowKey(row);
 334      }
 335  
 336      private Map&lt;String, Object&gt; rowConvertMap(Row record) {
 337          Map&lt;String, Object&gt; rowValue = Maps.newHashMap();
 338          for (int i = 0; i &lt; columnNames.length; i++) {
 339              rowValue.put(columnNames[i], record.getField(i));
 340          }
 341          return rowValue;
 342      }
 343  
 344      @Override
 345      public synchronized void close() throws IOException {
 346          if (closed) {
 347              return;
 348          }
 349  
 350          closed = true;
 351          if (!records.isEmpty()) {
 352              dealBatchOperation(records);
 353          }
 354  
 355          if (scheduledFuture != null) {
 356              scheduledFuture.cancel(false);
 357              if (scheduler != null) {
 358                  scheduler.shutdownNow();
 359              }
 360          }
 361  
 362          if (conn != null) {
 363              conn.close();
 364              conn = null;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 365 +        }</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 366 +</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 367 +        if (dirtyDataManager != null) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 368 +            dirtyDataManager.close();</span>
 369          }
 370      }
 371  
 372      private void fillSyncKerberosConfig(org.apache.hadoop.conf.Configuration config,
 373                                          String regionserverPrincipal,
 374                                          String zookeeperSaslClient,
 375                                          String securityKrb5Conf) {
 376          if (StringUtils.isEmpty(regionserverPrincipal)) {
<abbr title=" 377              throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos&quot;);"> 377              throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos🔵</abbr>
 378          }
 379          config.set(HbaseConfigUtils.KEY_HBASE_MASTER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 380          config.set(HbaseConfigUtils.KEY_HBASE_REGIONSERVER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 381          config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHORIZATION, &quot;true&quot;);
 382          config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHENTICATION, &quot;kerberos&quot;);
 383  
 384  
 385          if (!StringUtils.isEmpty(zookeeperSaslClient)) {
 386              System.setProperty(HbaseConfigUtils.KEY_ZOOKEEPER_SASL_CLIENT, zookeeperSaslClient);
 387          }
 388  
 389          if (!StringUtils.isEmpty(securityKrb5Conf)) {
 390              String krb5ConfPath = System.getProperty(&quot;user.dir&quot;) + File.separator + securityKrb5Conf;
 391              LOG.info(&quot;krb5ConfPath:{}&quot;, krb5ConfPath);
 392              System.setProperty(HbaseConfigUtils.KEY_JAVA_SECURITY_KRB5_CONF, krb5ConfPath);
 393          }
 394      }
 395  
 396      @Override
 397      public String toString() {
 398          return &quot;HbaseOutputFormat kerberos{&quot; +
 399                  &quot;kerberosAuthEnable=&quot; + kerberosAuthEnable +
 400                  &quot;, regionserverKeytabFile=&#x27;&quot; + regionserverKeytabFile + &#x27;\&#x27;&#x27; +
 401                  &quot;, regionserverPrincipal=&#x27;&quot; + regionserverPrincipal + &#x27;\&#x27;&#x27; +
 402                  &quot;, securityKrb5Conf=&#x27;&quot; + securityKrb5Conf + &#x27;\&#x27;&#x27; +
 403                  &quot;, zookeeperSaslClient=&#x27;&quot; + zookeeperSaslClient + &#x27;\&#x27;&#x27; +
 404                  &quot;, clientPrincipal=&#x27;&quot; + clientPrincipal + &#x27;\&#x27;&#x27; +
 405                  &quot;, clientKeytabFile=&#x27;&quot; + clientKeytabFile + &#x27;\&#x27;&#x27; +
 406                  &quot;, batchSize=&#x27;&quot; + batchSize + &#x27;\&#x27;&#x27; +
 407                  &quot;, batchWaitInterval=&#x27;&quot; + batchWaitInterval + &#x27;\&#x27;&#x27; +
 408                  &#x27;}&#x27;;
 409      }
 410  
 411      public static class HbaseOutputFormatBuilder {
 412  
 413          private final HbaseOutputFormat format;
 414  
 415          private HbaseOutputFormatBuilder() {
 416              format = new HbaseOutputFormat();
 417          }
 418  
 419          public HbaseOutputFormatBuilder setHost(String host) {
 420              format.host = host;
 421              return this;
 422          }
 423  
 424          public HbaseOutputFormatBuilder setZkParent(String parent) {
 425              format.zkParent = parent;
 426              return this;
 427          }
 428  
 429  
 430          public HbaseOutputFormatBuilder setTable(String tableName) {
 431              format.tableName = tableName;
 432              return this;
 433          }
 434  
 435          public HbaseOutputFormatBuilder setRowkey(String rowkey) {
 436              format.rowkey = rowkey;
 437              return this;
 438          }
 439  
 440          public HbaseOutputFormatBuilder setColumnNames(String[] columnNames) {
 441              format.columnNames = columnNames;
 442              return this;
 443          }
 444  
 445          public HbaseOutputFormatBuilder setColumnNameFamily(Map&lt;String, String&gt; columnNameFamily) {
 446              format.columnNameFamily = columnNameFamily;
 447              return this;
 448          }
 449  
 450          public HbaseOutputFormatBuilder setKerberosAuthEnable(boolean kerberosAuthEnable) {
 451              format.kerberosAuthEnable = kerberosAuthEnable;
 452              return this;
 453          }
 454  
 455          public HbaseOutputFormatBuilder setRegionserverKeytabFile(String regionserverKeytabFile) {
 456              format.regionserverKeytabFile = regionserverKeytabFile;
 457              return this;
 458          }
 459  
 460          public HbaseOutputFormatBuilder setRegionserverPrincipal(String regionserverPrincipal) {
 461              format.regionserverPrincipal = regionserverPrincipal;
 462              return this;
 463          }
 464  
 465          public HbaseOutputFormatBuilder setSecurityKrb5Conf(String securityKrb5Conf) {
 466              format.securityKrb5Conf = securityKrb5Conf;
 467              return this;
 468          }
 469  
 470          public HbaseOutputFormatBuilder setZookeeperSaslClient(String zookeeperSaslClient) {
 471              format.zookeeperSaslClient = zookeeperSaslClient;
 472              return this;
 473          }
 474  
 475          public HbaseOutputFormatBuilder setClientPrincipal(String clientPrincipal) {
 476              format.clientPrincipal = clientPrincipal;
 477              return this;
 478          }
 479  
 480          public HbaseOutputFormatBuilder setClientKeytabFile(String clientKeytabFile) {
 481              format.clientKeytabFile = clientKeytabFile;
 482              return this;
 483          }
 484  
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 485 +        public HbaseOutputFormatBuilder setDirtyManager(DirtyDataManager dirtyDataManager) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 486 +            format.dirtyDataManager = dirtyDataManager;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 487 +            return this;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 488 +        }</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 489 +</span>
 490          public HbaseOutputFormatBuilder setBatchSize(Integer batchSize) {
 491              format.batchSize = batchSize;
 492              return this;
 493          }
 494  
 495          public HbaseOutputFormatBuilder setBatchWaitInterval(Long batchWaitInterval) {
 496              format.batchWaitInterval = batchWaitInterval;
 497              return this;
 498          }
 499  
 500          public HbaseOutputFormat finish() {
 501              Preconditions.checkNotNull(format.host, &quot;zookeeperQuorum should be specified&quot;);
 502              Preconditions.checkNotNull(format.tableName, &quot;tableName should be specified&quot;);
 503              Preconditions.checkNotNull(format.columnNames, &quot;columnNames should be specified&quot;);
 504              Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be zero&quot;);
 505  
 506              String[] families = new String[format.columnNames.length];
 507              String[] qualifiers = new String[format.columnNames.length];
 508  
 509              if (format.columnNameFamily != null) {
 510                  List&lt;String&gt; keyList = new LinkedList&lt;&gt;(format.columnNameFamily.keySet());
 511                  String[] columns = keyList.toArray(new String[0]);
 512                  for (int i = 0; i &lt; columns.length; ++i) {
 513                      String col = columns[i];
 514                      String[] part = col.split(&quot;:&quot;);
 515                      families[i] = part[0];
 516                      qualifiers[i] = part[1];
 517                  }
 518              }
 519              format.families = families;
 520              format.qualifiers = qualifiers;
 521  
 522              return format;
 523          }
 524      }
 525  }</pre></td>
                            <td><pre>   1  /*
   2   * Licensed to the Apache Software Foundation (ASF) under one
   3   * or more contributor license agreements.  See the NOTICE file
   4   * distributed with this work for additional information
   5   * regarding copyright ownership.  The ASF licenses this file
   6   * to you under the Apache License, Version 2.0 (the
   7   * &quot;License&quot;); you may not use this file except in compliance
   8   * with the License.  You may obtain a copy of the License at
   9   *
  10   *     http://www.apache.org/licenses/LICENSE-2.0
  11   *
  12   * Unless required by applicable law or agreed to in writing, software
  13   * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15   * See the License for the specific language governing permissions and
  16   * limitations under the License.
  17   */
  18  
  19  
  20  package com.dtstack.flink.sql.sink.hbase;
  21  
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  22 +import com.dtstack.flink.sql.exception.ExceptionTrace;</span>

  23  import com.dtstack.flink.sql.factory.DTThreadFactory;
  24  import com.dtstack.flink.sql.outputformat.AbstractDtRichOutputFormat;
  25  import com.google.common.collect.Maps;
  26  import org.apache.commons.lang3.StringUtils;
  27  import org.apache.flink.api.java.tuple.Tuple2;
  28  import org.apache.flink.configuration.Configuration;
  29  import org.apache.flink.types.Row;
  30  import org.apache.flink.util.Preconditions;
  31  import org.apache.hadoop.hbase.AuthUtil;
  32  import org.apache.hadoop.hbase.ChoreService;
  33  import org.apache.hadoop.hbase.HBaseConfiguration;
  34  import org.apache.hadoop.hbase.ScheduledChore;
  35  import org.apache.hadoop.hbase.TableName;
  36  import org.apache.hadoop.hbase.client.Connection;
  37  import org.apache.hadoop.hbase.client.ConnectionFactory;
  38  import org.apache.hadoop.hbase.client.Put;
  39  import org.apache.hadoop.hbase.client.Table;
  40  import org.apache.hadoop.security.UserGroupInformation;
  41  import org.slf4j.Logger;
  42  import org.slf4j.LoggerFactory;
  43  
  44  import java.io.File;
  45  import java.io.IOException;
  46  import java.security.PrivilegedAction;
  47  import java.util.ArrayList;
  48  import java.util.LinkedList;
  49  import java.util.List;
  50  import java.util.Map;
  51  import java.util.concurrent.ScheduledExecutorService;
  52  import java.util.concurrent.ScheduledFuture;
  53  import java.util.concurrent.ScheduledThreadPoolExecutor;
  54  import java.util.concurrent.TimeUnit;
  55  
  56  /**
  57   * @author: jingzhen@dtstack.com
  58   * date: 2017-6-29
  59   */
  60  public class HbaseOutputFormat extends AbstractDtRichOutputFormat&lt;Tuple2&lt;Boolean, Row&gt;&gt; {
  61  
  62      private static final Logger LOG = LoggerFactory.getLogger(HbaseOutputFormat.class);
  63      private String host;
  64      private String zkParent;
  65      private String rowkey;
  66      private String tableName;
  67      private String[] columnNames;
  68      private Map&lt;String, String&gt; columnNameFamily;
  69      private boolean kerberosAuthEnable;
  70      private String regionserverKeytabFile;
  71      private String regionserverPrincipal;
  72      private String securityKrb5Conf;
  73      private String zookeeperSaslClient;
  74      private String clientPrincipal;
  75      private String clientKeytabFile;
  76      private String[] families;
  77      private String[] qualifiers;
  78      private transient org.apache.hadoop.conf.Configuration conf;
  79      private transient Connection conn;
  80      private transient Table table;
  81      private transient ChoreService choreService;
  82      private transient List&lt;Row&gt; records;
  83      private transient volatile boolean closed = false;
  84      /**
  85       * 批量写入的参数
  86       */
  87      private Integer batchSize;
  88      private Long batchWaitInterval;
  89      /**
  90       * 定时任务
  91       */
  92      private transient ScheduledExecutorService scheduler;
  93      private transient ScheduledFuture&lt;?&gt; scheduledFuture;
  94  





  95      private HbaseOutputFormat() {
  96      }
  97  
  98      public static HbaseOutputFormatBuilder buildHbaseOutputFormat() {
  99          return new HbaseOutputFormatBuilder();
 100      }
 101  
 102      @Override
 103      public void configure(Configuration parameters) {
 104          // 这里不要做耗时较长的操作，否则会导致AKKA通信超时
 105          // DO NOTHING
 106      }
 107  
 108      @Override
 109      public void open(int taskNumber, int numTasks) throws IOException {
 110          LOG.warn(&quot;---open---&quot;);
 111          records = new ArrayList&lt;&gt;();
 112          conf = HBaseConfiguration.create();
 113          openConn();
 114          table = conn.getTable(TableName.valueOf(tableName));
 115          LOG.warn(&quot;---open end(get table from hbase) ---&quot;);
 116          initMetric();
 117      }
 118  
 119      private void openConn() {
 120          try {
 121              if (kerberosAuthEnable) {
 122                  LOG.info(&quot;open kerberos conn&quot;);
 123                  openKerberosConn();
 124              } else {
 125                  LOG.info(&quot;open conn&quot;);
 126                  conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 127                  conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 128                  conn = ConnectionFactory.createConnection(conf);
 129              }
 130          } catch (Exception e) {
 131              throw new RuntimeException(e);
 132          }
 133          initScheduledTask(batchWaitInterval);
 134      }
 135  
 136      /**
 137       * 初始化定时写入任务
 138       *
 139       * @param batchWaitInterval 定时任务时间
 140       */
 141      private void initScheduledTask(Long batchWaitInterval) {
 142          try {
 143              if (batchWaitInterval &gt; 0) {
 144                  this.scheduler = new ScheduledThreadPoolExecutor(
 145                          1,
 146                          new DTThreadFactory(&quot;hbase-batch-flusher&quot;)
 147                  );
 148  
 149                  this.scheduledFuture = this.scheduler.scheduleWithFixedDelay(
 150                          () -&gt; {
 151                              synchronized (HbaseOutputFormat.this) {
 152                                  if (!records.isEmpty()) {
 153                                      dealBatchOperation(records);
 154                                  }
 155                              }
 156                          }, batchWaitInterval, batchWaitInterval, TimeUnit.MILLISECONDS
 157                  );
 158              }
 159          } catch (Exception e) {
 160              LOG.error(&quot;init schedule task failed !&quot;);
 161              throw new RuntimeException(e);
 162          }
 163      }
 164  
 165      private void openKerberosConn() throws Exception {
 166          conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_QUORUM, host);
 167          conf.set(HbaseConfigUtils.KEY_HBASE_ZOOKEEPER_ZNODE_QUORUM, zkParent);
 168  
 169          LOG.info(&quot;kerberos config:{}&quot;, this.conf.toString());
 170          Preconditions.checkArgument(!StringUtils.isEmpty(clientPrincipal), &quot; clientPrincipal not null!&quot;);
 171          Preconditions.checkArgument(!StringUtils.isEmpty(clientKeytabFile), &quot; clientKeytabFile not null!&quot;);
 172  
 173          fillSyncKerberosConfig(conf, regionserverPrincipal, zookeeperSaslClient, securityKrb5Conf);
 174  
 175          clientKeytabFile = System.getProperty(&quot;user.dir&quot;) + File.separator + clientKeytabFile;
 176          clientPrincipal = !StringUtils.isEmpty(clientPrincipal) ? clientPrincipal : regionserverPrincipal;
 177  
 178          conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KEYTAB_FILE, clientKeytabFile);
 179          conf.set(HbaseConfigUtils.KEY_HBASE_CLIENT_KERBEROS_PRINCIPAL, clientPrincipal);
 180  
<abbr title=" 181          UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clientKeytabFile);"> 181          UserGroupInformation userGroupInformation = HbaseConfigUtils.loginAndReturnUGI(conf, clientPrincipal, clie🔵</abbr>
 182          org.apache.hadoop.conf.Configuration finalConf = conf;
 183          conn = userGroupInformation.doAs((PrivilegedAction&lt;Connection&gt;) () -&gt; {
 184              try {
 185                  ScheduledChore authChore = AuthUtil.getAuthChore(finalConf);
 186                  if (authChore != null) {
 187                      choreService = new ChoreService(&quot;hbaseKerberosSink&quot;);
 188                      choreService.scheduleChore(authChore);
 189                  }
 190  
 191                  return ConnectionFactory.createConnection(finalConf);
 192              } catch (IOException e) {
 193                  LOG.error(&quot;Get connection fail with config:{}&quot;, finalConf);
 194                  throw new RuntimeException(e);
 195              }
 196          });
 197      }
 198  
 199      @Override
 200      public void writeRecord(Tuple2&lt;Boolean, Row&gt; record) {
 201          if (record.f0) {
 202              if (this.batchSize != 0) {

 203                  writeBatchRecord(record.f1);
 204              } else {
 205                  dealInsert(record.f1);
 206              }
 207          }
 208      }
 209  
 210      public void writeBatchRecord(Row row) {
 211          records.add(row);
 212          // 数据累计到batchSize之后开始处理
 213          if (records.size() == this.batchSize) {
 214              dealBatchOperation(records);
 215          }
 216      }
 217  
 218      protected synchronized void dealBatchOperation(List&lt;Row&gt; records) {
 219          // A null in the result array means that the call for that action failed, even after retries.
 220          Object[] results = new Object[records.size()];
 221          try {
 222              List&lt;Put&gt; puts = new ArrayList&lt;&gt;();
 223              for (Row record : records) {
 224                  puts.add(getPutByRow(record));
 225              }
 226              table.batch(puts, results);
 227  
 228              // 打印结果
 229              if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {
 230                  // 只打印最后一条数据
 231                  LOG.info(records.get(records.size() - 1).toString());
 232              }
 233          } catch (IOException | InterruptedException ignored) {


 234          } finally {
 235              // 判断数据是否插入成功
 236              for (int i = 0; i &lt; results.length; i++) {
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 237 -                if (results[i] != null) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 238 +                if (results[i] instanceof Exception) {</span>
 239                      if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0) {
 240                          LOG.error(&quot;Get dirty data: {}&quot;, records.get(i).toString());
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 241 -                        LOG.error(&quot;Error cause: &quot; + results[i]);</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 242 +                        LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause((Exception) results[i]));</span>
 243                      }


 244                      // 脏数据记录




 245                      outDirtyRecords.inc();
 246                  } else {
 247                      // 输出结果条数记录
 248                      outRecords.inc();
 249                  }
 250              }
 251              // 添加完数据之后数据清空records
 252              records.clear();
 253          }
 254      }
 255  
 256      protected void dealInsert(Row record) {
 257          Put put = getPutByRow(record);
 258          if (put == null || put.isEmpty()) {
 259              // 记录脏数据
 260              outDirtyRecords.inc();
 261              return;
 262          }
 263  
 264          try {
 265              table.put(put);



 266          } catch (Exception e) {
 267              if (outDirtyRecords.getCount() % DIRTY_PRINT_FREQUENCY == 0 || LOG.isDebugEnabled()) {
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 268 -                LOG.error(&quot;record insert failed ..{}&quot;, record.toString());</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 269 -                LOG.error(&quot;&quot;, e);</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 270 +                LOG.error(&quot;Get dirty data: {}&quot;, record.toString());</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 271 +                LOG.error(&quot;Error cause: &quot; + ExceptionTrace.traceOriginalCause(e));</span>
 272              }



 273              outDirtyRecords.inc();
 274          }
 275  
 276          if (outRecords.getCount() % ROW_PRINT_FREQUENCY == 0) {
 277              LOG.info(record.toString());
 278          }
 279          outRecords.inc();
 280      }
 281  
 282      private Put getPutByRow(Row record) {
 283          String rowKey = buildRowKey(record);
 284          if (StringUtils.isEmpty(rowKey)) {
 285              return null;
 286          }
 287          Put put = new Put(rowKey.getBytes());
 288          for (int i = 0; i &lt; record.getArity(); ++i) {
 289              Object fieldVal = record.getField(i);
 290              if (fieldVal != null) {
 291                  byte[] val = fieldVal.toString().getBytes();
 292                  byte[] cf = families[i].getBytes();
 293                  byte[] qualifier = qualifiers[i].getBytes();
 294  
 295                  put.addColumn(cf, qualifier, val);
 296              }
 297          }
 298          return put;
 299      }
 300  
 301      private String buildRowKey(Row record) {
 302          String rowKeyValues = getRowKeyValues(record);
 303          // all rowkey not null
 304          if (StringUtils.isBlank(rowKeyValues)) {
 305              LOG.error(&quot;row key value must not null,record is ..{}&quot;, record);
 306              outDirtyRecords.inc();
 307              return &quot;&quot;;
 308          }
 309          return rowKeyValues;
 310      }
 311  
 312      private String getRowKeyValues(Row record) {
 313          Map&lt;String, Object&gt; row = rowConvertMap(record);
 314          RowKeyBuilder rowKeyBuilder = new RowKeyBuilder();
 315          rowKeyBuilder.init(rowkey);
 316          return rowKeyBuilder.getRowKey(row);
 317      }
 318  
 319      private Map&lt;String, Object&gt; rowConvertMap(Row record) {
 320          Map&lt;String, Object&gt; rowValue = Maps.newHashMap();
 321          for (int i = 0; i &lt; columnNames.length; i++) {
 322              rowValue.put(columnNames[i], record.getField(i));
 323          }
 324          return rowValue;
 325      }
 326  
 327      @Override
 328      public synchronized void close() throws IOException {
 329          if (closed) {
 330              return;
 331          }
 332  
 333          closed = true;
 334          if (!records.isEmpty()) {
 335              dealBatchOperation(records);
 336          }
 337  
 338          if (scheduledFuture != null) {
 339              scheduledFuture.cancel(false);
 340              if (scheduler != null) {
 341                  scheduler.shutdownNow();
 342              }
 343          }
 344  
 345          if (conn != null) {
 346              conn.close();
 347              conn = null;




 348          }
 349      }
 350  
 351      private void fillSyncKerberosConfig(org.apache.hadoop.conf.Configuration config,
 352                                          String regionserverPrincipal,
 353                                          String zookeeperSaslClient,
 354                                          String securityKrb5Conf) {
 355          if (StringUtils.isEmpty(regionserverPrincipal)) {
<abbr title=" 356              throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos&quot;);"> 356              throw new IllegalArgumentException(&quot;Must provide regionserverPrincipal when authentication is Kerberos🔵</abbr>
 357          }
 358          config.set(HbaseConfigUtils.KEY_HBASE_MASTER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 359          config.set(HbaseConfigUtils.KEY_HBASE_REGIONSERVER_KERBEROS_PRINCIPAL, regionserverPrincipal);
 360          config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHORIZATION, &quot;true&quot;);
 361          config.set(HbaseConfigUtils.KEY_HBASE_SECURITY_AUTHENTICATION, &quot;kerberos&quot;);
 362  
 363  
 364          if (!StringUtils.isEmpty(zookeeperSaslClient)) {
 365              System.setProperty(HbaseConfigUtils.KEY_ZOOKEEPER_SASL_CLIENT, zookeeperSaslClient);
 366          }
 367  
 368          if (!StringUtils.isEmpty(securityKrb5Conf)) {
 369              String krb5ConfPath = System.getProperty(&quot;user.dir&quot;) + File.separator + securityKrb5Conf;
 370              LOG.info(&quot;krb5ConfPath:{}&quot;, krb5ConfPath);
 371              System.setProperty(HbaseConfigUtils.KEY_JAVA_SECURITY_KRB5_CONF, krb5ConfPath);
 372          }
 373      }
 374  
 375      @Override
 376      public String toString() {
 377          return &quot;HbaseOutputFormat kerberos{&quot; +
 378                  &quot;kerberosAuthEnable=&quot; + kerberosAuthEnable +
 379                  &quot;, regionserverKeytabFile=&#x27;&quot; + regionserverKeytabFile + &#x27;\&#x27;&#x27; +
 380                  &quot;, regionserverPrincipal=&#x27;&quot; + regionserverPrincipal + &#x27;\&#x27;&#x27; +
 381                  &quot;, securityKrb5Conf=&#x27;&quot; + securityKrb5Conf + &#x27;\&#x27;&#x27; +
 382                  &quot;, zookeeperSaslClient=&#x27;&quot; + zookeeperSaslClient + &#x27;\&#x27;&#x27; +
 383                  &quot;, clientPrincipal=&#x27;&quot; + clientPrincipal + &#x27;\&#x27;&#x27; +
 384                  &quot;, clientKeytabFile=&#x27;&quot; + clientKeytabFile + &#x27;\&#x27;&#x27; +
 385                  &quot;, batchSize=&#x27;&quot; + batchSize + &#x27;\&#x27;&#x27; +
 386                  &quot;, batchWaitInterval=&#x27;&quot; + batchWaitInterval + &#x27;\&#x27;&#x27; +
 387                  &#x27;}&#x27;;
 388      }
 389  
 390      public static class HbaseOutputFormatBuilder {
 391  
 392          private final HbaseOutputFormat format;
 393  
 394          private HbaseOutputFormatBuilder() {
 395              format = new HbaseOutputFormat();
 396          }
 397  
 398          public HbaseOutputFormatBuilder setHost(String host) {
 399              format.host = host;
 400              return this;
 401          }
 402  
 403          public HbaseOutputFormatBuilder setZkParent(String parent) {
 404              format.zkParent = parent;
 405              return this;
 406          }
 407  
 408  
 409          public HbaseOutputFormatBuilder setTable(String tableName) {
 410              format.tableName = tableName;
 411              return this;
 412          }
 413  
 414          public HbaseOutputFormatBuilder setRowkey(String rowkey) {
 415              format.rowkey = rowkey;
 416              return this;
 417          }
 418  
 419          public HbaseOutputFormatBuilder setColumnNames(String[] columnNames) {
 420              format.columnNames = columnNames;
 421              return this;
 422          }
 423  
 424          public HbaseOutputFormatBuilder setColumnNameFamily(Map&lt;String, String&gt; columnNameFamily) {
 425              format.columnNameFamily = columnNameFamily;
 426              return this;
 427          }
 428  
 429          public HbaseOutputFormatBuilder setKerberosAuthEnable(boolean kerberosAuthEnable) {
 430              format.kerberosAuthEnable = kerberosAuthEnable;
 431              return this;
 432          }
 433  
 434          public HbaseOutputFormatBuilder setRegionserverKeytabFile(String regionserverKeytabFile) {
 435              format.regionserverKeytabFile = regionserverKeytabFile;
 436              return this;
 437          }
 438  
 439          public HbaseOutputFormatBuilder setRegionserverPrincipal(String regionserverPrincipal) {
 440              format.regionserverPrincipal = regionserverPrincipal;
 441              return this;
 442          }
 443  
 444          public HbaseOutputFormatBuilder setSecurityKrb5Conf(String securityKrb5Conf) {
 445              format.securityKrb5Conf = securityKrb5Conf;
 446              return this;
 447          }
 448  
 449          public HbaseOutputFormatBuilder setZookeeperSaslClient(String zookeeperSaslClient) {
 450              format.zookeeperSaslClient = zookeeperSaslClient;
 451              return this;
 452          }
 453  
 454          public HbaseOutputFormatBuilder setClientPrincipal(String clientPrincipal) {
 455              format.clientPrincipal = clientPrincipal;
 456              return this;
 457          }
 458  
 459          public HbaseOutputFormatBuilder setClientKeytabFile(String clientKeytabFile) {
 460              format.clientKeytabFile = clientKeytabFile;
 461              return this;
 462          }
 463  





 464          public HbaseOutputFormatBuilder setBatchSize(Integer batchSize) {
 465              format.batchSize = batchSize;
 466              return this;
 467          }
 468  
 469          public HbaseOutputFormatBuilder setBatchWaitInterval(Long batchWaitInterval) {
 470              format.batchWaitInterval = batchWaitInterval;
 471              return this;
 472          }
 473  
 474          public HbaseOutputFormat finish() {
 475              Preconditions.checkNotNull(format.host, &quot;zookeeperQuorum should be specified&quot;);
 476              Preconditions.checkNotNull(format.tableName, &quot;tableName should be specified&quot;);
 477              Preconditions.checkNotNull(format.columnNames, &quot;columnNames should be specified&quot;);
 478              Preconditions.checkArgument(format.columnNames.length != 0, &quot;columnNames length should not be zero&quot;);
 479  
 480              String[] families = new String[format.columnNames.length];
 481              String[] qualifiers = new String[format.columnNames.length];
 482  
 483              if (format.columnNameFamily != null) {
 484                  List&lt;String&gt; keyList = new LinkedList&lt;&gt;(format.columnNameFamily.keySet());
 485                  String[] columns = keyList.toArray(new String[0]);
 486                  for (int i = 0; i &lt; columns.length; ++i) {
 487                      String col = columns[i];
 488                      String[] part = col.split(&quot;:&quot;);
 489                      families[i] = part[0];
 490                      qualifiers[i] = part[1];
 491                  }
 492              }
 493              format.families = families;
 494              format.qualifiers = qualifiers;
 495  
 496              return format;
 497          }
 498      }
 499  }</pre></td>
                        </tr>
                    </table>
                </div>
              </body>
            </html>
            