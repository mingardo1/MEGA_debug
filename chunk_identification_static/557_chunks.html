<!DOCTYPE html>
<html lang="en">
          <head>
            <meta charset="utf-8">
            <title>557 chunks</title>
                <style>
                    #top {
                        height: 48vh;
                        overflow-y: auto;
                    }
                    #bottom {
                        height: 48vh;
                        overflow-y: auto;
                    }
                </style>
          </head>
          <body>
            <pre>[[{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;\t\tTypeInformation[] types = new &#x27;
                         &#x27;TypeInformation[kafkaSourceTableInfo.getFields().length];\n&#x27;
                         &#x27;\t\tfor (int i = 0; i &lt; &#x27;
                         &#x27;kafkaSourceTableInfo.getFieldClasses().length; i++) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;\t\t\ttypes[i] = &#x27;
                         &#x27;TypeInformation.of(kafkaSourceTableInfo.getFieldClasses()[i]);\n&#x27;
                         &#x27;\t\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tTypeInformation&lt;Row&gt; typeInformation = new &#x27;
                         &#x27;RowTypeInfo(types, &#x27;
                         &#x27;kafkaSourceTableInfo.getFields());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tFlinkKafkaConsumer011&lt;Row&gt; kafkaSrc = &#x27;
                         &#x27;(FlinkKafkaConsumer011&lt;Row&gt;) new &#x27;
                         &#x27;KafkaConsumer011Factory()\n&#x27;
                         &#x27;\t\t\t\t&#x27;
                         &#x27;.createKafkaTableSource(kafkaSourceTableInfo, &#x27;
                         &#x27;typeInformation, props);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\t//earliest,latest\n&#x27;
                         &#x27;\t\tif &#x27;
                         &#x27;(&quot;earliest&quot;.equalsIgnoreCase(kafkaSourceTableInfo.getOffsetReset())) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;\t\t\tkafkaSrc.setStartFromEarliest();\n&#x27;
                         &#x27;\t\t} else if &#x27;
                         &#x27;(DtStringUtil.isJosn(kafkaSourceTableInfo.getOffsetReset())) &#x27;
                         &#x27;{// {&quot;0&quot;:12312,&quot;1&quot;:12321,&quot;2&quot;:12312}\n&#x27;
                         &#x27;\t\t\ttry {\n&#x27;
                         &#x27;\t\t\t\tProperties properties = &#x27;
                         &#x27;PluginUtil.jsonStrToObject(kafkaSourceTableInfo.getOffsetReset(), &#x27;
                         &#x27;Properties.class);\n&#x27;
                         &#x27;\t\t\t\tMap&lt;String, Object&gt; offsetMap = &#x27;
                         &#x27;PluginUtil.objectToMap(properties);\n&#x27;
                         &#x27;\t\t\t\tMap&lt;KafkaTopicPartition, Long&gt; &#x27;
                         &#x27;specificStartupOffsets = new HashMap&lt;&gt;();\n&#x27;
                         &#x27;\t\t\t\tfor (Map.Entry&lt;String, Object&gt; entry : &#x27;
                         &#x27;offsetMap.entrySet()) {\n&#x27;
                         &#x27;\t\t\t\t\tspecificStartupOffsets.put(new &#x27;
                         &#x27;KafkaTopicPartition(topicName, &#x27;
                         &#x27;Integer.valueOf(entry.getKey())), &#x27;
                         &#x27;Long.valueOf(entry.getValue().toString()));\n&#x27;
                         &#x27;\t\t\t\t}\n&#x27;
                         &#x27;\t\t\t\t&#x27;
                         &#x27;kafkaSrc.setStartFromSpecificOffsets(specificStartupOffsets);\n&#x27;
                         &#x27;\t\t\t} catch (Exception e) {\n&#x27;
                         &#x27;\t\t\t\tthrow new RuntimeException(&quot;not support &#x27;
                         &#x27;offsetReset type:&quot; + &#x27;
                         &#x27;kafkaSourceTableInfo.getOffsetReset());\n&#x27;
                         &#x27;\t\t\t}\n&#x27;
                         &#x27;\t\t} else {\n&#x27;
                         &#x27;\t\t\tkafkaSrc.setStartFromLatest();\n&#x27;
                         &#x27;\t\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tString fields = &#x27;
                         &#x27;StringUtils.join(kafkaSourceTableInfo.getFields(), &#x27;
                         &#x27;&quot;,&quot;);\n&#x27;
                         &#x27;\t\tString sourceOperatorName = &#x27;
                         &#x27;SOURCE_OPERATOR_NAME_TPL.replace(&quot;${topic}&quot;, &#x27;
                         &#x27;topicName).replace(&quot;${table}&quot;, &#x27;
                         &#x27;sourceTableInfo.getName());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tDataStreamSource kafkaSource = &#x27;
                         &#x27;env.addSource(kafkaSrc, sourceOperatorName, &#x27;
                         &#x27;typeInformation);\n&#x27;
                         &#x27;\t\tInteger parallelism = &#x27;
                         &#x27;kafkaSourceTableInfo.getParallelism();\n&#x27;
                         &#x27;\t\tif (parallelism &gt; 0) {\n&#x27;
                         &#x27;\t\t\tkafkaSource.setParallelism(parallelism);\n&#x27;
                         &#x27;\t\t}\n&#x27;
                         &#x27;\t\treturn tableEnv.fromDataStream(kafkaSource, &#x27;
                         &#x27;fields);\n&#x27;
                         &#x27;\t}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        return &#x27;
                           &#x27;tableEnv.fromDataStream(kafkaSource, fields);\n&#x27;
                           &#x27;    }\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;\t\tInteger parallelism = &#x27;
                         &#x27;kafkaSourceTableInfo.getParallelism();\n&#x27;
                         &#x27;\t\tif (parallelism &gt; 0) {\n&#x27;
                         &#x27;\t\t\tkafkaSource.setParallelism(parallelism);\n&#x27;
                         &#x27;\t\t}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        &#x27;
                           &#x27;kafkaSource.setParallelism(kafkaSourceTableInfo.getParallelism());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;setStartPosition(kafkaSourceTableInfo.getOffsetReset(), &#x27;
                           &#x27;topicName, kafkaSrc);\n&#x27;
                           &#x27;        String fields = &#x27;
                           &#x27;StringUtils.join(kafkaSourceTableInfo.getFields(), &#x27;
                           &#x27;&quot;,&quot;);\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;jfstmerge&#x27;}}],
 [{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;\t\tTypeInformation[] types = new &#x27;
                         &#x27;TypeInformation[kafkaSourceTableInfo.getFields().length];\n&#x27;
                         &#x27;\t\tfor (int i = 0; i &lt; &#x27;
                         &#x27;kafkaSourceTableInfo.getFieldClasses().length; i++) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;\t\t\ttypes[i] = &#x27;
                         &#x27;TypeInformation.of(kafkaSourceTableInfo.getFieldClasses()[i]);\n&#x27;
                         &#x27;\t\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tTypeInformation&lt;Row&gt; typeInformation = new &#x27;
                         &#x27;RowTypeInfo(types, &#x27;
                         &#x27;kafkaSourceTableInfo.getFields());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tFlinkKafkaConsumer011&lt;Row&gt; kafkaSrc = &#x27;
                         &#x27;(FlinkKafkaConsumer011&lt;Row&gt;) new &#x27;
                         &#x27;KafkaConsumer011Factory()\n&#x27;
                         &#x27;\t\t\t\t&#x27;
                         &#x27;.createKafkaTableSource(kafkaSourceTableInfo, &#x27;
                         &#x27;typeInformation, props);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\t//earliest,latest\n&#x27;
                         &#x27;\t\tif &#x27;
                         &#x27;(&quot;earliest&quot;.equalsIgnoreCase(kafkaSourceTableInfo.getOffsetReset())) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;\t\t\tkafkaSrc.setStartFromEarliest();\n&#x27;
                         &#x27;\t\t} else if &#x27;
                         &#x27;(DtStringUtil.isJosn(kafkaSourceTableInfo.getOffsetReset())) &#x27;
                         &#x27;{// {&quot;0&quot;:12312,&quot;1&quot;:12321,&quot;2&quot;:12312}\n&#x27;
                         &#x27;\t\t\ttry {\n&#x27;
                         &#x27;\t\t\t\tProperties properties = &#x27;
                         &#x27;PluginUtil.jsonStrToObject(kafkaSourceTableInfo.getOffsetReset(), &#x27;
                         &#x27;Properties.class);\n&#x27;
                         &#x27;\t\t\t\tMap&lt;String, Object&gt; offsetMap = &#x27;
                         &#x27;PluginUtil.objectToMap(properties);\n&#x27;
                         &#x27;\t\t\t\tMap&lt;KafkaTopicPartition, Long&gt; &#x27;
                         &#x27;specificStartupOffsets = new HashMap&lt;&gt;();\n&#x27;
                         &#x27;\t\t\t\tfor (Map.Entry&lt;String, Object&gt; entry : &#x27;
                         &#x27;offsetMap.entrySet()) {\n&#x27;
                         &#x27;\t\t\t\t\tspecificStartupOffsets.put(new &#x27;
                         &#x27;KafkaTopicPartition(topicName, &#x27;
                         &#x27;Integer.valueOf(entry.getKey())), &#x27;
                         &#x27;Long.valueOf(entry.getValue().toString()));\n&#x27;
                         &#x27;\t\t\t\t}\n&#x27;
                         &#x27;\t\t\t\t&#x27;
                         &#x27;kafkaSrc.setStartFromSpecificOffsets(specificStartupOffsets);\n&#x27;
                         &#x27;\t\t\t} catch (Exception e) {\n&#x27;
                         &#x27;\t\t\t\tthrow new RuntimeException(&quot;not support &#x27;
                         &#x27;offsetReset type:&quot; + &#x27;
                         &#x27;kafkaSourceTableInfo.getOffsetReset());\n&#x27;
                         &#x27;\t\t\t}\n&#x27;
                         &#x27;\t\t} else {\n&#x27;
                         &#x27;\t\t\tkafkaSrc.setStartFromLatest();\n&#x27;
                         &#x27;\t\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tString fields = &#x27;
                         &#x27;StringUtils.join(kafkaSourceTableInfo.getFields(), &#x27;
                         &#x27;&quot;,&quot;);\n&#x27;
                         &#x27;\t\tString sourceOperatorName = &#x27;
                         &#x27;SOURCE_OPERATOR_NAME_TPL.replace(&quot;${topic}&quot;, &#x27;
                         &#x27;topicName).replace(&quot;${table}&quot;, &#x27;
                         &#x27;sourceTableInfo.getName());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tDataStreamSource kafkaSource = &#x27;
                         &#x27;env.addSource(kafkaSrc, sourceOperatorName, &#x27;
                         &#x27;typeInformation);\n&#x27;
                         &#x27;\t\tInteger parallelism = &#x27;
                         &#x27;kafkaSourceTableInfo.getParallelism();\n&#x27;
                         &#x27;\t\tif (parallelism &gt; 0) {\n&#x27;
                         &#x27;\t\t\tkafkaSource.setParallelism(parallelism);\n&#x27;
                         &#x27;\t\t}\n&#x27;
                         &#x27;\t\treturn tableEnv.fromDataStream(kafkaSource, &#x27;
                         &#x27;fields);\n&#x27;
                         &#x27;\t}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        return &#x27;
                           &#x27;tableEnv.fromDataStream(kafkaSource, fields);\n&#x27;
                           &#x27;    }\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;\t\tInteger parallelism = &#x27;
                         &#x27;kafkaSourceTableInfo.getParallelism();\n&#x27;
                         &#x27;\t\tif (parallelism &gt; 0) {\n&#x27;
                         &#x27;\t\t\tkafkaSource.setParallelism(parallelism);\n&#x27;
                         &#x27;\t\t}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        &#x27;
                           &#x27;kafkaSource.setParallelism(kafkaSourceTableInfo.getParallelism());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;setStartPosition(kafkaSourceTableInfo.getOffsetReset(), &#x27;
                           &#x27;topicName, kafkaSrc);\n&#x27;
                           &#x27;        String fields = &#x27;
                           &#x27;StringUtils.join(kafkaSourceTableInfo.getFields(), &#x27;
                           &#x27;&quot;,&quot;);\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;jfstmerge&#x27;}}]]</pre>
          </body>
        </html>
        