<!DOCTYPE html>
<html lang="en">
          <head>
            <meta charset="utf-8">
            <title>574 chunks</title>
                <style>
                    #top {
                        height: 48vh;
                        overflow-y: auto;
                    }
                    #bottom {
                        height: 48vh;
                        overflow-y: auto;
                    }
                </style>
          </head>
          <body>
            <pre>[[{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.KafkaTableSinkBase;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        KafkaTableSinkBase kafkaTableSink = new &#x27;
                         &#x27;CustomerKafka11JsonTableSink(\n&#x27;
                         &#x27;                schema,\n&#x27;
                         &#x27;                topic,\n&#x27;
                         &#x27;                properties,\n&#x27;
                         &#x27;                partitioner,\n&#x27;
                         &#x27;                serializationSchema\n&#x27;
                         &#x27;        );\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataStream&lt;Row&gt; ds = dataStream\n&#x27;
                         &#x27;                .filter((Tuple2&lt;Boolean, Row&gt; &#x27;
                         &#x27;record) -&gt; record.f0)\n&#x27;
                         &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                         &#x27;-&gt; {return record.f1;})\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        DataStream&lt;Row&gt; mapDataStream = &#x27;
                           &#x27;dataStream.filter((Tuple2&lt;Boolean, Row&gt; record) -&gt; &#x27;
                           &#x27;record.f0)\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; record.f1)\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        KafkaTableSinkBase kafkaTableSink = new &#x27;
                         &#x27;CustomerKafka11JsonTableSink(\n&#x27;
                         &#x27;                schema,\n&#x27;
                         &#x27;                topic,\n&#x27;
                         &#x27;                properties,\n&#x27;
                         &#x27;                partitioner,\n&#x27;
                         &#x27;                serializationSchema\n&#x27;
                         &#x27;        );\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataStream&lt;Row&gt; ds = dataStream\n&#x27;
                         &#x27;                .filter((Tuple2&lt;Boolean, Row&gt; &#x27;
                         &#x27;record) -&gt; record.f0)\n&#x27;
                         &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                         &#x27;-&gt; {return record.f1;})\n&#x27;
                         &#x27;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;
                           &#x27;        DataStream&lt;Row&gt; mapDataStream = &#x27;
                           &#x27;dataStream.filter((Tuple2&lt;Boolean, Row&gt; record) -&gt; &#x27;
                           &#x27;record.f0)\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; record.f1)\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;, &#x27;spork&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;        DataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;(DataStreamSink&lt;Row&gt;) &#x27;
                         &#x27;kafkaTableSink.consumeDataStream(ds);\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer011).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                           &#x27;getFieldNames()));\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;        DataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;(DataStreamSink&lt;Row&gt;) &#x27;
                         &#x27;kafkaTableSink.consumeDataStream(ds);\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;
                         &#x27;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer011).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                           &#x27;getFieldNames()));\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;, &#x27;spork&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;        consumeDataStream(dataStream);\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        DataStream&lt;Row&gt; mapDataStream = &#x27;
                           &#x27;dataStream.filter((Tuple2&lt;Boolean, Row&gt; record) -&gt; &#x27;
                           &#x27;record.f0)\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; record.f1)\n&#x27;
                           &#x27;                &#x27;
                           &#x27;.returns(getOutputType().getTypeAt(1))\n&#x27;
                           &#x27;                .setParallelism(parallelism);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer011).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                           &#x27;getFieldNames()));\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;jfstmerge&#x27;}}],
 [{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.KafkaTableSinkBase;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        KafkaTableSinkBase kafkaTableSink = new &#x27;
                         &#x27;CustomerKafka11JsonTableSink(\n&#x27;
                         &#x27;                schema,\n&#x27;
                         &#x27;                topic,\n&#x27;
                         &#x27;                properties,\n&#x27;
                         &#x27;                partitioner,\n&#x27;
                         &#x27;                serializationSchema\n&#x27;
                         &#x27;        );\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataStream&lt;Row&gt; ds = dataStream\n&#x27;
                         &#x27;                .filter((Tuple2&lt;Boolean, Row&gt; &#x27;
                         &#x27;record) -&gt; record.f0)\n&#x27;
                         &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                         &#x27;-&gt; {return record.f1;})\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        DataStream&lt;Row&gt; mapDataStream = &#x27;
                           &#x27;dataStream.filter((Tuple2&lt;Boolean, Row&gt; record) -&gt; &#x27;
                           &#x27;record.f0)\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; record.f1)\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        KafkaTableSinkBase kafkaTableSink = new &#x27;
                         &#x27;CustomerKafka11JsonTableSink(\n&#x27;
                         &#x27;                schema,\n&#x27;
                         &#x27;                topic,\n&#x27;
                         &#x27;                properties,\n&#x27;
                         &#x27;                partitioner,\n&#x27;
                         &#x27;                serializationSchema\n&#x27;
                         &#x27;        );\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataStream&lt;Row&gt; ds = dataStream\n&#x27;
                         &#x27;                .filter((Tuple2&lt;Boolean, Row&gt; &#x27;
                         &#x27;record) -&gt; record.f0)\n&#x27;
                         &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                         &#x27;-&gt; {return record.f1;})\n&#x27;
                         &#x27;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;
                           &#x27;        DataStream&lt;Row&gt; mapDataStream = &#x27;
                           &#x27;dataStream.filter((Tuple2&lt;Boolean, Row&gt; record) -&gt; &#x27;
                           &#x27;record.f0)\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; record.f1)\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;, &#x27;spork&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;        DataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;(DataStreamSink&lt;Row&gt;) &#x27;
                         &#x27;kafkaTableSink.consumeDataStream(ds);\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer011).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                           &#x27;getFieldNames()));\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;        DataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;(DataStreamSink&lt;Row&gt;) &#x27;
                         &#x27;kafkaTableSink.consumeDataStream(ds);\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;
                         &#x27;\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer011).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                           &#x27;getFieldNames()));\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;, &#x27;spork&#x27;}},
  {&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;        consumeDataStream(dataStream);\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;        DataStream&lt;Row&gt; mapDataStream = &#x27;
                           &#x27;dataStream.filter((Tuple2&lt;Boolean, Row&gt; record) -&gt; &#x27;
                           &#x27;record.f0)\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; record.f1)\n&#x27;
                           &#x27;                &#x27;
                           &#x27;.returns(getOutputType().getTypeAt(1))\n&#x27;
                           &#x27;                .setParallelism(parallelism);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer011).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                           &#x27;getFieldNames()));\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;jfstmerge&#x27;}}]]</pre>
          </body>
        </html>
        