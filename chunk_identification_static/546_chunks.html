<!DOCTYPE html>
<html lang="en">
          <head>
            <meta charset="utf-8">
            <title>546 chunks</title>
                <style>
                    #top {
                        height: 48vh;
                        overflow-y: auto;
                    }
                    #bottom {
                        height: 48vh;
                        overflow-y: auto;
                    }
                </style>
          </head>
          <body>
            <pre>[[{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.types.DataType;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;import java.util.stream.IntStream;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import static &#x27;
                         &#x27;org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; * Date: 2020/3/30\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public abstract class AbstractKafkaSink implements &#x27;
                         &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                         &#x27;    public static final String &#x27;
                         &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] fieldNames;\n&#x27;
                         &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] partitionKeys;\n&#x27;
                         &#x27;    protected String sinkOperatorName;\n&#x27;
                         &#x27;    protected Properties properties;\n&#x27;
                         &#x27;    protected int parallelism;\n&#x27;
                         &#x27;    protected String topic;\n&#x27;
                         &#x27;    protected String tableName;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema schema;\n&#x27;
                         &#x27;    protected SinkFunction&lt;Tuple2&lt;Boolean,Row&gt;&gt; &#x27;
                         &#x27;kafkaProducer011;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Tuple2&lt;Boolean,Row&gt;&gt;&gt; &#x27;
                         &#x27;partitioner;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected Properties &#x27;
                         &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                         &#x27;KafkaSinkTableInfo) {\n&#x27;
                         &#x27;        Properties props = new Properties();\n&#x27;
                         &#x27;        &#x27;
                         &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                         &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        for (String key : &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                         &#x27;            props.setProperty(key, &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return props;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TypeInformation[] &#x27;
                         &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                         &#x27;kafka11SinkTableInfo) {\n&#x27;
                         &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                         &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                         &#x27;        TypeInformation[] types = IntStream.range(0, &#x27;
                         &#x27;fieldClasses.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                         &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                         &#x27;        return types;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema buildTableSchema(String[] &#x27;
                         &#x27;fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                         &#x27;        &#x27;
                         &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                         &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                         &#x27;fieldTypes length !&quot;);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataType[] dataTypes = IntStream.range(0, &#x27;
                         &#x27;fieldTypes.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;fromLegacyInfoToDataType(fieldTypes[i]))\n&#x27;
                         &#x27;                .toArray(DataType[]::new);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        TableSchema tableSchema = &#x27;
                         &#x27;TableSchema.builder()\n&#x27;
                         &#x27;                .fields(fieldNames, dataTypes)\n&#x27;
                         &#x27;                .build();\n&#x27;
                         &#x27;        return tableSchema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] &#x27;
                         &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                         &#x27;kafkaSinkTableInfo) {\n&#x27;
                         &#x27;        if &#x27;
                         &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;            return &#x27;
                         &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                         &quot;&#x27;,&#x27;);\n&quot;
                         &#x27;        }\n&#x27;
                         &#x27;        return null;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer011).name(sinkOperatorName);\n&#x27;
                         &#x27;        if (parallelism &gt; 0) {\n&#x27;
                         &#x27;            &#x27;
                         &#x27;dataStreamSink.setParallelism(parallelism);\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public void &#x27;
                         &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;configure(String[] fieldNames, TypeInformation&lt;?&gt;[] &#x27;
                         &#x27;fieldTypes) {\n&#x27;
                         &#x27;        this.fieldNames = fieldNames;\n&#x27;
                         &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                         &#x27;        return this;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;getOutputType() {\n&#x27;
                         &#x27;        return new &#x27;
                         &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                         &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSchema getTableSchema() {\n&#x27;
                         &#x27;        return schema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                         &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                         &#x27;fieldNames);\n&#x27;
                         &#x27;    }\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                           &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.runtime.types.CRowTypeInfo;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                           &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.util.Optional;\n&#x27;
                           &#x27;import java.util.Properties;\n&#x27;
                           &#x27;import java.util.stream.IntStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Date: 2020/4/1\n&#x27;
                           &#x27; * Company: www.dtstack.com\n&#x27;
                           &#x27; * @author maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public abstract class AbstractKafkaSink implements &#x27;
                           &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public static final String &#x27;
                           &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] fieldNames;\n&#x27;
                           &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] partitionKeys;\n&#x27;
                           &#x27;    protected String sinkOperatorName;\n&#x27;
                           &#x27;    protected Properties properties;\n&#x27;
                           &#x27;    protected int parallelism;\n&#x27;
                           &#x27;    protected String topic;\n&#x27;
                           &#x27;    protected String tableName;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema schema;\n&#x27;
                           &#x27;    protected SinkFunction&lt;CRow&gt; kafkaProducer;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected &#x27;
                           &#x27;Optional&lt;FlinkKafkaPartitioner&lt;CRow&gt;&gt; &#x27;
                           &#x27;partitioner;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected Properties &#x27;
                           &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                           &#x27;KafkaSinkTableInfo) {\n&#x27;
                           &#x27;        Properties props = new Properties();\n&#x27;
                           &#x27;        &#x27;
                           &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                           &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        for (String key : &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                           &#x27;            props.setProperty(key, &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                           &#x27;        }\n&#x27;
                           &#x27;        return props;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TypeInformation[] &#x27;
                           &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                           &#x27;kafka11SinkTableInfo) {\n&#x27;
                           &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                           &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                           &#x27;        TypeInformation[] types = &#x27;
                           &#x27;IntStream.range(0, fieldClasses.length)\n&#x27;
                           &#x27;                .mapToObj(i -&gt; &#x27;
                           &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                           &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                           &#x27;        return types;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema &#x27;
                           &#x27;buildTableSchema(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        &#x27;
                           &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                           &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                           &#x27;fieldTypes length !&quot;);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        TableSchema.Builder builder = &#x27;
                           &#x27;TableSchema.builder();\n&#x27;
                           &#x27;        IntStream.range(0, fieldTypes.length)\n&#x27;
                           &#x27;                .forEach(i -&gt; &#x27;
                           &#x27;builder.field(fieldNames[i], fieldTypes[i]));\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        return builder.build();\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public void &#x27;
                           &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;dataStream) {\n&#x27;
                           &#x27;        DataStream&lt;CRow&gt; mapDataStream = &#x27;
                           &#x27;dataStream\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; new CRow(record.f1, record.f0))\n&#x27;
                           &#x27;                .returns(getRowTypeInfo())\n&#x27;
                           &#x27;                .setParallelism(parallelism);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer).name(sinkOperatorName);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public CRowTypeInfo getRowTypeInfo() {\n&#x27;
                           &#x27;        return new CRowTypeInfo(new &#x27;
                           &#x27;RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] &#x27;
                           &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                           &#x27;kafkaSinkTableInfo) {\n&#x27;
                           &#x27;        if &#x27;
                           &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;            return &#x27;
                           &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                           &quot;&#x27;,&#x27;);\n&quot;
                           &#x27;        }\n&#x27;
                           &#x27;        return null;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;getOutputType() {\n&#x27;
                           &#x27;        return new &#x27;
                           &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                           &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public String[] getFieldNames() {\n&#x27;
                           &#x27;        return fieldNames;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;?&gt;[] getFieldTypes() {\n&#x27;
                           &#x27;        return fieldTypes;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;configure(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        this.fieldNames = fieldNames;\n&#x27;
                           &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                           &#x27;        return this;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                           &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                           &#x27;fieldNames);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.types.DataType;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;import java.util.stream.IntStream;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import static &#x27;
                         &#x27;org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; * Date: 2020/3/30\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public abstract class AbstractKafkaSink implements &#x27;
                         &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                         &#x27;    public static final String &#x27;
                         &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] fieldNames;\n&#x27;
                         &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] partitionKeys;\n&#x27;
                         &#x27;    protected String sinkOperatorName;\n&#x27;
                         &#x27;    protected Properties properties;\n&#x27;
                         &#x27;    protected int parallelism;\n&#x27;
                         &#x27;    protected String topic;\n&#x27;
                         &#x27;    protected String tableName;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema schema;\n&#x27;
                         &#x27;    protected SinkFunction&lt;Tuple2&lt;Boolean,Row&gt;&gt; &#x27;
                         &#x27;kafkaProducer011;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Tuple2&lt;Boolean,Row&gt;&gt;&gt; &#x27;
                         &#x27;partitioner;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected Properties &#x27;
                         &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                         &#x27;KafkaSinkTableInfo) {\n&#x27;
                         &#x27;        Properties props = new Properties();\n&#x27;
                         &#x27;        &#x27;
                         &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                         &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        for (String key : &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                         &#x27;            props.setProperty(key, &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return props;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TypeInformation[] &#x27;
                         &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                         &#x27;kafka11SinkTableInfo) {\n&#x27;
                         &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                         &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                         &#x27;        TypeInformation[] types = IntStream.range(0, &#x27;
                         &#x27;fieldClasses.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                         &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                         &#x27;        return types;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema buildTableSchema(String[] &#x27;
                         &#x27;fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                         &#x27;        &#x27;
                         &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                         &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                         &#x27;fieldTypes length !&quot;);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataType[] dataTypes = IntStream.range(0, &#x27;
                         &#x27;fieldTypes.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;fromLegacyInfoToDataType(fieldTypes[i]))\n&#x27;
                         &#x27;                .toArray(DataType[]::new);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        TableSchema tableSchema = &#x27;
                         &#x27;TableSchema.builder()\n&#x27;
                         &#x27;                .fields(fieldNames, dataTypes)\n&#x27;
                         &#x27;                .build();\n&#x27;
                         &#x27;        return tableSchema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] &#x27;
                         &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                         &#x27;kafkaSinkTableInfo) {\n&#x27;
                         &#x27;        if &#x27;
                         &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;            return &#x27;
                         &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                         &quot;&#x27;,&#x27;);\n&quot;
                         &#x27;        }\n&#x27;
                         &#x27;        return null;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer011).name(sinkOperatorName);\n&#x27;
                         &#x27;        if (parallelism &gt; 0) {\n&#x27;
                         &#x27;            &#x27;
                         &#x27;dataStreamSink.setParallelism(parallelism);\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public void &#x27;
                         &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;configure(String[] fieldNames, TypeInformation&lt;?&gt;[] &#x27;
                         &#x27;fieldTypes) {\n&#x27;
                         &#x27;        this.fieldNames = fieldNames;\n&#x27;
                         &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                         &#x27;        return this;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;getOutputType() {\n&#x27;
                         &#x27;        return new &#x27;
                         &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                         &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSchema getTableSchema() {\n&#x27;
                         &#x27;        return schema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                         &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                         &#x27;fieldNames);\n&#x27;
                         &#x27;    }\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                           &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.runtime.types.CRowTypeInfo;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                           &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.util.Optional;\n&#x27;
                           &#x27;import java.util.Properties;\n&#x27;
                           &#x27;import java.util.stream.IntStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Date: 2020/4/1\n&#x27;
                           &#x27; * Company: www.dtstack.com\n&#x27;
                           &#x27; * @author maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public abstract class AbstractKafkaSink implements &#x27;
                           &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public static final String &#x27;
                           &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] fieldNames;\n&#x27;
                           &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] partitionKeys;\n&#x27;
                           &#x27;    protected String sinkOperatorName;\n&#x27;
                           &#x27;    protected Properties properties;\n&#x27;
                           &#x27;    protected int parallelism;\n&#x27;
                           &#x27;    protected String topic;\n&#x27;
                           &#x27;    protected String tableName;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema schema;\n&#x27;
                           &#x27;    protected SinkFunction&lt;CRow&gt; kafkaProducer;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected &#x27;
                           &#x27;Optional&lt;FlinkKafkaPartitioner&lt;CRow&gt;&gt; &#x27;
                           &#x27;partitioner;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected Properties &#x27;
                           &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                           &#x27;KafkaSinkTableInfo) {\n&#x27;
                           &#x27;        Properties props = new Properties();\n&#x27;
                           &#x27;        &#x27;
                           &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                           &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        for (String key : &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                           &#x27;            props.setProperty(key, &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                           &#x27;        }\n&#x27;
                           &#x27;        return props;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TypeInformation[] &#x27;
                           &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                           &#x27;kafka11SinkTableInfo) {\n&#x27;
                           &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                           &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                           &#x27;        TypeInformation[] types = &#x27;
                           &#x27;IntStream.range(0, fieldClasses.length)\n&#x27;
                           &#x27;                .mapToObj(i -&gt; &#x27;
                           &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                           &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                           &#x27;        return types;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema &#x27;
                           &#x27;buildTableSchema(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        &#x27;
                           &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                           &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                           &#x27;fieldTypes length !&quot;);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        TableSchema.Builder builder = &#x27;
                           &#x27;TableSchema.builder();\n&#x27;
                           &#x27;        IntStream.range(0, fieldTypes.length)\n&#x27;
                           &#x27;                .forEach(i -&gt; &#x27;
                           &#x27;builder.field(fieldNames[i], fieldTypes[i]));\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        return builder.build();\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public void &#x27;
                           &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;dataStream) {\n&#x27;
                           &#x27;        DataStream&lt;CRow&gt; mapDataStream = &#x27;
                           &#x27;dataStream\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; new CRow(record.f1, record.f0))\n&#x27;
                           &#x27;                .returns(getRowTypeInfo())\n&#x27;
                           &#x27;                .setParallelism(parallelism);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer).name(sinkOperatorName);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public CRowTypeInfo getRowTypeInfo() {\n&#x27;
                           &#x27;        return new CRowTypeInfo(new &#x27;
                           &#x27;RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] &#x27;
                           &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                           &#x27;kafkaSinkTableInfo) {\n&#x27;
                           &#x27;        if &#x27;
                           &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;            return &#x27;
                           &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                           &quot;&#x27;,&#x27;);\n&quot;
                           &#x27;        }\n&#x27;
                           &#x27;        return null;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;getOutputType() {\n&#x27;
                           &#x27;        return new &#x27;
                           &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                           &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public String[] getFieldNames() {\n&#x27;
                           &#x27;        return fieldNames;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;?&gt;[] getFieldTypes() {\n&#x27;
                           &#x27;        return fieldTypes;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;configure(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        this.fieldNames = fieldNames;\n&#x27;
                           &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                           &#x27;        return this;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                           &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                           &#x27;fieldNames);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;, &#x27;jfstmerge&#x27;}}],
 [{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.types.DataType;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;import java.util.stream.IntStream;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import static &#x27;
                         &#x27;org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; * Date: 2020/3/30\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public abstract class AbstractKafkaSink implements &#x27;
                         &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                         &#x27;    public static final String &#x27;
                         &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] fieldNames;\n&#x27;
                         &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] partitionKeys;\n&#x27;
                         &#x27;    protected String sinkOperatorName;\n&#x27;
                         &#x27;    protected Properties properties;\n&#x27;
                         &#x27;    protected int parallelism;\n&#x27;
                         &#x27;    protected String topic;\n&#x27;
                         &#x27;    protected String tableName;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema schema;\n&#x27;
                         &#x27;    protected SinkFunction&lt;Tuple2&lt;Boolean,Row&gt;&gt; &#x27;
                         &#x27;kafkaProducer011;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Tuple2&lt;Boolean,Row&gt;&gt;&gt; &#x27;
                         &#x27;partitioner;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected Properties &#x27;
                         &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                         &#x27;KafkaSinkTableInfo) {\n&#x27;
                         &#x27;        Properties props = new Properties();\n&#x27;
                         &#x27;        &#x27;
                         &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                         &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        for (String key : &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                         &#x27;            props.setProperty(key, &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return props;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TypeInformation[] &#x27;
                         &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                         &#x27;kafka11SinkTableInfo) {\n&#x27;
                         &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                         &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                         &#x27;        TypeInformation[] types = IntStream.range(0, &#x27;
                         &#x27;fieldClasses.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                         &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                         &#x27;        return types;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema buildTableSchema(String[] &#x27;
                         &#x27;fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                         &#x27;        &#x27;
                         &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                         &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                         &#x27;fieldTypes length !&quot;);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataType[] dataTypes = IntStream.range(0, &#x27;
                         &#x27;fieldTypes.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;fromLegacyInfoToDataType(fieldTypes[i]))\n&#x27;
                         &#x27;                .toArray(DataType[]::new);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        TableSchema tableSchema = &#x27;
                         &#x27;TableSchema.builder()\n&#x27;
                         &#x27;                .fields(fieldNames, dataTypes)\n&#x27;
                         &#x27;                .build();\n&#x27;
                         &#x27;        return tableSchema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] &#x27;
                         &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                         &#x27;kafkaSinkTableInfo) {\n&#x27;
                         &#x27;        if &#x27;
                         &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;            return &#x27;
                         &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                         &quot;&#x27;,&#x27;);\n&quot;
                         &#x27;        }\n&#x27;
                         &#x27;        return null;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer011).name(sinkOperatorName);\n&#x27;
                         &#x27;        if (parallelism &gt; 0) {\n&#x27;
                         &#x27;            &#x27;
                         &#x27;dataStreamSink.setParallelism(parallelism);\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public void &#x27;
                         &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;configure(String[] fieldNames, TypeInformation&lt;?&gt;[] &#x27;
                         &#x27;fieldTypes) {\n&#x27;
                         &#x27;        this.fieldNames = fieldNames;\n&#x27;
                         &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                         &#x27;        return this;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;getOutputType() {\n&#x27;
                         &#x27;        return new &#x27;
                         &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                         &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSchema getTableSchema() {\n&#x27;
                         &#x27;        return schema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                         &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                         &#x27;fieldNames);\n&#x27;
                         &#x27;    }\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                           &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.runtime.types.CRowTypeInfo;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                           &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.util.Optional;\n&#x27;
                           &#x27;import java.util.Properties;\n&#x27;
                           &#x27;import java.util.stream.IntStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Date: 2020/4/1\n&#x27;
                           &#x27; * Company: www.dtstack.com\n&#x27;
                           &#x27; * @author maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public abstract class AbstractKafkaSink implements &#x27;
                           &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public static final String &#x27;
                           &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] fieldNames;\n&#x27;
                           &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] partitionKeys;\n&#x27;
                           &#x27;    protected String sinkOperatorName;\n&#x27;
                           &#x27;    protected Properties properties;\n&#x27;
                           &#x27;    protected int parallelism;\n&#x27;
                           &#x27;    protected String topic;\n&#x27;
                           &#x27;    protected String tableName;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema schema;\n&#x27;
                           &#x27;    protected SinkFunction&lt;CRow&gt; kafkaProducer;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected &#x27;
                           &#x27;Optional&lt;FlinkKafkaPartitioner&lt;CRow&gt;&gt; &#x27;
                           &#x27;partitioner;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected Properties &#x27;
                           &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                           &#x27;KafkaSinkTableInfo) {\n&#x27;
                           &#x27;        Properties props = new Properties();\n&#x27;
                           &#x27;        &#x27;
                           &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                           &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        for (String key : &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                           &#x27;            props.setProperty(key, &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                           &#x27;        }\n&#x27;
                           &#x27;        return props;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TypeInformation[] &#x27;
                           &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                           &#x27;kafka11SinkTableInfo) {\n&#x27;
                           &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                           &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                           &#x27;        TypeInformation[] types = &#x27;
                           &#x27;IntStream.range(0, fieldClasses.length)\n&#x27;
                           &#x27;                .mapToObj(i -&gt; &#x27;
                           &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                           &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                           &#x27;        return types;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema &#x27;
                           &#x27;buildTableSchema(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        &#x27;
                           &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                           &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                           &#x27;fieldTypes length !&quot;);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        TableSchema.Builder builder = &#x27;
                           &#x27;TableSchema.builder();\n&#x27;
                           &#x27;        IntStream.range(0, fieldTypes.length)\n&#x27;
                           &#x27;                .forEach(i -&gt; &#x27;
                           &#x27;builder.field(fieldNames[i], fieldTypes[i]));\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        return builder.build();\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public void &#x27;
                           &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;dataStream) {\n&#x27;
                           &#x27;        DataStream&lt;CRow&gt; mapDataStream = &#x27;
                           &#x27;dataStream\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; new CRow(record.f1, record.f0))\n&#x27;
                           &#x27;                .returns(getRowTypeInfo())\n&#x27;
                           &#x27;                .setParallelism(parallelism);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer).name(sinkOperatorName);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public CRowTypeInfo getRowTypeInfo() {\n&#x27;
                           &#x27;        return new CRowTypeInfo(new &#x27;
                           &#x27;RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] &#x27;
                           &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                           &#x27;kafkaSinkTableInfo) {\n&#x27;
                           &#x27;        if &#x27;
                           &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;            return &#x27;
                           &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                           &quot;&#x27;,&#x27;);\n&quot;
                           &#x27;        }\n&#x27;
                           &#x27;        return null;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;getOutputType() {\n&#x27;
                           &#x27;        return new &#x27;
                           &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                           &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public String[] getFieldNames() {\n&#x27;
                           &#x27;        return fieldNames;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;?&gt;[] getFieldTypes() {\n&#x27;
                           &#x27;        return fieldTypes;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;configure(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        this.fieldNames = fieldNames;\n&#x27;
                           &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                           &#x27;        return this;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                           &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                           &#x27;fieldNames);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                         &#x27;import org.apache.flink.table.types.DataType;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;import java.util.stream.IntStream;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import static &#x27;
                         &#x27;org.apache.flink.table.types.utils.TypeConversions.fromLegacyInfoToDataType;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; * Date: 2020/3/30\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public abstract class AbstractKafkaSink implements &#x27;
                         &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                         &#x27;    public static final String &#x27;
                         &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] fieldNames;\n&#x27;
                         &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] partitionKeys;\n&#x27;
                         &#x27;    protected String sinkOperatorName;\n&#x27;
                         &#x27;    protected Properties properties;\n&#x27;
                         &#x27;    protected int parallelism;\n&#x27;
                         &#x27;    protected String topic;\n&#x27;
                         &#x27;    protected String tableName;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema schema;\n&#x27;
                         &#x27;    protected SinkFunction&lt;Tuple2&lt;Boolean,Row&gt;&gt; &#x27;
                         &#x27;kafkaProducer011;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Tuple2&lt;Boolean,Row&gt;&gt;&gt; &#x27;
                         &#x27;partitioner;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected Properties &#x27;
                         &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                         &#x27;KafkaSinkTableInfo) {\n&#x27;
                         &#x27;        Properties props = new Properties();\n&#x27;
                         &#x27;        &#x27;
                         &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                         &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        for (String key : &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                         &#x27;            props.setProperty(key, &#x27;
                         &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return props;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TypeInformation[] &#x27;
                         &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                         &#x27;kafka11SinkTableInfo) {\n&#x27;
                         &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                         &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                         &#x27;        TypeInformation[] types = IntStream.range(0, &#x27;
                         &#x27;fieldClasses.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                         &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                         &#x27;        return types;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected TableSchema buildTableSchema(String[] &#x27;
                         &#x27;fieldNames, TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                         &#x27;        &#x27;
                         &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                         &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                         &#x27;fieldTypes length !&quot;);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        DataType[] dataTypes = IntStream.range(0, &#x27;
                         &#x27;fieldTypes.length)\n&#x27;
                         &#x27;                .mapToObj(i -&gt; &#x27;
                         &#x27;fromLegacyInfoToDataType(fieldTypes[i]))\n&#x27;
                         &#x27;                .toArray(DataType[]::new);\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;        TableSchema tableSchema = &#x27;
                         &#x27;TableSchema.builder()\n&#x27;
                         &#x27;                .fields(fieldNames, dataTypes)\n&#x27;
                         &#x27;                .build();\n&#x27;
                         &#x27;        return tableSchema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    protected String[] &#x27;
                         &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                         &#x27;kafkaSinkTableInfo) {\n&#x27;
                         &#x27;        if &#x27;
                         &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                         &#x27;{\n&#x27;
                         &#x27;            return &#x27;
                         &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                         &quot;&#x27;,&#x27;);\n&quot;
                         &#x27;        }\n&#x27;
                         &#x27;        return null;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        DataStreamSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer011).name(sinkOperatorName);\n&#x27;
                         &#x27;        if (parallelism &gt; 0) {\n&#x27;
                         &#x27;            &#x27;
                         &#x27;dataStreamSink.setParallelism(parallelism);\n&#x27;
                         &#x27;        }\n&#x27;
                         &#x27;        return dataStreamSink;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public void &#x27;
                         &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;        consumeDataStream(dataStream);\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;configure(String[] fieldNames, TypeInformation&lt;?&gt;[] &#x27;
                         &#x27;fieldTypes) {\n&#x27;
                         &#x27;        this.fieldNames = fieldNames;\n&#x27;
                         &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                         &#x27;        return this;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                         &#x27;getOutputType() {\n&#x27;
                         &#x27;        return new &#x27;
                         &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                         &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TableSchema getTableSchema() {\n&#x27;
                         &#x27;        return schema;\n&#x27;
                         &#x27;    }\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;    @Override\n&#x27;
                         &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                         &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                         &#x27;fieldNames);\n&#x27;
                         &#x27;    }\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                           &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.runtime.types.CRowTypeInfo;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.table.sinks.RetractStreamTableSink;\n&#x27;
                           &#x27;import org.apache.flink.table.sinks.TableSink;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.kafka.clients.consumer.ConsumerConfig;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.util.Optional;\n&#x27;
                           &#x27;import java.util.Properties;\n&#x27;
                           &#x27;import java.util.stream.IntStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Date: 2020/4/1\n&#x27;
                           &#x27; * Company: www.dtstack.com\n&#x27;
                           &#x27; * @author maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public abstract class AbstractKafkaSink implements &#x27;
                           &#x27;RetractStreamTableSink&lt;Row&gt;, IStreamSinkGener {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public static final String &#x27;
                           &#x27;SINK_OPERATOR_NAME_TPL = &quot;${topic}_${table}&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] fieldNames;\n&#x27;
                           &#x27;    protected TypeInformation&lt;?&gt;[] fieldTypes;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] partitionKeys;\n&#x27;
                           &#x27;    protected String sinkOperatorName;\n&#x27;
                           &#x27;    protected Properties properties;\n&#x27;
                           &#x27;    protected int parallelism;\n&#x27;
                           &#x27;    protected String topic;\n&#x27;
                           &#x27;    protected String tableName;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema schema;\n&#x27;
                           &#x27;    protected SinkFunction&lt;CRow&gt; kafkaProducer;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected &#x27;
                           &#x27;Optional&lt;FlinkKafkaPartitioner&lt;CRow&gt;&gt; &#x27;
                           &#x27;partitioner;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected Properties &#x27;
                           &#x27;getKafkaProperties(KafkaSinkTableInfo &#x27;
                           &#x27;KafkaSinkTableInfo) {\n&#x27;
                           &#x27;        Properties props = new Properties();\n&#x27;
                           &#x27;        &#x27;
                           &#x27;props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, &#x27;
                           &#x27;KafkaSinkTableInfo.getBootstrapServers());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        for (String key : &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParamKeys()) {\n&#x27;
                           &#x27;            props.setProperty(key, &#x27;
                           &#x27;KafkaSinkTableInfo.getKafkaParam(key));\n&#x27;
                           &#x27;        }\n&#x27;
                           &#x27;        return props;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TypeInformation[] &#x27;
                           &#x27;getTypeInformations(KafkaSinkTableInfo &#x27;
                           &#x27;kafka11SinkTableInfo) {\n&#x27;
                           &#x27;        Class&lt;?&gt;[] fieldClasses = &#x27;
                           &#x27;kafka11SinkTableInfo.getFieldClasses();\n&#x27;
                           &#x27;        TypeInformation[] types = &#x27;
                           &#x27;IntStream.range(0, fieldClasses.length)\n&#x27;
                           &#x27;                .mapToObj(i -&gt; &#x27;
                           &#x27;TypeInformation.of(fieldClasses[i]))\n&#x27;
                           &#x27;                .toArray(TypeInformation[]::new);\n&#x27;
                           &#x27;        return types;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected TableSchema &#x27;
                           &#x27;buildTableSchema(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        &#x27;
                           &#x27;Preconditions.checkArgument(fieldNames.length == &#x27;
                           &#x27;fieldTypes.length, &quot;fieldNames length must equals &#x27;
                           &#x27;fieldTypes length !&quot;);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        TableSchema.Builder builder = &#x27;
                           &#x27;TableSchema.builder();\n&#x27;
                           &#x27;        IntStream.range(0, fieldTypes.length)\n&#x27;
                           &#x27;                .forEach(i -&gt; &#x27;
                           &#x27;builder.field(fieldNames[i], fieldTypes[i]));\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        return builder.build();\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public void &#x27;
                           &#x27;emitDataStream(DataStream&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;dataStream) {\n&#x27;
                           &#x27;        DataStream&lt;CRow&gt; mapDataStream = &#x27;
                           &#x27;dataStream\n&#x27;
                           &#x27;                .map((Tuple2&lt;Boolean, Row&gt; record) &#x27;
                           &#x27;-&gt; new CRow(record.f1, record.f0))\n&#x27;
                           &#x27;                .returns(getRowTypeInfo())\n&#x27;
                           &#x27;                .setParallelism(parallelism);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;        &#x27;
                           &#x27;mapDataStream.addSink(kafkaProducer).name(sinkOperatorName);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    public CRowTypeInfo getRowTypeInfo() {\n&#x27;
                           &#x27;        return new CRowTypeInfo(new &#x27;
                           &#x27;RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    protected String[] &#x27;
                           &#x27;getPartitionKeys(KafkaSinkTableInfo &#x27;
                           &#x27;kafkaSinkTableInfo) {\n&#x27;
                           &#x27;        if &#x27;
                           &#x27;(StringUtils.isNotBlank(kafkaSinkTableInfo.getPartitionKeys())) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;            return &#x27;
                           &#x27;StringUtils.split(kafkaSinkTableInfo.getPartitionKeys(), &#x27;
                           &quot;&#x27;,&#x27;);\n&quot;
                           &#x27;        }\n&#x27;
                           &#x27;        return null;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TupleTypeInfo&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;getOutputType() {\n&#x27;
                           &#x27;        return new &#x27;
                           &#x27;TupleTypeInfo(org.apache.flink.table.api.Types.BOOLEAN(), &#x27;
                           &#x27;new RowTypeInfo(fieldTypes, fieldNames));\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public String[] getFieldNames() {\n&#x27;
                           &#x27;        return fieldNames;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;?&gt;[] getFieldTypes() {\n&#x27;
                           &#x27;        return fieldTypes;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TableSink&lt;Tuple2&lt;Boolean, Row&gt;&gt; &#x27;
                           &#x27;configure(String[] fieldNames, &#x27;
                           &#x27;TypeInformation&lt;?&gt;[] fieldTypes) {\n&#x27;
                           &#x27;        this.fieldNames = fieldNames;\n&#x27;
                           &#x27;        this.fieldTypes = fieldTypes;\n&#x27;
                           &#x27;        return this;\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;    @Override\n&#x27;
                           &#x27;    public TypeInformation&lt;Row&gt; getRecordType() {\n&#x27;
                           &#x27;        return new RowTypeInfo(fieldTypes, &#x27;
                           &#x27;fieldNames);\n&#x27;
                           &#x27;    }\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;baseline&#x27;, &#x27;jfstmerge&#x27;}}]]</pre>
          </body>
        </html>
        