<!DOCTYPE html>
<html lang="en">
          <head>
            <meta charset="utf-8">
            <title>522 chunks</title>
                <style>
                    #top {
                        height: 48vh;
                        overflow-y: auto;
                    }
                    #bottom {
                        height: 48vh;
                        overflow-y: auto;
                    }
                </style>
          </head>
          <body>
            <pre>[[{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;/*\n&#x27;
                           &#x27; * Licensed to the Apache Software Foundation &#x27;
                           &#x27;(ASF) under one\n&#x27;
                           &#x27; * or more contributor license agreements.  See &#x27;
                           &#x27;the NOTICE file\n&#x27;
                           &#x27; * distributed with this work for additional &#x27;
                           &#x27;information\n&#x27;
                           &#x27; * regarding copyright ownership.  The ASF &#x27;
                           &#x27;licenses this file\n&#x27;
                           &#x27; * to you under the Apache License, Version 2.0 &#x27;
                           &#x27;(the\n&#x27;
                           &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                           &#x27;compliance\n&#x27;
                           &#x27; * with the License.  You may obtain a copy of the &#x27;
                           &#x27;License at\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; *     http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * Unless required by applicable law or agreed to &#x27;
                           &#x27;in writing, software\n&#x27;
                           &#x27; * distributed under the License is distributed on &#x27;
                           &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                           &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                           &#x27;either express or implied.\n&#x27;
                           &#x27; * See the License for the specific language &#x27;
                           &#x27;governing permissions and\n&#x27;
                           &#x27; * limitations under the License.\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;package &#x27;
                           &#x27;com.dtstack.flink.sql.sink.kafka.serialization;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import com.dtstack.flink.sql.enums.EUpdateMode;\n&#x27;
                           &#x27;import org.apache.avro.LogicalType;\n&#x27;
                           &#x27;import org.apache.avro.LogicalTypes;\n&#x27;
                           &#x27;import org.apache.avro.Schema;\n&#x27;
                           &#x27;import org.apache.avro.SchemaParseException;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.generic.GenericDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericRecord;\n&#x27;
                           &#x27;import org.apache.avro.generic.IndexedRecord;\n&#x27;
                           &#x27;import org.apache.avro.io.DatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.io.Encoder;\n&#x27;
                           &#x27;import org.apache.avro.io.EncoderFactory;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.specific.SpecificDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificRecord;\n&#x27;
                           &#x27;import org.apache.avro.util.Utf8;\n&#x27;
                           &#x27;import org.apache.commons.lang3.StringUtils;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.io.ByteArrayOutputStream;\n&#x27;
                           &#x27;import java.io.IOException;\n&#x27;
                           &#x27;import java.io.ObjectInputStream;\n&#x27;
                           &#x27;import java.io.ObjectOutputStream;\n&#x27;
                           &#x27;import java.math.BigDecimal;\n&#x27;
                           &#x27;import java.nio.ByteBuffer;\n&#x27;
                           &#x27;import java.sql.Date;\n&#x27;
                           &#x27;import java.sql.Time;\n&#x27;
                           &#x27;import java.sql.Timestamp;\n&#x27;
                           &#x27;import java.util.HashMap;\n&#x27;
                           &#x27;import java.util.List;\n&#x27;
                           &#x27;import java.util.Map;\n&#x27;
                           &#x27;import java.util.Objects;\n&#x27;
                           &#x27;import java.util.TimeZone;\n&#x27;
                           &#x27;import java.util.stream.Collectors;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Serialization schema that serializes CROW into &#x27;
                           &#x27;Avro bytes.\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * &lt;p&gt;Serializes objects that are represented in &#x27;
                           &#x27;(nested) Flink rows. It support types that\n&#x27;
                           &quot; * are compatible with Flink&#x27;s Table &amp; SQL API.\n&quot;
                           &#x27; **\n&#x27;
                           &#x27; * @author  maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public class AvroCRowSerializationSchema &#x27;
                           &#x27;implements SerializationSchema&lt;CRow&gt; {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Used for time conversions from SQL types.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate static final TimeZone LOCAL_TZ = &#x27;
                           &#x27;TimeZone.getDefault();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro record class for serialization. Might be &#x27;
                           &#x27;null if record class is not available.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate Class&lt;? extends SpecificRecord&gt; &#x27;
                           &#x27;recordClazz;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Schema string for deserialization.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate String schemaString;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro serialization schema.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Schema schema;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Writer to serialize Avro record into a byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient DatumWriter&lt;IndexedRecord&gt; &#x27;
                           &#x27;datumWriter;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Output stream to serialize records into byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient ByteArrayOutputStream &#x27;
                           &#x27;arrayOutputStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Low-level class for serialization of Avro &#x27;
                           &#x27;values.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Encoder encoder;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String updateMode;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String retractKey = &quot;retract&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given specific record class.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param recordClazz Avro record class used to &#x27;
                           &quot;serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(Class&lt;? &#x27;
                           &#x27;extends SpecificRecord&gt; recordClazz, String &#x27;
                           &#x27;updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(recordClazz, &quot;Avro &#x27;
                           &#x27;record class must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = recordClazz;\n&#x27;
                           &#x27;\t\tthis.schema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\tthis.schemaString = schema.toString();\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given Avro schema string.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param avroSchemaString Avro schema string &#x27;
                           &quot;used to serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(String &#x27;
                           &#x27;avroSchemaString,String updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(avroSchemaString, &#x27;
                           &#x27;&quot;Avro schema must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = null;\n&#x27;
                           &#x27;\t\tthis.schemaString = avroSchemaString;\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tthis.schema = new &#x27;
                           &#x27;Schema.Parser().parse(avroSchemaString);\n&#x27;
                           &#x27;\t\t} catch (SchemaParseException e) {\n&#x27;
                           &#x27;\t\t\tthrow new IllegalArgumentException(&quot;Could &#x27;
                           &#x27;not parse Avro schema string.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;GenericDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic byte[] serialize(CRow crow) {\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tRow row = crow.row();\n&#x27;
                           &#x27;\t\t\tboolean change = crow.change();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\t// convert to record\n&#x27;
                           &#x27;\t\t\tfinal GenericRecord record = &#x27;
                           &#x27;convertRowToAvroRecord(schema, row);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tdealRetractField(change, record);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tarrayOutputStream.reset();\n&#x27;
                           &#x27;\t\t\tdatumWriter.write(record, encoder);\n&#x27;
                           &#x27;\t\t\tencoder.flush();\n&#x27;
                           &#x27;\t\t\treturn arrayOutputStream.toByteArray();\n&#x27;
                           &#x27;\t\t} catch (Exception e) {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Failed to &#x27;
                           &#x27;serialize row.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprotected void dealRetractField(boolean change, &#x27;
                           &#x27;GenericRecord record) {\n&#x27;
                           &#x27;\t\tschema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.findFirst()\n&#x27;
                           &#x27;\t\t\t\t.ifPresent(field -&gt; {\n&#x27;
                           &#x27;\t\t\t\t\tif &#x27;
                           &#x27;(StringUtils.equalsIgnoreCase(updateMode, &#x27;
                           &#x27;EUpdateMode.UPSERT.name())) {\n&#x27;
                           &#x27;\t\t\t\t\t\trecord.put(retractKey, &#x27;
                           &#x27;convertFlinkType(field.schema(), change));\n&#x27;
                           &#x27;\t\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\t});\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic boolean equals(Object o) {\n&#x27;
                           &#x27;\t\tif (this == o) {\n&#x27;
                           &#x27;\t\t\treturn true;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tif (o == null || getClass() != o.getClass()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\treturn false;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tfinal AvroCRowSerializationSchema that = &#x27;
                           &#x27;(AvroCRowSerializationSchema) o;\n&#x27;
                           &#x27;\t\treturn Objects.equals(recordClazz, &#x27;
                           &#x27;that.recordClazz) &amp;&amp; Objects.equals(schemaString, &#x27;
                           &#x27;that.schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic int hashCode() {\n&#x27;
                           &#x27;\t\treturn Objects.hash(recordClazz, &#x27;
                           &#x27;schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t// &#x27;
                           &#x27;--------------------------------------------------------------------------------------------\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate GenericRecord &#x27;
                           &#x27;convertRowToAvroRecord(Schema schema, Row row) {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal List&lt;Schema.Field&gt; fields = &#x27;
                           &#x27;schema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;!StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.collect(Collectors.toList());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal int length = fields.size();\n&#x27;
                           &#x27;\t\tfinal GenericRecord record = new &#x27;
                           &#x27;GenericData.Record(schema);\n&#x27;
                           &#x27;\t\tfor (int i = 0; i &lt; length; i++) {\n&#x27;
                           &#x27;\t\t\tfinal Schema.Field field = fields.get(i);\n&#x27;
                           &#x27;\t\t\trecord.put(i, &#x27;
                           &#x27;convertFlinkType(field.schema(), &#x27;
                           &#x27;row.getField(i)));\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\treturn record;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate Object convertFlinkType(Schema schema, &#x27;
                           &#x27;Object object) {\n&#x27;
                           &#x27;\t\tif (object == null) {\n&#x27;
                           &#x27;\t\t\treturn null;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tswitch (schema.getType()) {\n&#x27;
                           &#x27;\t\t\tcase RECORD:\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Row) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertRowToAvroRecord(schema, &#x27;
                           &#x27;(Row) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\tthrow new IllegalStateException(&quot;Row &#x27;
                           &#x27;expected but was: &quot; + object.getClass());\n&#x27;
                           &#x27;\t\t\tcase ENUM:\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.EnumSymbol(schema, &#x27;
                           &#x27;object.toString());\n&#x27;
                           &#x27;\t\t\tcase ARRAY:\n&#x27;
                           &#x27;\t\t\t\tfinal Schema elementSchema = &#x27;
                           &#x27;schema.getElementType();\n&#x27;
                           &#x27;\t\t\t\tfinal Object[] array = (Object[]) object;\n&#x27;
                           &#x27;\t\t\t\tfinal GenericData.Array&lt;Object&gt; &#x27;
                           &#x27;convertedArray = new &#x27;
                           &#x27;GenericData.Array&lt;&gt;(array.length, schema);\n&#x27;
                           &#x27;\t\t\t\tfor (Object element : array) {\n&#x27;
                           &#x27;\t\t\t\t\t&#x27;
                           &#x27;convertedArray.add(convertFlinkType(elementSchema, &#x27;
                           &#x27;element));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedArray;\n&#x27;
                           &#x27;\t\t\tcase MAP:\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) object;\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;Utf8, Object&gt; convertedMap = new &#x27;
                           &#x27;HashMap&lt;&gt;();\n&#x27;
                           &#x27;\t\t\t\tfor (Map.Entry&lt;?, ?&gt; entry : &#x27;
                           &#x27;map.entrySet()) {\n&#x27;
                           &#x27;\t\t\t\t\tconvertedMap.put(\n&#x27;
                           &#x27;\t\t\t\t\t\tnew Utf8(entry.getKey().toString()),\n&#x27;
                           &#x27;\t\t\t\t\t\t&#x27;
                           &#x27;convertFlinkType(schema.getValueType(), &#x27;
                           &#x27;entry.getValue()));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedMap;\n&#x27;
                           &#x27;\t\t\tcase UNION:\n&#x27;
                           &#x27;\t\t\t\tfinal List&lt;Schema&gt; types = &#x27;
                           &#x27;schema.getTypes();\n&#x27;
                           &#x27;\t\t\t\tfinal int size = types.size();\n&#x27;
                           &#x27;\t\t\t\tfinal Schema actualSchema;\n&#x27;
                           &#x27;\t\t\t\tif (size == 2 &amp;&amp; types.get(0).getType() == &#x27;
                           &#x27;Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(1);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 2 &amp;&amp; &#x27;
                           &#x27;types.get(1).getType() == Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 1) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else {\n&#x27;
                           &#x27;\t\t\t\t\t// generic type\n&#x27;
                           &#x27;\t\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertFlinkType(actualSchema, &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\tcase FIXED:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn new GenericData.Fixed(\n&#x27;
                           &#x27;\t\t\t\t\t\tschema,\n&#x27;
                           &#x27;\t\t\t\t\t\tconvertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.Fixed(schema, &#x27;
                           &#x27;(byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase STRING:\n&#x27;
                           &#x27;\t\t\t\treturn new Utf8(object.toString());\n&#x27;
                           &#x27;\t\t\tcase BYTES:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn &#x27;
                           &#x27;ByteBuffer.wrap(convertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn ByteBuffer.wrap((byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase INT:\n&#x27;
                           &#x27;\t\t\t\t// check for logical types\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Date) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromDate(schema, (Date) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t} else if (object instanceof Time) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTime(schema, (Time) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase LONG:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Timestamp) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTimestamp(schema, &#x27;
                           &#x27;(Timestamp) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase FLOAT:\n&#x27;
                           &#x27;\t\t\tcase DOUBLE:\n&#x27;
                           &#x27;\t\t\tcase BOOLEAN:\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tdefault:\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthrow new RuntimeException(&quot;Unsupported Avro &#x27;
                           &#x27;type:&quot; + schema);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate byte[] convertFromDecimal(Schema schema, &#x27;
                           &#x27;BigDecimal decimal) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType instanceof &#x27;
                           &#x27;LogicalTypes.Decimal) {\n&#x27;
                           &#x27;\t\t\tfinal LogicalTypes.Decimal decimalType = &#x27;
                           &#x27;(LogicalTypes.Decimal) logicalType;\n&#x27;
                           &#x27;\t\t\t// rescale to target type\n&#x27;
                           &#x27;\t\t\tfinal BigDecimal rescaled = &#x27;
                           &#x27;decimal.setScale(decimalType.getScale(), &#x27;
                           &#x27;BigDecimal.ROUND_UNNECESSARY);\n&#x27;
                           &#x27;\t\t\t// byte array must contain the &#x27;
                           &quot;two&#x27;s-complement representation of the\n&quot;
                           &#x27;\t\t\t// unscaled integer value in big-endian byte &#x27;
                           &#x27;order\n&#x27;
                           &#x27;\t\t\treturn &#x27;
                           &#x27;decimal.unscaledValue().toByteArray();\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;decimal type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromDate(Schema schema, Date &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.date()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted / 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported date &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromTime(Schema schema, Time &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.timeMillis()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted % 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported time &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate long convertFromTimestamp(Schema schema, &#x27;
                           &#x27;Timestamp date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == &#x27;
                           &#x27;LogicalTypes.timestampMillis()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\treturn time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;timestamp type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate void writeObject(ObjectOutputStream &#x27;
                           &#x27;outputStream) throws IOException {\n&#x27;
                           &#x27;\t\toutputStream.writeObject(recordClazz);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(schemaString); // &#x27;
                           &#x27;support for null\n&#x27;
                           &#x27;\t\toutputStream.writeObject(retractKey);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(updateMode);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@SuppressWarnings(&quot;unchecked&quot;)\n&#x27;
                           &#x27;\tprivate void readObject(ObjectInputStream &#x27;
                           &#x27;inputStream) throws ClassNotFoundException, &#x27;
                           &#x27;IOException {\n&#x27;
                           &#x27;\t\trecordClazz = (Class&lt;? extends &#x27;
                           &#x27;SpecificRecord&gt;) inputStream.readObject();\n&#x27;
                           &#x27;\t\tschemaString = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tif (recordClazz != null) {\n&#x27;
                           &#x27;\t\t\tschema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tschema = new &#x27;
                           &#x27;Schema.Parser().parse(schemaString);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tretractKey = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tupdateMode = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tdatumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tarrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tencoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;}\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;/*\n&#x27;
                           &#x27; * Licensed to the Apache Software Foundation &#x27;
                           &#x27;(ASF) under one\n&#x27;
                           &#x27; * or more contributor license agreements.  See &#x27;
                           &#x27;the NOTICE file\n&#x27;
                           &#x27; * distributed with this work for additional &#x27;
                           &#x27;information\n&#x27;
                           &#x27; * regarding copyright ownership.  The ASF &#x27;
                           &#x27;licenses this file\n&#x27;
                           &#x27; * to you under the Apache License, Version 2.0 &#x27;
                           &#x27;(the\n&#x27;
                           &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                           &#x27;compliance\n&#x27;
                           &#x27; * with the License.  You may obtain a copy of the &#x27;
                           &#x27;License at\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; *     http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * Unless required by applicable law or agreed to &#x27;
                           &#x27;in writing, software\n&#x27;
                           &#x27; * distributed under the License is distributed on &#x27;
                           &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                           &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                           &#x27;either express or implied.\n&#x27;
                           &#x27; * See the License for the specific language &#x27;
                           &#x27;governing permissions and\n&#x27;
                           &#x27; * limitations under the License.\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;package &#x27;
                           &#x27;com.dtstack.flink.sql.sink.kafka.serialization;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import com.dtstack.flink.sql.enums.EUpdateMode;\n&#x27;
                           &#x27;import org.apache.avro.LogicalType;\n&#x27;
                           &#x27;import org.apache.avro.LogicalTypes;\n&#x27;
                           &#x27;import org.apache.avro.Schema;\n&#x27;
                           &#x27;import org.apache.avro.SchemaParseException;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.generic.GenericDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericRecord;\n&#x27;
                           &#x27;import org.apache.avro.generic.IndexedRecord;\n&#x27;
                           &#x27;import org.apache.avro.io.DatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.io.Encoder;\n&#x27;
                           &#x27;import org.apache.avro.io.EncoderFactory;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.specific.SpecificDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificRecord;\n&#x27;
                           &#x27;import org.apache.avro.util.Utf8;\n&#x27;
                           &#x27;import org.apache.commons.lang3.StringUtils;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.io.ByteArrayOutputStream;\n&#x27;
                           &#x27;import java.io.IOException;\n&#x27;
                           &#x27;import java.io.ObjectInputStream;\n&#x27;
                           &#x27;import java.io.ObjectOutputStream;\n&#x27;
                           &#x27;import java.math.BigDecimal;\n&#x27;
                           &#x27;import java.nio.ByteBuffer;\n&#x27;
                           &#x27;import java.sql.Date;\n&#x27;
                           &#x27;import java.sql.Time;\n&#x27;
                           &#x27;import java.sql.Timestamp;\n&#x27;
                           &#x27;import java.util.HashMap;\n&#x27;
                           &#x27;import java.util.List;\n&#x27;
                           &#x27;import java.util.Map;\n&#x27;
                           &#x27;import java.util.Objects;\n&#x27;
                           &#x27;import java.util.TimeZone;\n&#x27;
                           &#x27;import java.util.stream.Collectors;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Serialization schema that serializes CROW into &#x27;
                           &#x27;Avro bytes.\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * &lt;p&gt;Serializes objects that are represented in &#x27;
                           &#x27;(nested) Flink rows. It support types that\n&#x27;
                           &quot; * are compatible with Flink&#x27;s Table &amp; SQL API.\n&quot;
                           &#x27; **\n&#x27;
                           &#x27; * @author  maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public class AvroCRowSerializationSchema &#x27;
                           &#x27;implements SerializationSchema&lt;CRow&gt; {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Used for time conversions from SQL types.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate static final TimeZone LOCAL_TZ = &#x27;
                           &#x27;TimeZone.getDefault();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro record class for serialization. Might be &#x27;
                           &#x27;null if record class is not available.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate Class&lt;? extends SpecificRecord&gt; &#x27;
                           &#x27;recordClazz;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Schema string for deserialization.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate String schemaString;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro serialization schema.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Schema schema;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Writer to serialize Avro record into a byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient DatumWriter&lt;IndexedRecord&gt; &#x27;
                           &#x27;datumWriter;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Output stream to serialize records into byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient ByteArrayOutputStream &#x27;
                           &#x27;arrayOutputStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Low-level class for serialization of Avro &#x27;
                           &#x27;values.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Encoder encoder;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String updateMode;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String retractKey = &quot;retract&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given specific record class.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param recordClazz Avro record class used to &#x27;
                           &quot;serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(Class&lt;? &#x27;
                           &#x27;extends SpecificRecord&gt; recordClazz, String &#x27;
                           &#x27;updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(recordClazz, &quot;Avro &#x27;
                           &#x27;record class must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = recordClazz;\n&#x27;
                           &#x27;\t\tthis.schema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\tthis.schemaString = schema.toString();\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given Avro schema string.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param avroSchemaString Avro schema string &#x27;
                           &quot;used to serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(String &#x27;
                           &#x27;avroSchemaString,String updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(avroSchemaString, &#x27;
                           &#x27;&quot;Avro schema must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = null;\n&#x27;
                           &#x27;\t\tthis.schemaString = avroSchemaString;\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tthis.schema = new &#x27;
                           &#x27;Schema.Parser().parse(avroSchemaString);\n&#x27;
                           &#x27;\t\t} catch (SchemaParseException e) {\n&#x27;
                           &#x27;\t\t\tthrow new IllegalArgumentException(&quot;Could &#x27;
                           &#x27;not parse Avro schema string.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;GenericDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic byte[] serialize(CRow crow) {\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tRow row = crow.row();\n&#x27;
                           &#x27;\t\t\tboolean change = crow.change();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\t// convert to record\n&#x27;
                           &#x27;\t\t\tfinal GenericRecord record = &#x27;
                           &#x27;convertRowToAvroRecord(schema, row);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tdealRetractField(change, record);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tarrayOutputStream.reset();\n&#x27;
                           &#x27;\t\t\tdatumWriter.write(record, encoder);\n&#x27;
                           &#x27;\t\t\tencoder.flush();\n&#x27;
                           &#x27;\t\t\treturn arrayOutputStream.toByteArray();\n&#x27;
                           &#x27;\t\t} catch (Exception e) {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Failed to &#x27;
                           &#x27;serialize row.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprotected void dealRetractField(boolean change, &#x27;
                           &#x27;GenericRecord record) {\n&#x27;
                           &#x27;\t\tschema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.findFirst()\n&#x27;
                           &#x27;\t\t\t\t.ifPresent(field -&gt; {\n&#x27;
                           &#x27;\t\t\t\t\tif &#x27;
                           &#x27;(StringUtils.equalsIgnoreCase(updateMode, &#x27;
                           &#x27;EUpdateMode.UPSERT.name())) {\n&#x27;
                           &#x27;\t\t\t\t\t\trecord.put(retractKey, &#x27;
                           &#x27;convertFlinkType(field.schema(), change));\n&#x27;
                           &#x27;\t\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\t});\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic boolean equals(Object o) {\n&#x27;
                           &#x27;\t\tif (this == o) {\n&#x27;
                           &#x27;\t\t\treturn true;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tif (o == null || getClass() != o.getClass()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\treturn false;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tfinal AvroCRowSerializationSchema that = &#x27;
                           &#x27;(AvroCRowSerializationSchema) o;\n&#x27;
                           &#x27;\t\treturn Objects.equals(recordClazz, &#x27;
                           &#x27;that.recordClazz) &amp;&amp; Objects.equals(schemaString, &#x27;
                           &#x27;that.schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic int hashCode() {\n&#x27;
                           &#x27;\t\treturn Objects.hash(recordClazz, &#x27;
                           &#x27;schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t// &#x27;
                           &#x27;--------------------------------------------------------------------------------------------\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate GenericRecord &#x27;
                           &#x27;convertRowToAvroRecord(Schema schema, Row row) {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal List&lt;Schema.Field&gt; fields = &#x27;
                           &#x27;schema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;!StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.collect(Collectors.toList());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal int length = fields.size();\n&#x27;
                           &#x27;\t\tfinal GenericRecord record = new &#x27;
                           &#x27;GenericData.Record(schema);\n&#x27;
                           &#x27;\t\tfor (int i = 0; i &lt; length; i++) {\n&#x27;
                           &#x27;\t\t\tfinal Schema.Field field = fields.get(i);\n&#x27;
                           &#x27;\t\t\trecord.put(i, &#x27;
                           &#x27;convertFlinkType(field.schema(), &#x27;
                           &#x27;row.getField(i)));\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\treturn record;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate Object convertFlinkType(Schema schema, &#x27;
                           &#x27;Object object) {\n&#x27;
                           &#x27;\t\tif (object == null) {\n&#x27;
                           &#x27;\t\t\treturn null;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tswitch (schema.getType()) {\n&#x27;
                           &#x27;\t\t\tcase RECORD:\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Row) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertRowToAvroRecord(schema, &#x27;
                           &#x27;(Row) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\tthrow new IllegalStateException(&quot;Row &#x27;
                           &#x27;expected but was: &quot; + object.getClass());\n&#x27;
                           &#x27;\t\t\tcase ENUM:\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.EnumSymbol(schema, &#x27;
                           &#x27;object.toString());\n&#x27;
                           &#x27;\t\t\tcase ARRAY:\n&#x27;
                           &#x27;\t\t\t\tfinal Schema elementSchema = &#x27;
                           &#x27;schema.getElementType();\n&#x27;
                           &#x27;\t\t\t\tfinal Object[] array = (Object[]) object;\n&#x27;
                           &#x27;\t\t\t\tfinal GenericData.Array&lt;Object&gt; &#x27;
                           &#x27;convertedArray = new &#x27;
                           &#x27;GenericData.Array&lt;&gt;(array.length, schema);\n&#x27;
                           &#x27;\t\t\t\tfor (Object element : array) {\n&#x27;
                           &#x27;\t\t\t\t\t&#x27;
                           &#x27;convertedArray.add(convertFlinkType(elementSchema, &#x27;
                           &#x27;element));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedArray;\n&#x27;
                           &#x27;\t\t\tcase MAP:\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) object;\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;Utf8, Object&gt; convertedMap = new &#x27;
                           &#x27;HashMap&lt;&gt;();\n&#x27;
                           &#x27;\t\t\t\tfor (Map.Entry&lt;?, ?&gt; entry : &#x27;
                           &#x27;map.entrySet()) {\n&#x27;
                           &#x27;\t\t\t\t\tconvertedMap.put(\n&#x27;
                           &#x27;\t\t\t\t\t\tnew Utf8(entry.getKey().toString()),\n&#x27;
                           &#x27;\t\t\t\t\t\t&#x27;
                           &#x27;convertFlinkType(schema.getValueType(), &#x27;
                           &#x27;entry.getValue()));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedMap;\n&#x27;
                           &#x27;\t\t\tcase UNION:\n&#x27;
                           &#x27;\t\t\t\tfinal List&lt;Schema&gt; types = &#x27;
                           &#x27;schema.getTypes();\n&#x27;
                           &#x27;\t\t\t\tfinal int size = types.size();\n&#x27;
                           &#x27;\t\t\t\tfinal Schema actualSchema;\n&#x27;
                           &#x27;\t\t\t\tif (size == 2 &amp;&amp; types.get(0).getType() == &#x27;
                           &#x27;Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(1);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 2 &amp;&amp; &#x27;
                           &#x27;types.get(1).getType() == Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 1) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else {\n&#x27;
                           &#x27;\t\t\t\t\t// generic type\n&#x27;
                           &#x27;\t\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertFlinkType(actualSchema, &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\tcase FIXED:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn new GenericData.Fixed(\n&#x27;
                           &#x27;\t\t\t\t\t\tschema,\n&#x27;
                           &#x27;\t\t\t\t\t\tconvertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.Fixed(schema, &#x27;
                           &#x27;(byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase STRING:\n&#x27;
                           &#x27;\t\t\t\treturn new Utf8(object.toString());\n&#x27;
                           &#x27;\t\t\tcase BYTES:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn &#x27;
                           &#x27;ByteBuffer.wrap(convertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn ByteBuffer.wrap((byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase INT:\n&#x27;
                           &#x27;\t\t\t\t// check for logical types\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Date) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromDate(schema, (Date) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t} else if (object instanceof Time) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTime(schema, (Time) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase LONG:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Timestamp) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTimestamp(schema, &#x27;
                           &#x27;(Timestamp) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase FLOAT:\n&#x27;
                           &#x27;\t\t\tcase DOUBLE:\n&#x27;
                           &#x27;\t\t\tcase BOOLEAN:\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tdefault:\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthrow new RuntimeException(&quot;Unsupported Avro &#x27;
                           &#x27;type:&quot; + schema);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate byte[] convertFromDecimal(Schema schema, &#x27;
                           &#x27;BigDecimal decimal) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType instanceof &#x27;
                           &#x27;LogicalTypes.Decimal) {\n&#x27;
                           &#x27;\t\t\tfinal LogicalTypes.Decimal decimalType = &#x27;
                           &#x27;(LogicalTypes.Decimal) logicalType;\n&#x27;
                           &#x27;\t\t\t// rescale to target type\n&#x27;
                           &#x27;\t\t\tfinal BigDecimal rescaled = &#x27;
                           &#x27;decimal.setScale(decimalType.getScale(), &#x27;
                           &#x27;BigDecimal.ROUND_UNNECESSARY);\n&#x27;
                           &#x27;\t\t\t// byte array must contain the &#x27;
                           &quot;two&#x27;s-complement representation of the\n&quot;
                           &#x27;\t\t\t// unscaled integer value in big-endian byte &#x27;
                           &#x27;order\n&#x27;
                           &#x27;\t\t\treturn &#x27;
                           &#x27;decimal.unscaledValue().toByteArray();\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;decimal type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromDate(Schema schema, Date &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.date()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted / 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported date &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromTime(Schema schema, Time &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.timeMillis()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted % 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported time &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate long convertFromTimestamp(Schema schema, &#x27;
                           &#x27;Timestamp date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == &#x27;
                           &#x27;LogicalTypes.timestampMillis()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\treturn time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;timestamp type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate void writeObject(ObjectOutputStream &#x27;
                           &#x27;outputStream) throws IOException {\n&#x27;
                           &#x27;\t\toutputStream.writeObject(recordClazz);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(schemaString); // &#x27;
                           &#x27;support for null\n&#x27;
                           &#x27;\t\toutputStream.writeObject(retractKey);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(updateMode);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@SuppressWarnings(&quot;unchecked&quot;)\n&#x27;
                           &#x27;\tprivate void readObject(ObjectInputStream &#x27;
                           &#x27;inputStream) throws ClassNotFoundException, &#x27;
                           &#x27;IOException {\n&#x27;
                           &#x27;\t\trecordClazz = (Class&lt;? extends &#x27;
                           &#x27;SpecificRecord&gt;) inputStream.readObject();\n&#x27;
                           &#x27;\t\tschemaString = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tif (recordClazz != null) {\n&#x27;
                           &#x27;\t\t\tschema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tschema = new &#x27;
                           &#x27;Schema.Parser().parse(schemaString);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tretractKey = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tupdateMode = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tdatumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tarrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tencoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;}\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;
                           &#x27;/*\n&#x27;
                           &#x27; * Licensed to the Apache Software Foundation &#x27;
                           &#x27;(ASF) under one\n&#x27;
                           &#x27; * or more contributor license agreements.  See &#x27;
                           &#x27;the NOTICE file\n&#x27;
                           &#x27; * distributed with this work for additional &#x27;
                           &#x27;information\n&#x27;
                           &#x27; * regarding copyright ownership.  The ASF &#x27;
                           &#x27;licenses this file\n&#x27;
                           &#x27; * to you under the Apache License, Version 2.0 &#x27;
                           &#x27;(the\n&#x27;
                           &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                           &#x27;compliance\n&#x27;
                           &#x27; * with the License.  You may obtain a copy of the &#x27;
                           &#x27;License at\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; *     http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * Unless required by applicable law or agreed to &#x27;
                           &#x27;in writing, software\n&#x27;
                           &#x27; * distributed under the License is distributed on &#x27;
                           &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                           &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                           &#x27;either express or implied.\n&#x27;
                           &#x27; * See the License for the specific language &#x27;
                           &#x27;governing permissions and\n&#x27;
                           &#x27; * limitations under the License.\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;package &#x27;
                           &#x27;com.dtstack.flink.sql.sink.kafka.serialization;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import com.dtstack.flink.sql.enums.EUpdateMode;\n&#x27;
                           &#x27;import org.apache.avro.LogicalType;\n&#x27;
                           &#x27;import org.apache.avro.LogicalTypes;\n&#x27;
                           &#x27;import org.apache.avro.Schema;\n&#x27;
                           &#x27;import org.apache.avro.SchemaParseException;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.generic.GenericDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericRecord;\n&#x27;
                           &#x27;import org.apache.avro.generic.IndexedRecord;\n&#x27;
                           &#x27;import org.apache.avro.io.DatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.io.Encoder;\n&#x27;
                           &#x27;import org.apache.avro.io.EncoderFactory;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.specific.SpecificDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificRecord;\n&#x27;
                           &#x27;import org.apache.avro.util.Utf8;\n&#x27;
                           &#x27;import org.apache.commons.lang3.StringUtils;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.io.ByteArrayOutputStream;\n&#x27;
                           &#x27;import java.io.IOException;\n&#x27;
                           &#x27;import java.io.ObjectInputStream;\n&#x27;
                           &#x27;import java.io.ObjectOutputStream;\n&#x27;
                           &#x27;import java.math.BigDecimal;\n&#x27;
                           &#x27;import java.nio.ByteBuffer;\n&#x27;
                           &#x27;import java.sql.Date;\n&#x27;
                           &#x27;import java.sql.Time;\n&#x27;
                           &#x27;import java.sql.Timestamp;\n&#x27;
                           &#x27;import java.util.HashMap;\n&#x27;
                           &#x27;import java.util.List;\n&#x27;
                           &#x27;import java.util.Map;\n&#x27;
                           &#x27;import java.util.Objects;\n&#x27;
                           &#x27;import java.util.TimeZone;\n&#x27;
                           &#x27;import java.util.stream.Collectors;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Serialization schema that serializes CROW into &#x27;
                           &#x27;Avro bytes.\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * &lt;p&gt;Serializes objects that are represented in &#x27;
                           &#x27;(nested) Flink rows. It support types that\n&#x27;
                           &quot; * are compatible with Flink&#x27;s Table &amp; SQL API.\n&quot;
                           &#x27; **\n&#x27;
                           &#x27; * @author  maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public class AvroCRowSerializationSchema &#x27;
                           &#x27;implements SerializationSchema&lt;CRow&gt; {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Used for time conversions from SQL types.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate static final TimeZone LOCAL_TZ = &#x27;
                           &#x27;TimeZone.getDefault();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro record class for serialization. Might be &#x27;
                           &#x27;null if record class is not available.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate Class&lt;? extends SpecificRecord&gt; &#x27;
                           &#x27;recordClazz;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Schema string for deserialization.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate String schemaString;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro serialization schema.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Schema schema;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Writer to serialize Avro record into a byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient DatumWriter&lt;IndexedRecord&gt; &#x27;
                           &#x27;datumWriter;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Output stream to serialize records into byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient ByteArrayOutputStream &#x27;
                           &#x27;arrayOutputStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Low-level class for serialization of Avro &#x27;
                           &#x27;values.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Encoder encoder;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String updateMode;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String retractKey = &quot;retract&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given specific record class.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param recordClazz Avro record class used to &#x27;
                           &quot;serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(Class&lt;? &#x27;
                           &#x27;extends SpecificRecord&gt; recordClazz, String &#x27;
                           &#x27;updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(recordClazz, &quot;Avro &#x27;
                           &#x27;record class must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = recordClazz;\n&#x27;
                           &#x27;\t\tthis.schema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\tthis.schemaString = schema.toString();\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given Avro schema string.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param avroSchemaString Avro schema string &#x27;
                           &quot;used to serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(String &#x27;
                           &#x27;avroSchemaString,String updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(avroSchemaString, &#x27;
                           &#x27;&quot;Avro schema must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = null;\n&#x27;
                           &#x27;\t\tthis.schemaString = avroSchemaString;\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tthis.schema = new &#x27;
                           &#x27;Schema.Parser().parse(avroSchemaString);\n&#x27;
                           &#x27;\t\t} catch (SchemaParseException e) {\n&#x27;
                           &#x27;\t\t\tthrow new IllegalArgumentException(&quot;Could &#x27;
                           &#x27;not parse Avro schema string.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;GenericDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic byte[] serialize(CRow crow) {\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tRow row = crow.row();\n&#x27;
                           &#x27;\t\t\tboolean change = crow.change();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\t// convert to record\n&#x27;
                           &#x27;\t\t\tfinal GenericRecord record = &#x27;
                           &#x27;convertRowToAvroRecord(schema, row);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tdealRetractField(change, record);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tarrayOutputStream.reset();\n&#x27;
                           &#x27;\t\t\tdatumWriter.write(record, encoder);\n&#x27;
                           &#x27;\t\t\tencoder.flush();\n&#x27;
                           &#x27;\t\t\treturn arrayOutputStream.toByteArray();\n&#x27;
                           &#x27;\t\t} catch (Exception e) {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Failed to &#x27;
                           &#x27;serialize row.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprotected void dealRetractField(boolean change, &#x27;
                           &#x27;GenericRecord record) {\n&#x27;
                           &#x27;\t\tschema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.findFirst()\n&#x27;
                           &#x27;\t\t\t\t.ifPresent(field -&gt; {\n&#x27;
                           &#x27;\t\t\t\t\tif &#x27;
                           &#x27;(StringUtils.equalsIgnoreCase(updateMode, &#x27;
                           &#x27;EUpdateMode.UPSERT.name())) {\n&#x27;
                           &#x27;\t\t\t\t\t\trecord.put(retractKey, &#x27;
                           &#x27;convertFlinkType(field.schema(), change));\n&#x27;
                           &#x27;\t\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\t});\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic boolean equals(Object o) {\n&#x27;
                           &#x27;\t\tif (this == o) {\n&#x27;
                           &#x27;\t\t\treturn true;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tif (o == null || getClass() != o.getClass()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\treturn false;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tfinal AvroCRowSerializationSchema that = &#x27;
                           &#x27;(AvroCRowSerializationSchema) o;\n&#x27;
                           &#x27;\t\treturn Objects.equals(recordClazz, &#x27;
                           &#x27;that.recordClazz) &amp;&amp; Objects.equals(schemaString, &#x27;
                           &#x27;that.schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic int hashCode() {\n&#x27;
                           &#x27;\t\treturn Objects.hash(recordClazz, &#x27;
                           &#x27;schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t// &#x27;
                           &#x27;--------------------------------------------------------------------------------------------\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate GenericRecord &#x27;
                           &#x27;convertRowToAvroRecord(Schema schema, Row row) {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal List&lt;Schema.Field&gt; fields = &#x27;
                           &#x27;schema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;!StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.collect(Collectors.toList());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal int length = fields.size();\n&#x27;
                           &#x27;\t\tfinal GenericRecord record = new &#x27;
                           &#x27;GenericData.Record(schema);\n&#x27;
                           &#x27;\t\tfor (int i = 0; i &lt; length; i++) {\n&#x27;
                           &#x27;\t\t\tfinal Schema.Field field = fields.get(i);\n&#x27;
                           &#x27;\t\t\trecord.put(i, &#x27;
                           &#x27;convertFlinkType(field.schema(), &#x27;
                           &#x27;row.getField(i)));\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\treturn record;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate Object convertFlinkType(Schema schema, &#x27;
                           &#x27;Object object) {\n&#x27;
                           &#x27;\t\tif (object == null) {\n&#x27;
                           &#x27;\t\t\treturn null;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tswitch (schema.getType()) {\n&#x27;
                           &#x27;\t\t\tcase RECORD:\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Row) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertRowToAvroRecord(schema, &#x27;
                           &#x27;(Row) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\tthrow new IllegalStateException(&quot;Row &#x27;
                           &#x27;expected but was: &quot; + object.getClass());\n&#x27;
                           &#x27;\t\t\tcase ENUM:\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.EnumSymbol(schema, &#x27;
                           &#x27;object.toString());\n&#x27;
                           &#x27;\t\t\tcase ARRAY:\n&#x27;
                           &#x27;\t\t\t\tfinal Schema elementSchema = &#x27;
                           &#x27;schema.getElementType();\n&#x27;
                           &#x27;\t\t\t\tfinal Object[] array = (Object[]) object;\n&#x27;
                           &#x27;\t\t\t\tfinal GenericData.Array&lt;Object&gt; &#x27;
                           &#x27;convertedArray = new &#x27;
                           &#x27;GenericData.Array&lt;&gt;(array.length, schema);\n&#x27;
                           &#x27;\t\t\t\tfor (Object element : array) {\n&#x27;
                           &#x27;\t\t\t\t\t&#x27;
                           &#x27;convertedArray.add(convertFlinkType(elementSchema, &#x27;
                           &#x27;element));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedArray;\n&#x27;
                           &#x27;\t\t\tcase MAP:\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) object;\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;Utf8, Object&gt; convertedMap = new &#x27;
                           &#x27;HashMap&lt;&gt;();\n&#x27;
                           &#x27;\t\t\t\tfor (Map.Entry&lt;?, ?&gt; entry : &#x27;
                           &#x27;map.entrySet()) {\n&#x27;
                           &#x27;\t\t\t\t\tconvertedMap.put(\n&#x27;
                           &#x27;\t\t\t\t\t\tnew Utf8(entry.getKey().toString()),\n&#x27;
                           &#x27;\t\t\t\t\t\t&#x27;
                           &#x27;convertFlinkType(schema.getValueType(), &#x27;
                           &#x27;entry.getValue()));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedMap;\n&#x27;
                           &#x27;\t\t\tcase UNION:\n&#x27;
                           &#x27;\t\t\t\tfinal List&lt;Schema&gt; types = &#x27;
                           &#x27;schema.getTypes();\n&#x27;
                           &#x27;\t\t\t\tfinal int size = types.size();\n&#x27;
                           &#x27;\t\t\t\tfinal Schema actualSchema;\n&#x27;
                           &#x27;\t\t\t\tif (size == 2 &amp;&amp; types.get(0).getType() == &#x27;
                           &#x27;Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(1);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 2 &amp;&amp; &#x27;
                           &#x27;types.get(1).getType() == Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 1) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else {\n&#x27;
                           &#x27;\t\t\t\t\t// generic type\n&#x27;
                           &#x27;\t\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertFlinkType(actualSchema, &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\tcase FIXED:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn new GenericData.Fixed(\n&#x27;
                           &#x27;\t\t\t\t\t\tschema,\n&#x27;
                           &#x27;\t\t\t\t\t\tconvertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.Fixed(schema, &#x27;
                           &#x27;(byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase STRING:\n&#x27;
                           &#x27;\t\t\t\treturn new Utf8(object.toString());\n&#x27;
                           &#x27;\t\t\tcase BYTES:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn &#x27;
                           &#x27;ByteBuffer.wrap(convertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn ByteBuffer.wrap((byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase INT:\n&#x27;
                           &#x27;\t\t\t\t// check for logical types\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Date) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromDate(schema, (Date) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t} else if (object instanceof Time) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTime(schema, (Time) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase LONG:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Timestamp) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTimestamp(schema, &#x27;
                           &#x27;(Timestamp) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase FLOAT:\n&#x27;
                           &#x27;\t\t\tcase DOUBLE:\n&#x27;
                           &#x27;\t\t\tcase BOOLEAN:\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tdefault:\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthrow new RuntimeException(&quot;Unsupported Avro &#x27;
                           &#x27;type:&quot; + schema);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate byte[] convertFromDecimal(Schema schema, &#x27;
                           &#x27;BigDecimal decimal) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType instanceof &#x27;
                           &#x27;LogicalTypes.Decimal) {\n&#x27;
                           &#x27;\t\t\tfinal LogicalTypes.Decimal decimalType = &#x27;
                           &#x27;(LogicalTypes.Decimal) logicalType;\n&#x27;
                           &#x27;\t\t\t// rescale to target type\n&#x27;
                           &#x27;\t\t\tfinal BigDecimal rescaled = &#x27;
                           &#x27;decimal.setScale(decimalType.getScale(), &#x27;
                           &#x27;BigDecimal.ROUND_UNNECESSARY);\n&#x27;
                           &#x27;\t\t\t// byte array must contain the &#x27;
                           &quot;two&#x27;s-complement representation of the\n&quot;
                           &#x27;\t\t\t// unscaled integer value in big-endian byte &#x27;
                           &#x27;order\n&#x27;
                           &#x27;\t\t\treturn &#x27;
                           &#x27;decimal.unscaledValue().toByteArray();\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;decimal type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromDate(Schema schema, Date &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.date()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted / 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported date &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromTime(Schema schema, Time &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.timeMillis()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted % 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported time &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate long convertFromTimestamp(Schema schema, &#x27;
                           &#x27;Timestamp date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == &#x27;
                           &#x27;LogicalTypes.timestampMillis()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\treturn time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;timestamp type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate void writeObject(ObjectOutputStream &#x27;
                           &#x27;outputStream) throws IOException {\n&#x27;
                           &#x27;\t\toutputStream.writeObject(recordClazz);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(schemaString); // &#x27;
                           &#x27;support for null\n&#x27;
                           &#x27;\t\toutputStream.writeObject(retractKey);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(updateMode);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@SuppressWarnings(&quot;unchecked&quot;)\n&#x27;
                           &#x27;\tprivate void readObject(ObjectInputStream &#x27;
                           &#x27;inputStream) throws ClassNotFoundException, &#x27;
                           &#x27;IOException {\n&#x27;
                           &#x27;\t\trecordClazz = (Class&lt;? extends &#x27;
                           &#x27;SpecificRecord&gt;) inputStream.readObject();\n&#x27;
                           &#x27;\t\tschemaString = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tif (recordClazz != null) {\n&#x27;
                           &#x27;\t\t\tschema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tschema = new &#x27;
                           &#x27;Schema.Parser().parse(schemaString);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tretractKey = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tupdateMode = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tdatumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tarrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tencoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;}\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;spork&#x27;, &#x27;baseline&#x27;, &#x27;jfstmerge&#x27;}}],
 [{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;/*\n&#x27;
                           &#x27; * Licensed to the Apache Software Foundation &#x27;
                           &#x27;(ASF) under one\n&#x27;
                           &#x27; * or more contributor license agreements.  See &#x27;
                           &#x27;the NOTICE file\n&#x27;
                           &#x27; * distributed with this work for additional &#x27;
                           &#x27;information\n&#x27;
                           &#x27; * regarding copyright ownership.  The ASF &#x27;
                           &#x27;licenses this file\n&#x27;
                           &#x27; * to you under the Apache License, Version 2.0 &#x27;
                           &#x27;(the\n&#x27;
                           &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                           &#x27;compliance\n&#x27;
                           &#x27; * with the License.  You may obtain a copy of the &#x27;
                           &#x27;License at\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; *     http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * Unless required by applicable law or agreed to &#x27;
                           &#x27;in writing, software\n&#x27;
                           &#x27; * distributed under the License is distributed on &#x27;
                           &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                           &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                           &#x27;either express or implied.\n&#x27;
                           &#x27; * See the License for the specific language &#x27;
                           &#x27;governing permissions and\n&#x27;
                           &#x27; * limitations under the License.\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;package &#x27;
                           &#x27;com.dtstack.flink.sql.sink.kafka.serialization;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import com.dtstack.flink.sql.enums.EUpdateMode;\n&#x27;
                           &#x27;import org.apache.avro.LogicalType;\n&#x27;
                           &#x27;import org.apache.avro.LogicalTypes;\n&#x27;
                           &#x27;import org.apache.avro.Schema;\n&#x27;
                           &#x27;import org.apache.avro.SchemaParseException;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.generic.GenericDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericRecord;\n&#x27;
                           &#x27;import org.apache.avro.generic.IndexedRecord;\n&#x27;
                           &#x27;import org.apache.avro.io.DatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.io.Encoder;\n&#x27;
                           &#x27;import org.apache.avro.io.EncoderFactory;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.specific.SpecificDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificRecord;\n&#x27;
                           &#x27;import org.apache.avro.util.Utf8;\n&#x27;
                           &#x27;import org.apache.commons.lang3.StringUtils;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.io.ByteArrayOutputStream;\n&#x27;
                           &#x27;import java.io.IOException;\n&#x27;
                           &#x27;import java.io.ObjectInputStream;\n&#x27;
                           &#x27;import java.io.ObjectOutputStream;\n&#x27;
                           &#x27;import java.math.BigDecimal;\n&#x27;
                           &#x27;import java.nio.ByteBuffer;\n&#x27;
                           &#x27;import java.sql.Date;\n&#x27;
                           &#x27;import java.sql.Time;\n&#x27;
                           &#x27;import java.sql.Timestamp;\n&#x27;
                           &#x27;import java.util.HashMap;\n&#x27;
                           &#x27;import java.util.List;\n&#x27;
                           &#x27;import java.util.Map;\n&#x27;
                           &#x27;import java.util.Objects;\n&#x27;
                           &#x27;import java.util.TimeZone;\n&#x27;
                           &#x27;import java.util.stream.Collectors;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Serialization schema that serializes CROW into &#x27;
                           &#x27;Avro bytes.\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * &lt;p&gt;Serializes objects that are represented in &#x27;
                           &#x27;(nested) Flink rows. It support types that\n&#x27;
                           &quot; * are compatible with Flink&#x27;s Table &amp; SQL API.\n&quot;
                           &#x27; **\n&#x27;
                           &#x27; * @author  maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public class AvroCRowSerializationSchema &#x27;
                           &#x27;implements SerializationSchema&lt;CRow&gt; {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Used for time conversions from SQL types.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate static final TimeZone LOCAL_TZ = &#x27;
                           &#x27;TimeZone.getDefault();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro record class for serialization. Might be &#x27;
                           &#x27;null if record class is not available.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate Class&lt;? extends SpecificRecord&gt; &#x27;
                           &#x27;recordClazz;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Schema string for deserialization.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate String schemaString;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro serialization schema.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Schema schema;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Writer to serialize Avro record into a byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient DatumWriter&lt;IndexedRecord&gt; &#x27;
                           &#x27;datumWriter;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Output stream to serialize records into byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient ByteArrayOutputStream &#x27;
                           &#x27;arrayOutputStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Low-level class for serialization of Avro &#x27;
                           &#x27;values.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Encoder encoder;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String updateMode;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String retractKey = &quot;retract&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given specific record class.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param recordClazz Avro record class used to &#x27;
                           &quot;serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(Class&lt;? &#x27;
                           &#x27;extends SpecificRecord&gt; recordClazz, String &#x27;
                           &#x27;updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(recordClazz, &quot;Avro &#x27;
                           &#x27;record class must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = recordClazz;\n&#x27;
                           &#x27;\t\tthis.schema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\tthis.schemaString = schema.toString();\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given Avro schema string.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param avroSchemaString Avro schema string &#x27;
                           &quot;used to serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(String &#x27;
                           &#x27;avroSchemaString,String updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(avroSchemaString, &#x27;
                           &#x27;&quot;Avro schema must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = null;\n&#x27;
                           &#x27;\t\tthis.schemaString = avroSchemaString;\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tthis.schema = new &#x27;
                           &#x27;Schema.Parser().parse(avroSchemaString);\n&#x27;
                           &#x27;\t\t} catch (SchemaParseException e) {\n&#x27;
                           &#x27;\t\t\tthrow new IllegalArgumentException(&quot;Could &#x27;
                           &#x27;not parse Avro schema string.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;GenericDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic byte[] serialize(CRow crow) {\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tRow row = crow.row();\n&#x27;
                           &#x27;\t\t\tboolean change = crow.change();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\t// convert to record\n&#x27;
                           &#x27;\t\t\tfinal GenericRecord record = &#x27;
                           &#x27;convertRowToAvroRecord(schema, row);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tdealRetractField(change, record);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tarrayOutputStream.reset();\n&#x27;
                           &#x27;\t\t\tdatumWriter.write(record, encoder);\n&#x27;
                           &#x27;\t\t\tencoder.flush();\n&#x27;
                           &#x27;\t\t\treturn arrayOutputStream.toByteArray();\n&#x27;
                           &#x27;\t\t} catch (Exception e) {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Failed to &#x27;
                           &#x27;serialize row.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprotected void dealRetractField(boolean change, &#x27;
                           &#x27;GenericRecord record) {\n&#x27;
                           &#x27;\t\tschema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.findFirst()\n&#x27;
                           &#x27;\t\t\t\t.ifPresent(field -&gt; {\n&#x27;
                           &#x27;\t\t\t\t\tif &#x27;
                           &#x27;(StringUtils.equalsIgnoreCase(updateMode, &#x27;
                           &#x27;EUpdateMode.UPSERT.name())) {\n&#x27;
                           &#x27;\t\t\t\t\t\trecord.put(retractKey, &#x27;
                           &#x27;convertFlinkType(field.schema(), change));\n&#x27;
                           &#x27;\t\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\t});\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic boolean equals(Object o) {\n&#x27;
                           &#x27;\t\tif (this == o) {\n&#x27;
                           &#x27;\t\t\treturn true;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tif (o == null || getClass() != o.getClass()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\treturn false;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tfinal AvroCRowSerializationSchema that = &#x27;
                           &#x27;(AvroCRowSerializationSchema) o;\n&#x27;
                           &#x27;\t\treturn Objects.equals(recordClazz, &#x27;
                           &#x27;that.recordClazz) &amp;&amp; Objects.equals(schemaString, &#x27;
                           &#x27;that.schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic int hashCode() {\n&#x27;
                           &#x27;\t\treturn Objects.hash(recordClazz, &#x27;
                           &#x27;schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t// &#x27;
                           &#x27;--------------------------------------------------------------------------------------------\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate GenericRecord &#x27;
                           &#x27;convertRowToAvroRecord(Schema schema, Row row) {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal List&lt;Schema.Field&gt; fields = &#x27;
                           &#x27;schema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;!StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.collect(Collectors.toList());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal int length = fields.size();\n&#x27;
                           &#x27;\t\tfinal GenericRecord record = new &#x27;
                           &#x27;GenericData.Record(schema);\n&#x27;
                           &#x27;\t\tfor (int i = 0; i &lt; length; i++) {\n&#x27;
                           &#x27;\t\t\tfinal Schema.Field field = fields.get(i);\n&#x27;
                           &#x27;\t\t\trecord.put(i, &#x27;
                           &#x27;convertFlinkType(field.schema(), &#x27;
                           &#x27;row.getField(i)));\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\treturn record;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate Object convertFlinkType(Schema schema, &#x27;
                           &#x27;Object object) {\n&#x27;
                           &#x27;\t\tif (object == null) {\n&#x27;
                           &#x27;\t\t\treturn null;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tswitch (schema.getType()) {\n&#x27;
                           &#x27;\t\t\tcase RECORD:\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Row) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertRowToAvroRecord(schema, &#x27;
                           &#x27;(Row) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\tthrow new IllegalStateException(&quot;Row &#x27;
                           &#x27;expected but was: &quot; + object.getClass());\n&#x27;
                           &#x27;\t\t\tcase ENUM:\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.EnumSymbol(schema, &#x27;
                           &#x27;object.toString());\n&#x27;
                           &#x27;\t\t\tcase ARRAY:\n&#x27;
                           &#x27;\t\t\t\tfinal Schema elementSchema = &#x27;
                           &#x27;schema.getElementType();\n&#x27;
                           &#x27;\t\t\t\tfinal Object[] array = (Object[]) object;\n&#x27;
                           &#x27;\t\t\t\tfinal GenericData.Array&lt;Object&gt; &#x27;
                           &#x27;convertedArray = new &#x27;
                           &#x27;GenericData.Array&lt;&gt;(array.length, schema);\n&#x27;
                           &#x27;\t\t\t\tfor (Object element : array) {\n&#x27;
                           &#x27;\t\t\t\t\t&#x27;
                           &#x27;convertedArray.add(convertFlinkType(elementSchema, &#x27;
                           &#x27;element));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedArray;\n&#x27;
                           &#x27;\t\t\tcase MAP:\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) object;\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;Utf8, Object&gt; convertedMap = new &#x27;
                           &#x27;HashMap&lt;&gt;();\n&#x27;
                           &#x27;\t\t\t\tfor (Map.Entry&lt;?, ?&gt; entry : &#x27;
                           &#x27;map.entrySet()) {\n&#x27;
                           &#x27;\t\t\t\t\tconvertedMap.put(\n&#x27;
                           &#x27;\t\t\t\t\t\tnew Utf8(entry.getKey().toString()),\n&#x27;
                           &#x27;\t\t\t\t\t\t&#x27;
                           &#x27;convertFlinkType(schema.getValueType(), &#x27;
                           &#x27;entry.getValue()));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedMap;\n&#x27;
                           &#x27;\t\t\tcase UNION:\n&#x27;
                           &#x27;\t\t\t\tfinal List&lt;Schema&gt; types = &#x27;
                           &#x27;schema.getTypes();\n&#x27;
                           &#x27;\t\t\t\tfinal int size = types.size();\n&#x27;
                           &#x27;\t\t\t\tfinal Schema actualSchema;\n&#x27;
                           &#x27;\t\t\t\tif (size == 2 &amp;&amp; types.get(0).getType() == &#x27;
                           &#x27;Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(1);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 2 &amp;&amp; &#x27;
                           &#x27;types.get(1).getType() == Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 1) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else {\n&#x27;
                           &#x27;\t\t\t\t\t// generic type\n&#x27;
                           &#x27;\t\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertFlinkType(actualSchema, &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\tcase FIXED:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn new GenericData.Fixed(\n&#x27;
                           &#x27;\t\t\t\t\t\tschema,\n&#x27;
                           &#x27;\t\t\t\t\t\tconvertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.Fixed(schema, &#x27;
                           &#x27;(byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase STRING:\n&#x27;
                           &#x27;\t\t\t\treturn new Utf8(object.toString());\n&#x27;
                           &#x27;\t\t\tcase BYTES:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn &#x27;
                           &#x27;ByteBuffer.wrap(convertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn ByteBuffer.wrap((byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase INT:\n&#x27;
                           &#x27;\t\t\t\t// check for logical types\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Date) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromDate(schema, (Date) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t} else if (object instanceof Time) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTime(schema, (Time) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase LONG:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Timestamp) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTimestamp(schema, &#x27;
                           &#x27;(Timestamp) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase FLOAT:\n&#x27;
                           &#x27;\t\t\tcase DOUBLE:\n&#x27;
                           &#x27;\t\t\tcase BOOLEAN:\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tdefault:\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthrow new RuntimeException(&quot;Unsupported Avro &#x27;
                           &#x27;type:&quot; + schema);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate byte[] convertFromDecimal(Schema schema, &#x27;
                           &#x27;BigDecimal decimal) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType instanceof &#x27;
                           &#x27;LogicalTypes.Decimal) {\n&#x27;
                           &#x27;\t\t\tfinal LogicalTypes.Decimal decimalType = &#x27;
                           &#x27;(LogicalTypes.Decimal) logicalType;\n&#x27;
                           &#x27;\t\t\t// rescale to target type\n&#x27;
                           &#x27;\t\t\tfinal BigDecimal rescaled = &#x27;
                           &#x27;decimal.setScale(decimalType.getScale(), &#x27;
                           &#x27;BigDecimal.ROUND_UNNECESSARY);\n&#x27;
                           &#x27;\t\t\t// byte array must contain the &#x27;
                           &quot;two&#x27;s-complement representation of the\n&quot;
                           &#x27;\t\t\t// unscaled integer value in big-endian byte &#x27;
                           &#x27;order\n&#x27;
                           &#x27;\t\t\treturn &#x27;
                           &#x27;decimal.unscaledValue().toByteArray();\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;decimal type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromDate(Schema schema, Date &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.date()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted / 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported date &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromTime(Schema schema, Time &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.timeMillis()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted % 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported time &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate long convertFromTimestamp(Schema schema, &#x27;
                           &#x27;Timestamp date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == &#x27;
                           &#x27;LogicalTypes.timestampMillis()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\treturn time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;timestamp type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate void writeObject(ObjectOutputStream &#x27;
                           &#x27;outputStream) throws IOException {\n&#x27;
                           &#x27;\t\toutputStream.writeObject(recordClazz);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(schemaString); // &#x27;
                           &#x27;support for null\n&#x27;
                           &#x27;\t\toutputStream.writeObject(retractKey);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(updateMode);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@SuppressWarnings(&quot;unchecked&quot;)\n&#x27;
                           &#x27;\tprivate void readObject(ObjectInputStream &#x27;
                           &#x27;inputStream) throws ClassNotFoundException, &#x27;
                           &#x27;IOException {\n&#x27;
                           &#x27;\t\trecordClazz = (Class&lt;? extends &#x27;
                           &#x27;SpecificRecord&gt;) inputStream.readObject();\n&#x27;
                           &#x27;\t\tschemaString = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tif (recordClazz != null) {\n&#x27;
                           &#x27;\t\t\tschema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tschema = new &#x27;
                           &#x27;Schema.Parser().parse(schemaString);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tretractKey = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tupdateMode = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tdatumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tarrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tencoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;}\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;/*\n&#x27;
                           &#x27; * Licensed to the Apache Software Foundation &#x27;
                           &#x27;(ASF) under one\n&#x27;
                           &#x27; * or more contributor license agreements.  See &#x27;
                           &#x27;the NOTICE file\n&#x27;
                           &#x27; * distributed with this work for additional &#x27;
                           &#x27;information\n&#x27;
                           &#x27; * regarding copyright ownership.  The ASF &#x27;
                           &#x27;licenses this file\n&#x27;
                           &#x27; * to you under the Apache License, Version 2.0 &#x27;
                           &#x27;(the\n&#x27;
                           &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                           &#x27;compliance\n&#x27;
                           &#x27; * with the License.  You may obtain a copy of the &#x27;
                           &#x27;License at\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; *     http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * Unless required by applicable law or agreed to &#x27;
                           &#x27;in writing, software\n&#x27;
                           &#x27; * distributed under the License is distributed on &#x27;
                           &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                           &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                           &#x27;either express or implied.\n&#x27;
                           &#x27; * See the License for the specific language &#x27;
                           &#x27;governing permissions and\n&#x27;
                           &#x27; * limitations under the License.\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;package &#x27;
                           &#x27;com.dtstack.flink.sql.sink.kafka.serialization;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import com.dtstack.flink.sql.enums.EUpdateMode;\n&#x27;
                           &#x27;import org.apache.avro.LogicalType;\n&#x27;
                           &#x27;import org.apache.avro.LogicalTypes;\n&#x27;
                           &#x27;import org.apache.avro.Schema;\n&#x27;
                           &#x27;import org.apache.avro.SchemaParseException;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.generic.GenericDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericRecord;\n&#x27;
                           &#x27;import org.apache.avro.generic.IndexedRecord;\n&#x27;
                           &#x27;import org.apache.avro.io.DatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.io.Encoder;\n&#x27;
                           &#x27;import org.apache.avro.io.EncoderFactory;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.specific.SpecificDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificRecord;\n&#x27;
                           &#x27;import org.apache.avro.util.Utf8;\n&#x27;
                           &#x27;import org.apache.commons.lang3.StringUtils;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.io.ByteArrayOutputStream;\n&#x27;
                           &#x27;import java.io.IOException;\n&#x27;
                           &#x27;import java.io.ObjectInputStream;\n&#x27;
                           &#x27;import java.io.ObjectOutputStream;\n&#x27;
                           &#x27;import java.math.BigDecimal;\n&#x27;
                           &#x27;import java.nio.ByteBuffer;\n&#x27;
                           &#x27;import java.sql.Date;\n&#x27;
                           &#x27;import java.sql.Time;\n&#x27;
                           &#x27;import java.sql.Timestamp;\n&#x27;
                           &#x27;import java.util.HashMap;\n&#x27;
                           &#x27;import java.util.List;\n&#x27;
                           &#x27;import java.util.Map;\n&#x27;
                           &#x27;import java.util.Objects;\n&#x27;
                           &#x27;import java.util.TimeZone;\n&#x27;
                           &#x27;import java.util.stream.Collectors;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Serialization schema that serializes CROW into &#x27;
                           &#x27;Avro bytes.\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * &lt;p&gt;Serializes objects that are represented in &#x27;
                           &#x27;(nested) Flink rows. It support types that\n&#x27;
                           &quot; * are compatible with Flink&#x27;s Table &amp; SQL API.\n&quot;
                           &#x27; **\n&#x27;
                           &#x27; * @author  maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public class AvroCRowSerializationSchema &#x27;
                           &#x27;implements SerializationSchema&lt;CRow&gt; {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Used for time conversions from SQL types.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate static final TimeZone LOCAL_TZ = &#x27;
                           &#x27;TimeZone.getDefault();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro record class for serialization. Might be &#x27;
                           &#x27;null if record class is not available.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate Class&lt;? extends SpecificRecord&gt; &#x27;
                           &#x27;recordClazz;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Schema string for deserialization.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate String schemaString;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro serialization schema.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Schema schema;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Writer to serialize Avro record into a byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient DatumWriter&lt;IndexedRecord&gt; &#x27;
                           &#x27;datumWriter;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Output stream to serialize records into byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient ByteArrayOutputStream &#x27;
                           &#x27;arrayOutputStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Low-level class for serialization of Avro &#x27;
                           &#x27;values.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Encoder encoder;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String updateMode;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String retractKey = &quot;retract&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given specific record class.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param recordClazz Avro record class used to &#x27;
                           &quot;serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(Class&lt;? &#x27;
                           &#x27;extends SpecificRecord&gt; recordClazz, String &#x27;
                           &#x27;updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(recordClazz, &quot;Avro &#x27;
                           &#x27;record class must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = recordClazz;\n&#x27;
                           &#x27;\t\tthis.schema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\tthis.schemaString = schema.toString();\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given Avro schema string.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param avroSchemaString Avro schema string &#x27;
                           &quot;used to serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(String &#x27;
                           &#x27;avroSchemaString,String updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(avroSchemaString, &#x27;
                           &#x27;&quot;Avro schema must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = null;\n&#x27;
                           &#x27;\t\tthis.schemaString = avroSchemaString;\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tthis.schema = new &#x27;
                           &#x27;Schema.Parser().parse(avroSchemaString);\n&#x27;
                           &#x27;\t\t} catch (SchemaParseException e) {\n&#x27;
                           &#x27;\t\t\tthrow new IllegalArgumentException(&quot;Could &#x27;
                           &#x27;not parse Avro schema string.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;GenericDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic byte[] serialize(CRow crow) {\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tRow row = crow.row();\n&#x27;
                           &#x27;\t\t\tboolean change = crow.change();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\t// convert to record\n&#x27;
                           &#x27;\t\t\tfinal GenericRecord record = &#x27;
                           &#x27;convertRowToAvroRecord(schema, row);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tdealRetractField(change, record);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tarrayOutputStream.reset();\n&#x27;
                           &#x27;\t\t\tdatumWriter.write(record, encoder);\n&#x27;
                           &#x27;\t\t\tencoder.flush();\n&#x27;
                           &#x27;\t\t\treturn arrayOutputStream.toByteArray();\n&#x27;
                           &#x27;\t\t} catch (Exception e) {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Failed to &#x27;
                           &#x27;serialize row.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprotected void dealRetractField(boolean change, &#x27;
                           &#x27;GenericRecord record) {\n&#x27;
                           &#x27;\t\tschema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.findFirst()\n&#x27;
                           &#x27;\t\t\t\t.ifPresent(field -&gt; {\n&#x27;
                           &#x27;\t\t\t\t\tif &#x27;
                           &#x27;(StringUtils.equalsIgnoreCase(updateMode, &#x27;
                           &#x27;EUpdateMode.UPSERT.name())) {\n&#x27;
                           &#x27;\t\t\t\t\t\trecord.put(retractKey, &#x27;
                           &#x27;convertFlinkType(field.schema(), change));\n&#x27;
                           &#x27;\t\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\t});\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic boolean equals(Object o) {\n&#x27;
                           &#x27;\t\tif (this == o) {\n&#x27;
                           &#x27;\t\t\treturn true;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tif (o == null || getClass() != o.getClass()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\treturn false;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tfinal AvroCRowSerializationSchema that = &#x27;
                           &#x27;(AvroCRowSerializationSchema) o;\n&#x27;
                           &#x27;\t\treturn Objects.equals(recordClazz, &#x27;
                           &#x27;that.recordClazz) &amp;&amp; Objects.equals(schemaString, &#x27;
                           &#x27;that.schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic int hashCode() {\n&#x27;
                           &#x27;\t\treturn Objects.hash(recordClazz, &#x27;
                           &#x27;schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t// &#x27;
                           &#x27;--------------------------------------------------------------------------------------------\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate GenericRecord &#x27;
                           &#x27;convertRowToAvroRecord(Schema schema, Row row) {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal List&lt;Schema.Field&gt; fields = &#x27;
                           &#x27;schema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;!StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.collect(Collectors.toList());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal int length = fields.size();\n&#x27;
                           &#x27;\t\tfinal GenericRecord record = new &#x27;
                           &#x27;GenericData.Record(schema);\n&#x27;
                           &#x27;\t\tfor (int i = 0; i &lt; length; i++) {\n&#x27;
                           &#x27;\t\t\tfinal Schema.Field field = fields.get(i);\n&#x27;
                           &#x27;\t\t\trecord.put(i, &#x27;
                           &#x27;convertFlinkType(field.schema(), &#x27;
                           &#x27;row.getField(i)));\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\treturn record;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate Object convertFlinkType(Schema schema, &#x27;
                           &#x27;Object object) {\n&#x27;
                           &#x27;\t\tif (object == null) {\n&#x27;
                           &#x27;\t\t\treturn null;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tswitch (schema.getType()) {\n&#x27;
                           &#x27;\t\t\tcase RECORD:\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Row) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertRowToAvroRecord(schema, &#x27;
                           &#x27;(Row) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\tthrow new IllegalStateException(&quot;Row &#x27;
                           &#x27;expected but was: &quot; + object.getClass());\n&#x27;
                           &#x27;\t\t\tcase ENUM:\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.EnumSymbol(schema, &#x27;
                           &#x27;object.toString());\n&#x27;
                           &#x27;\t\t\tcase ARRAY:\n&#x27;
                           &#x27;\t\t\t\tfinal Schema elementSchema = &#x27;
                           &#x27;schema.getElementType();\n&#x27;
                           &#x27;\t\t\t\tfinal Object[] array = (Object[]) object;\n&#x27;
                           &#x27;\t\t\t\tfinal GenericData.Array&lt;Object&gt; &#x27;
                           &#x27;convertedArray = new &#x27;
                           &#x27;GenericData.Array&lt;&gt;(array.length, schema);\n&#x27;
                           &#x27;\t\t\t\tfor (Object element : array) {\n&#x27;
                           &#x27;\t\t\t\t\t&#x27;
                           &#x27;convertedArray.add(convertFlinkType(elementSchema, &#x27;
                           &#x27;element));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedArray;\n&#x27;
                           &#x27;\t\t\tcase MAP:\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) object;\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;Utf8, Object&gt; convertedMap = new &#x27;
                           &#x27;HashMap&lt;&gt;();\n&#x27;
                           &#x27;\t\t\t\tfor (Map.Entry&lt;?, ?&gt; entry : &#x27;
                           &#x27;map.entrySet()) {\n&#x27;
                           &#x27;\t\t\t\t\tconvertedMap.put(\n&#x27;
                           &#x27;\t\t\t\t\t\tnew Utf8(entry.getKey().toString()),\n&#x27;
                           &#x27;\t\t\t\t\t\t&#x27;
                           &#x27;convertFlinkType(schema.getValueType(), &#x27;
                           &#x27;entry.getValue()));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedMap;\n&#x27;
                           &#x27;\t\t\tcase UNION:\n&#x27;
                           &#x27;\t\t\t\tfinal List&lt;Schema&gt; types = &#x27;
                           &#x27;schema.getTypes();\n&#x27;
                           &#x27;\t\t\t\tfinal int size = types.size();\n&#x27;
                           &#x27;\t\t\t\tfinal Schema actualSchema;\n&#x27;
                           &#x27;\t\t\t\tif (size == 2 &amp;&amp; types.get(0).getType() == &#x27;
                           &#x27;Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(1);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 2 &amp;&amp; &#x27;
                           &#x27;types.get(1).getType() == Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 1) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else {\n&#x27;
                           &#x27;\t\t\t\t\t// generic type\n&#x27;
                           &#x27;\t\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertFlinkType(actualSchema, &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\tcase FIXED:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn new GenericData.Fixed(\n&#x27;
                           &#x27;\t\t\t\t\t\tschema,\n&#x27;
                           &#x27;\t\t\t\t\t\tconvertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.Fixed(schema, &#x27;
                           &#x27;(byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase STRING:\n&#x27;
                           &#x27;\t\t\t\treturn new Utf8(object.toString());\n&#x27;
                           &#x27;\t\t\tcase BYTES:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn &#x27;
                           &#x27;ByteBuffer.wrap(convertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn ByteBuffer.wrap((byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase INT:\n&#x27;
                           &#x27;\t\t\t\t// check for logical types\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Date) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromDate(schema, (Date) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t} else if (object instanceof Time) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTime(schema, (Time) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase LONG:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Timestamp) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTimestamp(schema, &#x27;
                           &#x27;(Timestamp) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase FLOAT:\n&#x27;
                           &#x27;\t\t\tcase DOUBLE:\n&#x27;
                           &#x27;\t\t\tcase BOOLEAN:\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tdefault:\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthrow new RuntimeException(&quot;Unsupported Avro &#x27;
                           &#x27;type:&quot; + schema);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate byte[] convertFromDecimal(Schema schema, &#x27;
                           &#x27;BigDecimal decimal) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType instanceof &#x27;
                           &#x27;LogicalTypes.Decimal) {\n&#x27;
                           &#x27;\t\t\tfinal LogicalTypes.Decimal decimalType = &#x27;
                           &#x27;(LogicalTypes.Decimal) logicalType;\n&#x27;
                           &#x27;\t\t\t// rescale to target type\n&#x27;
                           &#x27;\t\t\tfinal BigDecimal rescaled = &#x27;
                           &#x27;decimal.setScale(decimalType.getScale(), &#x27;
                           &#x27;BigDecimal.ROUND_UNNECESSARY);\n&#x27;
                           &#x27;\t\t\t// byte array must contain the &#x27;
                           &quot;two&#x27;s-complement representation of the\n&quot;
                           &#x27;\t\t\t// unscaled integer value in big-endian byte &#x27;
                           &#x27;order\n&#x27;
                           &#x27;\t\t\treturn &#x27;
                           &#x27;decimal.unscaledValue().toByteArray();\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;decimal type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromDate(Schema schema, Date &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.date()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted / 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported date &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromTime(Schema schema, Time &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.timeMillis()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted % 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported time &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate long convertFromTimestamp(Schema schema, &#x27;
                           &#x27;Timestamp date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == &#x27;
                           &#x27;LogicalTypes.timestampMillis()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\treturn time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;timestamp type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate void writeObject(ObjectOutputStream &#x27;
                           &#x27;outputStream) throws IOException {\n&#x27;
                           &#x27;\t\toutputStream.writeObject(recordClazz);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(schemaString); // &#x27;
                           &#x27;support for null\n&#x27;
                           &#x27;\t\toutputStream.writeObject(retractKey);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(updateMode);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@SuppressWarnings(&quot;unchecked&quot;)\n&#x27;
                           &#x27;\tprivate void readObject(ObjectInputStream &#x27;
                           &#x27;inputStream) throws ClassNotFoundException, &#x27;
                           &#x27;IOException {\n&#x27;
                           &#x27;\t\trecordClazz = (Class&lt;? extends &#x27;
                           &#x27;SpecificRecord&gt;) inputStream.readObject();\n&#x27;
                           &#x27;\t\tschemaString = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tif (recordClazz != null) {\n&#x27;
                           &#x27;\t\t\tschema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tschema = new &#x27;
                           &#x27;Schema.Parser().parse(schemaString);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tretractKey = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tupdateMode = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tdatumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tarrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tencoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;}\n&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;
                           &#x27;/*\n&#x27;
                           &#x27; * Licensed to the Apache Software Foundation &#x27;
                           &#x27;(ASF) under one\n&#x27;
                           &#x27; * or more contributor license agreements.  See &#x27;
                           &#x27;the NOTICE file\n&#x27;
                           &#x27; * distributed with this work for additional &#x27;
                           &#x27;information\n&#x27;
                           &#x27; * regarding copyright ownership.  The ASF &#x27;
                           &#x27;licenses this file\n&#x27;
                           &#x27; * to you under the Apache License, Version 2.0 &#x27;
                           &#x27;(the\n&#x27;
                           &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                           &#x27;compliance\n&#x27;
                           &#x27; * with the License.  You may obtain a copy of the &#x27;
                           &#x27;License at\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; *     http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * Unless required by applicable law or agreed to &#x27;
                           &#x27;in writing, software\n&#x27;
                           &#x27; * distributed under the License is distributed on &#x27;
                           &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                           &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                           &#x27;either express or implied.\n&#x27;
                           &#x27; * See the License for the specific language &#x27;
                           &#x27;governing permissions and\n&#x27;
                           &#x27; * limitations under the License.\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;package &#x27;
                           &#x27;com.dtstack.flink.sql.sink.kafka.serialization;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import com.dtstack.flink.sql.enums.EUpdateMode;\n&#x27;
                           &#x27;import org.apache.avro.LogicalType;\n&#x27;
                           &#x27;import org.apache.avro.LogicalTypes;\n&#x27;
                           &#x27;import org.apache.avro.Schema;\n&#x27;
                           &#x27;import org.apache.avro.SchemaParseException;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.generic.GenericDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.generic.GenericRecord;\n&#x27;
                           &#x27;import org.apache.avro.generic.IndexedRecord;\n&#x27;
                           &#x27;import org.apache.avro.io.DatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.io.Encoder;\n&#x27;
                           &#x27;import org.apache.avro.io.EncoderFactory;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificData;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.avro.specific.SpecificDatumWriter;\n&#x27;
                           &#x27;import org.apache.avro.specific.SpecificRecord;\n&#x27;
                           &#x27;import org.apache.avro.util.Utf8;\n&#x27;
                           &#x27;import org.apache.commons.lang3.StringUtils;\n&#x27;
                           &#x27;import &#x27;
                           &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                           &#x27;import org.apache.flink.table.runtime.types.CRow;\n&#x27;
                           &#x27;import org.apache.flink.types.Row;\n&#x27;
                           &#x27;import org.apache.flink.util.Preconditions;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;import java.io.ByteArrayOutputStream;\n&#x27;
                           &#x27;import java.io.IOException;\n&#x27;
                           &#x27;import java.io.ObjectInputStream;\n&#x27;
                           &#x27;import java.io.ObjectOutputStream;\n&#x27;
                           &#x27;import java.math.BigDecimal;\n&#x27;
                           &#x27;import java.nio.ByteBuffer;\n&#x27;
                           &#x27;import java.sql.Date;\n&#x27;
                           &#x27;import java.sql.Time;\n&#x27;
                           &#x27;import java.sql.Timestamp;\n&#x27;
                           &#x27;import java.util.HashMap;\n&#x27;
                           &#x27;import java.util.List;\n&#x27;
                           &#x27;import java.util.Map;\n&#x27;
                           &#x27;import java.util.Objects;\n&#x27;
                           &#x27;import java.util.TimeZone;\n&#x27;
                           &#x27;import java.util.stream.Collectors;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;/**\n&#x27;
                           &#x27; * Serialization schema that serializes CROW into &#x27;
                           &#x27;Avro bytes.\n&#x27;
                           &#x27; *\n&#x27;
                           &#x27; * &lt;p&gt;Serializes objects that are represented in &#x27;
                           &#x27;(nested) Flink rows. It support types that\n&#x27;
                           &quot; * are compatible with Flink&#x27;s Table &amp; SQL API.\n&quot;
                           &#x27; **\n&#x27;
                           &#x27; * @author  maqi\n&#x27;
                           &#x27; */\n&#x27;
                           &#x27;public class AvroCRowSerializationSchema &#x27;
                           &#x27;implements SerializationSchema&lt;CRow&gt; {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Used for time conversions from SQL types.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate static final TimeZone LOCAL_TZ = &#x27;
                           &#x27;TimeZone.getDefault();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro record class for serialization. Might be &#x27;
                           &#x27;null if record class is not available.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate Class&lt;? extends SpecificRecord&gt; &#x27;
                           &#x27;recordClazz;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Schema string for deserialization.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate String schemaString;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Avro serialization schema.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Schema schema;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Writer to serialize Avro record into a byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient DatumWriter&lt;IndexedRecord&gt; &#x27;
                           &#x27;datumWriter;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Output stream to serialize records into byte &#x27;
                           &#x27;array.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient ByteArrayOutputStream &#x27;
                           &#x27;arrayOutputStream;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Low-level class for serialization of Avro &#x27;
                           &#x27;values.\n&#x27;
                           &#x27;\t */\n&#x27;
                           &#x27;\tprivate transient Encoder encoder;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String updateMode;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate String retractKey = &quot;retract&quot;;\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given specific record class.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param recordClazz Avro record class used to &#x27;
                           &quot;serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(Class&lt;? &#x27;
                           &#x27;extends SpecificRecord&gt; recordClazz, String &#x27;
                           &#x27;updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(recordClazz, &quot;Avro &#x27;
                           &#x27;record class must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = recordClazz;\n&#x27;
                           &#x27;\t\tthis.schema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\tthis.schemaString = schema.toString();\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t/**\n&#x27;
                           &#x27;\t * Creates an Avro serialization schema for the &#x27;
                           &#x27;given Avro schema string.\n&#x27;
                           &#x27;\t *\n&#x27;
                           &#x27;\t * @param avroSchemaString Avro schema string &#x27;
                           &quot;used to serialize Flink&#x27;s row to Avro&#x27;s record\n&quot;
                           &#x27;\t */\n&#x27;
                           &#x27;\tpublic AvroCRowSerializationSchema(String &#x27;
                           &#x27;avroSchemaString,String updateMode) {\n&#x27;
                           &#x27;\t\tPreconditions.checkNotNull(avroSchemaString, &#x27;
                           &#x27;&quot;Avro schema must not be null.&quot;);\n&#x27;
                           &#x27;\t\tthis.recordClazz = null;\n&#x27;
                           &#x27;\t\tthis.schemaString = avroSchemaString;\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tthis.schema = new &#x27;
                           &#x27;Schema.Parser().parse(avroSchemaString);\n&#x27;
                           &#x27;\t\t} catch (SchemaParseException e) {\n&#x27;
                           &#x27;\t\t\tthrow new IllegalArgumentException(&quot;Could &#x27;
                           &#x27;not parse Avro schema string.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthis.datumWriter = new &#x27;
                           &#x27;GenericDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tthis.arrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tthis.encoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t\tthis.updateMode = updateMode;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic byte[] serialize(CRow crow) {\n&#x27;
                           &#x27;\t\ttry {\n&#x27;
                           &#x27;\t\t\tRow row = crow.row();\n&#x27;
                           &#x27;\t\t\tboolean change = crow.change();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\t// convert to record\n&#x27;
                           &#x27;\t\t\tfinal GenericRecord record = &#x27;
                           &#x27;convertRowToAvroRecord(schema, row);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tdealRetractField(change, record);\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\t\tarrayOutputStream.reset();\n&#x27;
                           &#x27;\t\t\tdatumWriter.write(record, encoder);\n&#x27;
                           &#x27;\t\t\tencoder.flush();\n&#x27;
                           &#x27;\t\t\treturn arrayOutputStream.toByteArray();\n&#x27;
                           &#x27;\t\t} catch (Exception e) {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Failed to &#x27;
                           &#x27;serialize row.&quot;, e);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprotected void dealRetractField(boolean change, &#x27;
                           &#x27;GenericRecord record) {\n&#x27;
                           &#x27;\t\tschema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.findFirst()\n&#x27;
                           &#x27;\t\t\t\t.ifPresent(field -&gt; {\n&#x27;
                           &#x27;\t\t\t\t\tif &#x27;
                           &#x27;(StringUtils.equalsIgnoreCase(updateMode, &#x27;
                           &#x27;EUpdateMode.UPSERT.name())) {\n&#x27;
                           &#x27;\t\t\t\t\t\trecord.put(retractKey, &#x27;
                           &#x27;convertFlinkType(field.schema(), change));\n&#x27;
                           &#x27;\t\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\t});\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic boolean equals(Object o) {\n&#x27;
                           &#x27;\t\tif (this == o) {\n&#x27;
                           &#x27;\t\t\treturn true;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tif (o == null || getClass() != o.getClass()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\treturn false;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tfinal AvroCRowSerializationSchema that = &#x27;
                           &#x27;(AvroCRowSerializationSchema) o;\n&#x27;
                           &#x27;\t\treturn Objects.equals(recordClazz, &#x27;
                           &#x27;that.recordClazz) &amp;&amp; Objects.equals(schemaString, &#x27;
                           &#x27;that.schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@Override\n&#x27;
                           &#x27;\tpublic int hashCode() {\n&#x27;
                           &#x27;\t\treturn Objects.hash(recordClazz, &#x27;
                           &#x27;schemaString);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t// &#x27;
                           &#x27;--------------------------------------------------------------------------------------------\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate GenericRecord &#x27;
                           &#x27;convertRowToAvroRecord(Schema schema, Row row) {\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal List&lt;Schema.Field&gt; fields = &#x27;
                           &#x27;schema.getFields()\n&#x27;
                           &#x27;\t\t\t\t.stream()\n&#x27;
                           &#x27;\t\t\t\t.filter(field -&gt; &#x27;
                           &#x27;!StringUtils.equalsIgnoreCase(field.name(), &#x27;
                           &#x27;retractKey))\n&#x27;
                           &#x27;\t\t\t\t.collect(Collectors.toList());\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tfinal int length = fields.size();\n&#x27;
                           &#x27;\t\tfinal GenericRecord record = new &#x27;
                           &#x27;GenericData.Record(schema);\n&#x27;
                           &#x27;\t\tfor (int i = 0; i &lt; length; i++) {\n&#x27;
                           &#x27;\t\t\tfinal Schema.Field field = fields.get(i);\n&#x27;
                           &#x27;\t\t\trecord.put(i, &#x27;
                           &#x27;convertFlinkType(field.schema(), &#x27;
                           &#x27;row.getField(i)));\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\treturn record;\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate Object convertFlinkType(Schema schema, &#x27;
                           &#x27;Object object) {\n&#x27;
                           &#x27;\t\tif (object == null) {\n&#x27;
                           &#x27;\t\t\treturn null;\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tswitch (schema.getType()) {\n&#x27;
                           &#x27;\t\t\tcase RECORD:\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Row) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertRowToAvroRecord(schema, &#x27;
                           &#x27;(Row) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\tthrow new IllegalStateException(&quot;Row &#x27;
                           &#x27;expected but was: &quot; + object.getClass());\n&#x27;
                           &#x27;\t\t\tcase ENUM:\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.EnumSymbol(schema, &#x27;
                           &#x27;object.toString());\n&#x27;
                           &#x27;\t\t\tcase ARRAY:\n&#x27;
                           &#x27;\t\t\t\tfinal Schema elementSchema = &#x27;
                           &#x27;schema.getElementType();\n&#x27;
                           &#x27;\t\t\t\tfinal Object[] array = (Object[]) object;\n&#x27;
                           &#x27;\t\t\t\tfinal GenericData.Array&lt;Object&gt; &#x27;
                           &#x27;convertedArray = new &#x27;
                           &#x27;GenericData.Array&lt;&gt;(array.length, schema);\n&#x27;
                           &#x27;\t\t\t\tfor (Object element : array) {\n&#x27;
                           &#x27;\t\t\t\t\t&#x27;
                           &#x27;convertedArray.add(convertFlinkType(elementSchema, &#x27;
                           &#x27;element));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedArray;\n&#x27;
                           &#x27;\t\t\tcase MAP:\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;?, ?&gt; map = (Map&lt;?, ?&gt;) object;\n&#x27;
                           &#x27;\t\t\t\tfinal Map&lt;Utf8, Object&gt; convertedMap = new &#x27;
                           &#x27;HashMap&lt;&gt;();\n&#x27;
                           &#x27;\t\t\t\tfor (Map.Entry&lt;?, ?&gt; entry : &#x27;
                           &#x27;map.entrySet()) {\n&#x27;
                           &#x27;\t\t\t\t\tconvertedMap.put(\n&#x27;
                           &#x27;\t\t\t\t\t\tnew Utf8(entry.getKey().toString()),\n&#x27;
                           &#x27;\t\t\t\t\t\t&#x27;
                           &#x27;convertFlinkType(schema.getValueType(), &#x27;
                           &#x27;entry.getValue()));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertedMap;\n&#x27;
                           &#x27;\t\t\tcase UNION:\n&#x27;
                           &#x27;\t\t\t\tfinal List&lt;Schema&gt; types = &#x27;
                           &#x27;schema.getTypes();\n&#x27;
                           &#x27;\t\t\t\tfinal int size = types.size();\n&#x27;
                           &#x27;\t\t\t\tfinal Schema actualSchema;\n&#x27;
                           &#x27;\t\t\t\tif (size == 2 &amp;&amp; types.get(0).getType() == &#x27;
                           &#x27;Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(1);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 2 &amp;&amp; &#x27;
                           &#x27;types.get(1).getType() == Schema.Type.NULL) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else if (size == 1) {\n&#x27;
                           &#x27;\t\t\t\t\tactualSchema = types.get(0);\n&#x27;
                           &#x27;\t\t\t\t} else {\n&#x27;
                           &#x27;\t\t\t\t\t// generic type\n&#x27;
                           &#x27;\t\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn convertFlinkType(actualSchema, &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\tcase FIXED:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn new GenericData.Fixed(\n&#x27;
                           &#x27;\t\t\t\t\t\tschema,\n&#x27;
                           &#x27;\t\t\t\t\t\tconvertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn new GenericData.Fixed(schema, &#x27;
                           &#x27;(byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase STRING:\n&#x27;
                           &#x27;\t\t\t\treturn new Utf8(object.toString());\n&#x27;
                           &#x27;\t\t\tcase BYTES:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof BigDecimal) {\n&#x27;
                           &#x27;\t\t\t\t\treturn &#x27;
                           &#x27;ByteBuffer.wrap(convertFromDecimal(schema, &#x27;
                           &#x27;(BigDecimal) object));\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn ByteBuffer.wrap((byte[]) object);\n&#x27;
                           &#x27;\t\t\tcase INT:\n&#x27;
                           &#x27;\t\t\t\t// check for logical types\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Date) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromDate(schema, (Date) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t} else if (object instanceof Time) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTime(schema, (Time) &#x27;
                           &#x27;object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase LONG:\n&#x27;
                           &#x27;\t\t\t\t// check for logical type\n&#x27;
                           &#x27;\t\t\t\tif (object instanceof Timestamp) {\n&#x27;
                           &#x27;\t\t\t\t\treturn convertFromTimestamp(schema, &#x27;
                           &#x27;(Timestamp) object);\n&#x27;
                           &#x27;\t\t\t\t}\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tcase FLOAT:\n&#x27;
                           &#x27;\t\t\tcase DOUBLE:\n&#x27;
                           &#x27;\t\t\tcase BOOLEAN:\n&#x27;
                           &#x27;\t\t\t\treturn object;\n&#x27;
                           &#x27;\t\t\tdefault:\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tthrow new RuntimeException(&quot;Unsupported Avro &#x27;
                           &#x27;type:&quot; + schema);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate byte[] convertFromDecimal(Schema schema, &#x27;
                           &#x27;BigDecimal decimal) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType instanceof &#x27;
                           &#x27;LogicalTypes.Decimal) {\n&#x27;
                           &#x27;\t\t\tfinal LogicalTypes.Decimal decimalType = &#x27;
                           &#x27;(LogicalTypes.Decimal) logicalType;\n&#x27;
                           &#x27;\t\t\t// rescale to target type\n&#x27;
                           &#x27;\t\t\tfinal BigDecimal rescaled = &#x27;
                           &#x27;decimal.setScale(decimalType.getScale(), &#x27;
                           &#x27;BigDecimal.ROUND_UNNECESSARY);\n&#x27;
                           &#x27;\t\t\t// byte array must contain the &#x27;
                           &quot;two&#x27;s-complement representation of the\n&quot;
                           &#x27;\t\t\t// unscaled integer value in big-endian byte &#x27;
                           &#x27;order\n&#x27;
                           &#x27;\t\t\treturn &#x27;
                           &#x27;decimal.unscaledValue().toByteArray();\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;decimal type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromDate(Schema schema, Date &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.date()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted / 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported date &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate int convertFromTime(Schema schema, Time &#x27;
                           &#x27;date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == LogicalTypes.timeMillis()) &#x27;
                           &#x27;{\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\tfinal long converted = time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t\treturn (int) (converted % 86400000L);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported time &#x27;
                           &#x27;type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate long convertFromTimestamp(Schema schema, &#x27;
                           &#x27;Timestamp date) {\n&#x27;
                           &#x27;\t\tfinal LogicalType logicalType = &#x27;
                           &#x27;schema.getLogicalType();\n&#x27;
                           &#x27;\t\tif (logicalType == &#x27;
                           &#x27;LogicalTypes.timestampMillis()) {\n&#x27;
                           &#x27;\t\t\t// adopted from Apache Calcite\n&#x27;
                           &#x27;\t\t\tfinal long time = date.getTime();\n&#x27;
                           &#x27;\t\t\treturn time + (long) &#x27;
                           &#x27;LOCAL_TZ.getOffset(time);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tthrow new RuntimeException(&quot;Unsupported &#x27;
                           &#x27;timestamp type.&quot;);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\tprivate void writeObject(ObjectOutputStream &#x27;
                           &#x27;outputStream) throws IOException {\n&#x27;
                           &#x27;\t\toutputStream.writeObject(recordClazz);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(schemaString); // &#x27;
                           &#x27;support for null\n&#x27;
                           &#x27;\t\toutputStream.writeObject(retractKey);\n&#x27;
                           &#x27;\t\toutputStream.writeObject(updateMode);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t@SuppressWarnings(&quot;unchecked&quot;)\n&#x27;
                           &#x27;\tprivate void readObject(ObjectInputStream &#x27;
                           &#x27;inputStream) throws ClassNotFoundException, &#x27;
                           &#x27;IOException {\n&#x27;
                           &#x27;\t\trecordClazz = (Class&lt;? extends &#x27;
                           &#x27;SpecificRecord&gt;) inputStream.readObject();\n&#x27;
                           &#x27;\t\tschemaString = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tif (recordClazz != null) {\n&#x27;
                           &#x27;\t\t\tschema = &#x27;
                           &#x27;SpecificData.get().getSchema(recordClazz);\n&#x27;
                           &#x27;\t\t} else {\n&#x27;
                           &#x27;\t\t\tschema = new &#x27;
                           &#x27;Schema.Parser().parse(schemaString);\n&#x27;
                           &#x27;\t\t}\n&#x27;
                           &#x27;\t\tretractKey = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\t\tupdateMode = (String) &#x27;
                           &#x27;inputStream.readObject();\n&#x27;
                           &#x27;\n&#x27;
                           &#x27;\t\tdatumWriter = new &#x27;
                           &#x27;SpecificDatumWriter&lt;&gt;(schema);\n&#x27;
                           &#x27;\t\tarrayOutputStream = new &#x27;
                           &#x27;ByteArrayOutputStream();\n&#x27;
                           &#x27;\t\tencoder = &#x27;
                           &#x27;EncoderFactory.get().binaryEncoder(arrayOutputStream, &#x27;
                           &#x27;null);\n&#x27;
                           &#x27;\t}\n&#x27;
                           &#x27;}\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;spork&#x27;, &#x27;baseline&#x27;, &#x27;jfstmerge&#x27;}}]]</pre>
          </body>
        </html>
        