<!DOCTYPE html>
<html lang="en">
          <head>
            <meta charset="utf-8">
            <title>581 chunks</title>
                <style>
                    #top {
                        height: 48vh;
                        overflow-y: auto;
                    }
                    #bottom {
                        height: 48vh;
                        overflow-y: auto;
                    }
                </style>
          </head>
          <body>
            <pre>[[{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;/**\n&#x27;
                         &#x27; * Licensed to the Apache Software Foundation (ASF) &#x27;
                         &#x27;under one\n&#x27;
                         &#x27; * or more contributor license agreements.  See the &#x27;
                         &#x27;NOTICE file\n&#x27;
                         &#x27; * distributed with this work for additional &#x27;
                         &#x27;information\n&#x27;
                         &#x27; * regarding copyright ownership.  The ASF licenses &#x27;
                         &#x27;this file\n&#x27;
                         &#x27; * to you under the Apache License, Version 2.0 &#x27;
                         &#x27;(the\n&#x27;
                         &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                         &#x27;compliance\n&#x27;
                         &#x27; * with the License.  You may obtain a copy of the &#x27;
                         &#x27;License at\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * Unless required by applicable law or agreed to in &#x27;
                         &#x27;writing, software\n&#x27;
                         &#x27; * distributed under the License is distributed on &#x27;
                         &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                         &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                         &#x27;either express or implied.\n&#x27;
                         &#x27; * See the License for the specific language &#x27;
                         &#x27;governing permissions and\n&#x27;
                         &#x27; * limitations under the License.\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;package com.dtstack.flink.sql.sink.kafka;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStream;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.Kafka011TableSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.utils.TableConnectorUtils;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * Reason: add schema info\n&#x27;
                         &#x27; * Date: 2019/4/8\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public class CustomerKafka11JsonTableSink extends &#x27;
                         &#x27;Kafka011TableSink {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tprotected SerializationSchema schema;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tpublic CustomerKafka11JsonTableSink(TableSchema &#x27;
                         &#x27;schema,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tString topic,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tProperties properties,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\t&#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tSerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema) {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tsuper(schema, topic, properties, partitioner, &#x27;
                         &#x27;serializationSchema);\n&#x27;
                         &#x27;\t\tthis.schema = serializationSchema;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tprotected SinkFunction&lt;Row&gt; &#x27;
                         &#x27;createKafkaProducer(String topic, Properties &#x27;
                         &#x27;properties, SerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema, &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; optional) {\n&#x27;
                         &#x27;\t\treturn new &#x27;
                         &#x27;CustomerFlinkKafkaProducer011&lt;Row&gt;(topic, &#x27;
                         &#x27;serializationSchema, properties);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic void emitDataStream(DataStream&lt;Row&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;\t\tconsumeDataStream(dataStream);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Row&gt; dataStream) {\n&#x27;
                         &#x27;\t\tSinkFunction&lt;Row&gt; kafkaProducer = &#x27;
                         &#x27;createKafkaProducer(topic, properties, schema, &#x27;
                         &#x27;partitioner);\n&#x27;
                         &#x27;\t\t// always enable flush on checkpoint to achieve &#x27;
                         &#x27;at-least-once if query runs with checkpointing &#x27;
                         &#x27;enabled.\n&#x27;
                         &#x27;\t\t//kafkaProducer.setFlushOnCheckpoint(true);\n&#x27;
                         &#x27;\t\tDataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                         &#x27;getFieldNames()));\n&#x27;
                         &#x27;\t\treturn dataStreamSink;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;/**\n&#x27;
                         &#x27; * Licensed to the Apache Software Foundation (ASF) &#x27;
                         &#x27;under one\n&#x27;
                         &#x27; * or more contributor license agreements.  See the &#x27;
                         &#x27;NOTICE file\n&#x27;
                         &#x27; * distributed with this work for additional &#x27;
                         &#x27;information\n&#x27;
                         &#x27; * regarding copyright ownership.  The ASF licenses &#x27;
                         &#x27;this file\n&#x27;
                         &#x27; * to you under the Apache License, Version 2.0 &#x27;
                         &#x27;(the\n&#x27;
                         &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                         &#x27;compliance\n&#x27;
                         &#x27; * with the License.  You may obtain a copy of the &#x27;
                         &#x27;License at\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * Unless required by applicable law or agreed to in &#x27;
                         &#x27;writing, software\n&#x27;
                         &#x27; * distributed under the License is distributed on &#x27;
                         &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                         &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                         &#x27;either express or implied.\n&#x27;
                         &#x27; * See the License for the specific language &#x27;
                         &#x27;governing permissions and\n&#x27;
                         &#x27; * limitations under the License.\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;package com.dtstack.flink.sql.sink.kafka;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStream;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.Kafka011TableSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.utils.TableConnectorUtils;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * Reason: add schema info\n&#x27;
                         &#x27; * Date: 2019/4/8\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public class CustomerKafka11JsonTableSink extends &#x27;
                         &#x27;Kafka011TableSink {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tprotected SerializationSchema schema;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tpublic CustomerKafka11JsonTableSink(TableSchema &#x27;
                         &#x27;schema,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tString topic,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tProperties properties,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\t&#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tSerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema) {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tsuper(schema, topic, properties, partitioner, &#x27;
                         &#x27;serializationSchema);\n&#x27;
                         &#x27;\t\tthis.schema = serializationSchema;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tprotected SinkFunction&lt;Row&gt; &#x27;
                         &#x27;createKafkaProducer(String topic, Properties &#x27;
                         &#x27;properties, SerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema, &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; optional) {\n&#x27;
                         &#x27;\t\treturn new &#x27;
                         &#x27;CustomerFlinkKafkaProducer011&lt;Row&gt;(topic, &#x27;
                         &#x27;serializationSchema, properties);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic void emitDataStream(DataStream&lt;Row&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;\t\tconsumeDataStream(dataStream);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Row&gt; dataStream) {\n&#x27;
                         &#x27;\t\tSinkFunction&lt;Row&gt; kafkaProducer = &#x27;
                         &#x27;createKafkaProducer(topic, properties, schema, &#x27;
                         &#x27;partitioner);\n&#x27;
                         &#x27;\t\t// always enable flush on checkpoint to achieve &#x27;
                         &#x27;at-least-once if query runs with checkpointing &#x27;
                         &#x27;enabled.\n&#x27;
                         &#x27;\t\t//kafkaProducer.setFlushOnCheckpoint(true);\n&#x27;
                         &#x27;\t\tDataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                         &#x27;getFieldNames()));\n&#x27;
                         &#x27;\t\treturn dataStreamSink;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;/**\n&#x27;
                         &#x27; * Licensed to the Apache Software Foundation (ASF) &#x27;
                         &#x27;under one\n&#x27;
                         &#x27; * or more contributor license agreements.  See the &#x27;
                         &#x27;NOTICE file\n&#x27;
                         &#x27; * distributed with this work for additional &#x27;
                         &#x27;information\n&#x27;
                         &#x27; * regarding copyright ownership.  The ASF licenses &#x27;
                         &#x27;this file\n&#x27;
                         &#x27; * to you under the Apache License, Version 2.0 &#x27;
                         &#x27;(the\n&#x27;
                         &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                         &#x27;compliance\n&#x27;
                         &#x27; * with the License.  You may obtain a copy of the &#x27;
                         &#x27;License at\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * Unless required by applicable law or agreed to in &#x27;
                         &#x27;writing, software\n&#x27;
                         &#x27; * distributed under the License is distributed on &#x27;
                         &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                         &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                         &#x27;either express or implied.\n&#x27;
                         &#x27; * See the License for the specific language &#x27;
                         &#x27;governing permissions and\n&#x27;
                         &#x27; * limitations under the License.\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;package com.dtstack.flink.sql.sink.kafka;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStream;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.Kafka011TableSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.utils.TableConnectorUtils;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * Reason: add schema info\n&#x27;
                         &#x27; * Date: 2019/4/8\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public class CustomerKafka11JsonTableSink extends &#x27;
                         &#x27;Kafka011TableSink {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tprotected SerializationSchema schema;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tpublic CustomerKafka11JsonTableSink(TableSchema &#x27;
                         &#x27;schema,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tString topic,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tProperties properties,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\t&#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tSerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema) {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tsuper(schema, topic, properties, partitioner, &#x27;
                         &#x27;serializationSchema);\n&#x27;
                         &#x27;\t\tthis.schema = serializationSchema;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tprotected SinkFunction&lt;Row&gt; &#x27;
                         &#x27;createKafkaProducer(String topic, Properties &#x27;
                         &#x27;properties, SerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema, &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; optional) {\n&#x27;
                         &#x27;\t\treturn new &#x27;
                         &#x27;CustomerFlinkKafkaProducer011&lt;Row&gt;(topic, &#x27;
                         &#x27;serializationSchema, properties);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic void emitDataStream(DataStream&lt;Row&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;\t\tconsumeDataStream(dataStream);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Row&gt; dataStream) {\n&#x27;
                         &#x27;\t\tSinkFunction&lt;Row&gt; kafkaProducer = &#x27;
                         &#x27;createKafkaProducer(topic, properties, schema, &#x27;
                         &#x27;partitioner);\n&#x27;
                         &#x27;\t\t// always enable flush on checkpoint to achieve &#x27;
                         &#x27;at-least-once if query runs with checkpointing &#x27;
                         &#x27;enabled.\n&#x27;
                         &#x27;\t\t//kafkaProducer.setFlushOnCheckpoint(true);\n&#x27;
                         &#x27;\t\tDataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                         &#x27;getFieldNames()));\n&#x27;
                         &#x27;\t\treturn dataStreamSink;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;spork&#x27;, &#x27;baseline&#x27;, &#x27;jfstmerge&#x27;}}],
 [{&#x27;eq&#x27;: [{&#x27;CHUNK_OURS&#x27;: &#x27;/**\n&#x27;
                         &#x27; * Licensed to the Apache Software Foundation (ASF) &#x27;
                         &#x27;under one\n&#x27;
                         &#x27; * or more contributor license agreements.  See the &#x27;
                         &#x27;NOTICE file\n&#x27;
                         &#x27; * distributed with this work for additional &#x27;
                         &#x27;information\n&#x27;
                         &#x27; * regarding copyright ownership.  The ASF licenses &#x27;
                         &#x27;this file\n&#x27;
                         &#x27; * to you under the Apache License, Version 2.0 &#x27;
                         &#x27;(the\n&#x27;
                         &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                         &#x27;compliance\n&#x27;
                         &#x27; * with the License.  You may obtain a copy of the &#x27;
                         &#x27;License at\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * Unless required by applicable law or agreed to in &#x27;
                         &#x27;writing, software\n&#x27;
                         &#x27; * distributed under the License is distributed on &#x27;
                         &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                         &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                         &#x27;either express or implied.\n&#x27;
                         &#x27; * See the License for the specific language &#x27;
                         &#x27;governing permissions and\n&#x27;
                         &#x27; * limitations under the License.\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;package com.dtstack.flink.sql.sink.kafka;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStream;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.Kafka011TableSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.utils.TableConnectorUtils;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * Reason: add schema info\n&#x27;
                         &#x27; * Date: 2019/4/8\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public class CustomerKafka11JsonTableSink extends &#x27;
                         &#x27;Kafka011TableSink {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tprotected SerializationSchema schema;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tpublic CustomerKafka11JsonTableSink(TableSchema &#x27;
                         &#x27;schema,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tString topic,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tProperties properties,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\t&#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tSerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema) {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tsuper(schema, topic, properties, partitioner, &#x27;
                         &#x27;serializationSchema);\n&#x27;
                         &#x27;\t\tthis.schema = serializationSchema;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tprotected SinkFunction&lt;Row&gt; &#x27;
                         &#x27;createKafkaProducer(String topic, Properties &#x27;
                         &#x27;properties, SerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema, &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; optional) {\n&#x27;
                         &#x27;\t\treturn new &#x27;
                         &#x27;CustomerFlinkKafkaProducer011&lt;Row&gt;(topic, &#x27;
                         &#x27;serializationSchema, properties);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic void emitDataStream(DataStream&lt;Row&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;\t\tconsumeDataStream(dataStream);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Row&gt; dataStream) {\n&#x27;
                         &#x27;\t\tSinkFunction&lt;Row&gt; kafkaProducer = &#x27;
                         &#x27;createKafkaProducer(topic, properties, schema, &#x27;
                         &#x27;partitioner);\n&#x27;
                         &#x27;\t\t// always enable flush on checkpoint to achieve &#x27;
                         &#x27;at-least-once if query runs with checkpointing &#x27;
                         &#x27;enabled.\n&#x27;
                         &#x27;\t\t//kafkaProducer.setFlushOnCheckpoint(true);\n&#x27;
                         &#x27;\t\tDataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                         &#x27;getFieldNames()));\n&#x27;
                         &#x27;\t\treturn dataStreamSink;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;/**\n&#x27;
                         &#x27; * Licensed to the Apache Software Foundation (ASF) &#x27;
                         &#x27;under one\n&#x27;
                         &#x27; * or more contributor license agreements.  See the &#x27;
                         &#x27;NOTICE file\n&#x27;
                         &#x27; * distributed with this work for additional &#x27;
                         &#x27;information\n&#x27;
                         &#x27; * regarding copyright ownership.  The ASF licenses &#x27;
                         &#x27;this file\n&#x27;
                         &#x27; * to you under the Apache License, Version 2.0 &#x27;
                         &#x27;(the\n&#x27;
                         &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                         &#x27;compliance\n&#x27;
                         &#x27; * with the License.  You may obtain a copy of the &#x27;
                         &#x27;License at\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * Unless required by applicable law or agreed to in &#x27;
                         &#x27;writing, software\n&#x27;
                         &#x27; * distributed under the License is distributed on &#x27;
                         &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                         &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                         &#x27;either express or implied.\n&#x27;
                         &#x27; * See the License for the specific language &#x27;
                         &#x27;governing permissions and\n&#x27;
                         &#x27; * limitations under the License.\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;package com.dtstack.flink.sql.sink.kafka;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStream;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.Kafka011TableSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.utils.TableConnectorUtils;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * Reason: add schema info\n&#x27;
                         &#x27; * Date: 2019/4/8\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public class CustomerKafka11JsonTableSink extends &#x27;
                         &#x27;Kafka011TableSink {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tprotected SerializationSchema schema;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tpublic CustomerKafka11JsonTableSink(TableSchema &#x27;
                         &#x27;schema,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tString topic,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tProperties properties,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\t&#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tSerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema) {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tsuper(schema, topic, properties, partitioner, &#x27;
                         &#x27;serializationSchema);\n&#x27;
                         &#x27;\t\tthis.schema = serializationSchema;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tprotected SinkFunction&lt;Row&gt; &#x27;
                         &#x27;createKafkaProducer(String topic, Properties &#x27;
                         &#x27;properties, SerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema, &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; optional) {\n&#x27;
                         &#x27;\t\treturn new &#x27;
                         &#x27;CustomerFlinkKafkaProducer011&lt;Row&gt;(topic, &#x27;
                         &#x27;serializationSchema, properties);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic void emitDataStream(DataStream&lt;Row&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;\t\tconsumeDataStream(dataStream);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Row&gt; dataStream) {\n&#x27;
                         &#x27;\t\tSinkFunction&lt;Row&gt; kafkaProducer = &#x27;
                         &#x27;createKafkaProducer(topic, properties, schema, &#x27;
                         &#x27;partitioner);\n&#x27;
                         &#x27;\t\t// always enable flush on checkpoint to achieve &#x27;
                         &#x27;at-least-once if query runs with checkpointing &#x27;
                         &#x27;enabled.\n&#x27;
                         &#x27;\t\t//kafkaProducer.setFlushOnCheckpoint(true);\n&#x27;
                         &#x27;\t\tDataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                         &#x27;getFieldNames()));\n&#x27;
                         &#x27;\t\treturn dataStreamSink;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;&#x27;},
          {&#x27;CHUNK_OURS&#x27;: &#x27;/**\n&#x27;
                         &#x27; * Licensed to the Apache Software Foundation (ASF) &#x27;
                         &#x27;under one\n&#x27;
                         &#x27; * or more contributor license agreements.  See the &#x27;
                         &#x27;NOTICE file\n&#x27;
                         &#x27; * distributed with this work for additional &#x27;
                         &#x27;information\n&#x27;
                         &#x27; * regarding copyright ownership.  The ASF licenses &#x27;
                         &#x27;this file\n&#x27;
                         &#x27; * to you under the Apache License, Version 2.0 &#x27;
                         &#x27;(the\n&#x27;
                         &#x27; * &quot;License&quot;); you may not use this file except in &#x27;
                         &#x27;compliance\n&#x27;
                         &#x27; * with the License.  You may obtain a copy of the &#x27;
                         &#x27;License at\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * http://www.apache.org/licenses/LICENSE-2.0\n&#x27;
                         &#x27; * &lt;p&gt;\n&#x27;
                         &#x27; * Unless required by applicable law or agreed to in &#x27;
                         &#x27;writing, software\n&#x27;
                         &#x27; * distributed under the License is distributed on &#x27;
                         &#x27;an &quot;AS IS&quot; BASIS,\n&#x27;
                         &#x27; * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, &#x27;
                         &#x27;either express or implied.\n&#x27;
                         &#x27; * See the License for the specific language &#x27;
                         &#x27;governing permissions and\n&#x27;
                         &#x27; * limitations under the License.\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;package com.dtstack.flink.sql.sink.kafka;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.api.common.serialization.SerializationSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStream;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.datastream.DataStreamSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.api.functions.sink.SinkFunction;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.Kafka011TableSink;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.streaming.connectors.kafka.partitioner.FlinkKafkaPartitioner;\n&#x27;
                         &#x27;import org.apache.flink.table.api.TableSchema;\n&#x27;
                         &#x27;import &#x27;
                         &#x27;org.apache.flink.table.utils.TableConnectorUtils;\n&#x27;
                         &#x27;import org.apache.flink.types.Row;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;import java.util.Optional;\n&#x27;
                         &#x27;import java.util.Properties;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;/**\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * Reason: add schema info\n&#x27;
                         &#x27; * Date: 2019/4/8\n&#x27;
                         &#x27; * Company: www.dtstack.com\n&#x27;
                         &#x27; *\n&#x27;
                         &#x27; * @author maqi\n&#x27;
                         &#x27; */\n&#x27;
                         &#x27;public class CustomerKafka11JsonTableSink extends &#x27;
                         &#x27;Kafka011TableSink {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tprotected SerializationSchema schema;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\tpublic CustomerKafka11JsonTableSink(TableSchema &#x27;
                         &#x27;schema,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tString topic,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tProperties properties,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\t&#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; partitioner,\n&#x27;
                         &#x27;\t\t\t\t\t\t\t\t\t\tSerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema) {\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t\tsuper(schema, topic, properties, partitioner, &#x27;
                         &#x27;serializationSchema);\n&#x27;
                         &#x27;\t\tthis.schema = serializationSchema;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tprotected SinkFunction&lt;Row&gt; &#x27;
                         &#x27;createKafkaProducer(String topic, Properties &#x27;
                         &#x27;properties, SerializationSchema&lt;Row&gt; &#x27;
                         &#x27;serializationSchema, &#x27;
                         &#x27;Optional&lt;FlinkKafkaPartitioner&lt;Row&gt;&gt; optional) {\n&#x27;
                         &#x27;\t\treturn new &#x27;
                         &#x27;CustomerFlinkKafkaProducer011&lt;Row&gt;(topic, &#x27;
                         &#x27;serializationSchema, properties);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic void emitDataStream(DataStream&lt;Row&gt; &#x27;
                         &#x27;dataStream) {\n&#x27;
                         &#x27;\t\tconsumeDataStream(dataStream);\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;\n&#x27;
                         &#x27;\t@Override\n&#x27;
                         &#x27;\tpublic DataStreamSink&lt;Row&gt; &#x27;
                         &#x27;consumeDataStream(DataStream&lt;Row&gt; dataStream) {\n&#x27;
                         &#x27;\t\tSinkFunction&lt;Row&gt; kafkaProducer = &#x27;
                         &#x27;createKafkaProducer(topic, properties, schema, &#x27;
                         &#x27;partitioner);\n&#x27;
                         &#x27;\t\t// always enable flush on checkpoint to achieve &#x27;
                         &#x27;at-least-once if query runs with checkpointing &#x27;
                         &#x27;enabled.\n&#x27;
                         &#x27;\t\t//kafkaProducer.setFlushOnCheckpoint(true);\n&#x27;
                         &#x27;\t\tDataStreamSink&lt;Row&gt; dataStreamSink = &#x27;
                         &#x27;dataStream.addSink(kafkaProducer).name(TableConnectorUtils.generateRuntimeName(this.getClass(), &#x27;
                         &#x27;getFieldNames()));\n&#x27;
                         &#x27;\t\treturn dataStreamSink;\n&#x27;
                         &#x27;\t}\n&#x27;
                         &#x27;}\n&#x27;,
           &#x27;CHUNK_THEIRS&#x27;: &#x27;\n&#x27;}],
   &#x27;mergers&#x27;: {&#x27;spork&#x27;, &#x27;baseline&#x27;, &#x27;jfstmerge&#x27;}}]]</pre>
          </body>
        </html>
        