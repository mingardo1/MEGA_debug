<!DOCTYPE html>
    <html lang="en">
              <head>
                <meta charset="utf-8">
                <title>315</title>
                    <style>
                        #top {
                            height: 48vh;
                            overflow-y: auto;
                        }
                        #bottom {
                            height: 48vh;
                            overflow-y: auto;
                        }
                        abbr {
                          /* Here is the delay */
                          transition-delay:0s;
                        }
                    </style>
              </head>
              <body>
                <span style="height: 4vh">
                    315
                    <a href="314.html">prev</a>
                    <a href="316.html">next</a>
                    <a href="315_chunks.html">chunks</a>
                    <a href="index.html">index</a>
                    DTStack/flinkStreamSQL_c0f3b0e4e2273e5916d132c932856901824174ce_core/src/main/java/com/dtstack/flink/sql/exec/ExecuteProcessHelper.java
                    <textarea rows=1 onclick='navigator.clipboard.writeText(this.value)'>cd C:\studies\se\mega\git-analyzer-plus\notebooks\debug
del /Q *
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;c0f3b0e4e2273e5916d132c932856901824174ce:core/src/main/java/com/dtstack/flink/sql/exec/ExecuteProcessHelper.java&quot; &gt; committed.java
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;c0f3b0e4e2273e5916d132c932856901824174ce^1:core/src/main/java/com/dtstack/flink/sql/exec/ExecuteProcessHelper.java&quot; &gt; ours.java
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;c0f3b0e4e2273e5916d132c932856901824174ce^2:core/src/main/java/com/dtstack/flink/sql/exec/ExecuteProcessHelper.java&quot; &gt; theirs.java
git -C C:\studies\se\mega\project-dirs\projects_Java_desc-stars-1000\DTStack\flinkStreamSQL show &quot;ea52aeb8ee6b75aae5869b2b55fe05aba7514bd3:core/src/main/java/com/dtstack/flink/sql/exec/ExecuteProcessHelper.java&quot; &gt; base.java
copy ours.java 1ours.java
copy ours.java 2ours.java
copy theirs.java 1theirs.java
copy theirs.java 2theirs.java
copy base.java 1base.java
copy base.java 2base.java
&quot;C:\Program Files\Java\jdk1.8.0_241\bin\java.exe&quot; -Dfile.encoding=UTF-8 -jar &quot;C:\studies\se\jFSTMerge\build\libs\jFSTMerge-all.jar&quot; C:\studies\se\mega\git-analyzer-plus\notebooks\debug\1ours.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\1base.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\1theirs.java -o C:\studies\se\mega\git-analyzer-plus\notebooks\debug\jfstmerge.java --show-base
&quot;C:\Program Files\Eclipse Adoptium\jdk-17.0.11.9-hotspot\bin\java.exe&quot; -Dfile.encoding=UTF-8 -jar &quot;C:\studies\se\spork\target\spork-0.5.0-SNAPSHOT.jar&quot; C:\studies\se\mega\git-analyzer-plus\notebooks\debug\2ours.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\2base.java C:\studies\se\mega\git-analyzer-plus\notebooks\debug\2theirs.java -o C:\studies\se\mega\git-analyzer-plus\notebooks\debug\spork.java
del /Q 1*.java
del /Q 2*.java
del /Q jfstmerge.java.merge
</textarea>
                    {strict: [[b], [b], [b]], subset: [[b], [b], [b]]}
                </span>
                <div id="top">

                    <table>
                        <tr>
                            <th>line based (standard git)</th>
                            <th>jfstmerge</th>
                            <th>spork</th>
                        </tr>
                        <tr>
                            <td><pre>   1 /*
   2  * Licensed to the Apache Software Foundation (ASF) under one
   3  * or more contributor license agreements.  See the NOTICE file
   4  * distributed with this work for additional information
   5  * regarding copyright ownership.  The ASF licenses this file
   6  * to you under the Apache License, Version 2.0 (the
   7  * &quot;License&quot;); you may not use this file except in compliance
   8  * with the License.  You may obtain a copy of the License at
   9  *
  10  *     http://www.apache.org/licenses/LICENSE-2.0
  11  *
  12  * Unless required by applicable law or agreed to in writing, software
  13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15  * See the License for the specific language governing permissions and
  16  * limitations under the License.
  17  */
  18 
  19 package com.dtstack.flink.sql.exec;
  20 
  21 &lt;&lt;&lt;&lt;&lt;&lt;&lt; GitAnalyzerPlus_ours
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  22 import com.dtstack.flink.sql.parser.CreateFuncParser;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  23 import com.dtstack.flink.sql.parser.CreateTmpTableParser;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  24 import com.dtstack.flink.sql.parser.FlinkPlanner;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  25 import com.dtstack.flink.sql.parser.InsertSqlParser;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  26 import com.dtstack.flink.sql.parser.SqlParser;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  27 import com.dtstack.flink.sql.parser.SqlTree;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  28 import com.dtstack.flink.sql.util.TypeInfoDataTypeConverter;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  29 import org.apache.flink.api.common.typeinfo.TypeInformation;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  30 import org.apache.flink.api.java.typeutils.RowTypeInfo;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  31 import org.apache.flink.streaming.api.datastream.DataStream;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  32 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  33 import org.apache.flink.table.api.*;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  34 import org.apache.flink.table.api.java.StreamTableEnvironment;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  35 import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  36 import org.apache.flink.table.sinks.TableSink;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  37 </span>
  38 ||||||| GitAnalyzerPlus_base
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  39 import com.dtstack.flink.sql.parser.CreateFuncParser;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  40 import com.dtstack.flink.sql.parser.CreateTmpTableParser;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  41 import com.dtstack.flink.sql.parser.FlinkPlanner;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  42 import com.dtstack.flink.sql.parser.InsertSqlParser;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  43 import com.dtstack.flink.sql.parser.SqlParser;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  44 import com.dtstack.flink.sql.parser.SqlTree;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  45 import org.apache.flink.api.common.typeinfo.TypeInformation;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  46 import org.apache.flink.api.java.typeutils.RowTypeInfo;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  47 import org.apache.flink.streaming.api.datastream.DataStream;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  48 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  49 import org.apache.flink.table.api.*;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  50 import org.apache.flink.table.api.java.StreamTableEnvironment;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  51 import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  52 import org.apache.flink.table.sinks.TableSink;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  53 </span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  54 import com.dtstack.flink.sql.classloader.ClassLoaderManager;</span>
  55 =======
  56 &gt;&gt;&gt;&gt;&gt;&gt;&gt; GitAnalyzerPlus_theirs
  57 import com.dtstack.flink.sql.classloader.ClassLoaderManager;
  58 import com.dtstack.flink.sql.enums.ClusterMode;
  59 import com.dtstack.flink.sql.enums.ECacheType;
  60 import com.dtstack.flink.sql.enums.EPluginLoadMode;
  61 import com.dtstack.flink.sql.environment.MyLocalStreamEnvironment;
  62 import com.dtstack.flink.sql.environment.StreamEnvConfigManager;
  63 import com.dtstack.flink.sql.function.FunctionManager;
  64 import com.dtstack.flink.sql.option.OptionParser;
  65 import com.dtstack.flink.sql.option.Options;
  66 import com.dtstack.flink.sql.parser.CreateFuncParser;
  67 import com.dtstack.flink.sql.parser.CreateTmpTableParser;
  68 import com.dtstack.flink.sql.parser.FlinkPlanner;
  69 import com.dtstack.flink.sql.parser.InsertSqlParser;
  70 import com.dtstack.flink.sql.parser.SqlParser;
  71 import com.dtstack.flink.sql.parser.SqlTree;
  72 import com.dtstack.flink.sql.side.AbstractSideTableInfo;
  73 import com.dtstack.flink.sql.side.SideSqlExec;
  74 import com.dtstack.flink.sql.sink.StreamSinkFactory;
  75 import com.dtstack.flink.sql.source.StreamSourceFactory;
  76 import com.dtstack.flink.sql.table.AbstractSourceTableInfo;
  77 import com.dtstack.flink.sql.table.AbstractTableInfo;
  78 import com.dtstack.flink.sql.table.AbstractTargetTableInfo;
  79 import com.dtstack.flink.sql.util.DtStringUtil;
  80 import com.dtstack.flink.sql.util.PluginUtil;
  81 import com.dtstack.flink.sql.watermarker.WaterMarkerAssigner;
  82 import com.fasterxml.jackson.databind.ObjectMapper;
  83 import com.google.common.base.Preconditions;
  84 import com.google.common.base.Strings;
  85 import com.google.common.collect.Lists;
  86 import com.google.common.collect.Maps;
  87 import com.google.common.collect.Sets;
  88 import org.apache.calcite.sql.SqlInsert;
  89 import org.apache.calcite.sql.SqlNode;
  90 import org.apache.commons.io.Charsets;
  91 import org.apache.commons.lang3.StringUtils;
  92 &lt;&lt;&lt;&lt;&lt;&lt;&lt; GitAnalyzerPlus_ours
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  93 import org.apache.flink.table.types.DataType;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0">  94 import org.apache.flink.table.types.utils.TypeConversions;</span>
  95 ||||||| GitAnalyzerPlus_base
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  96 import com.google.common.base.Preconditions;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  97 import com.google.common.base.Strings;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  98 import com.google.common.collect.Lists;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0">  99 import com.google.common.collect.Maps;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 100 import com.google.common.collect.Sets;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 101 import org.apache.calcite.sql.SqlInsert;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 102 import org.apache.calcite.sql.SqlNode;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 103 import org.apache.commons.io.Charsets;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 104 import org.apache.commons.lang3.StringUtils;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 105 import org.slf4j.Logger;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 106 import org.slf4j.LoggerFactory;</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 107 </span>
 108 =======
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 109 import org.apache.flink.api.common.typeinfo.TypeInformation;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 110 import org.apache.flink.api.java.typeutils.RowTypeInfo;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 111 import org.apache.flink.streaming.api.datastream.DataStream;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 112 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 113 import org.apache.flink.table.api.EnvironmentSettings;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 114 import org.apache.flink.table.api.Table;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 115 import org.apache.flink.table.api.TableConfig;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 116 import org.apache.flink.table.api.TableEnvironment;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 117 import org.apache.flink.table.api.java.StreamTableEnvironment;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 118 import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;</span>
<span style="background-color: rgba(0, 0, 255, 0.24); margin: 0"> 119 import org.apache.flink.table.sinks.TableSink;</span>
 120 &gt;&gt;&gt;&gt;&gt;&gt;&gt; GitAnalyzerPlus_theirs
 121 import org.slf4j.Logger;
 122 import org.slf4j.LoggerFactory;
 123 
 124 import java.io.File;
 125 import java.lang.reflect.InvocationTargetException;
 126 import java.net.URL;
 127 import java.net.URLClassLoader;
 128 import java.net.URLDecoder;
 129 import java.time.ZoneId;
 130 import java.util.ArrayList;
 131 import java.util.Arrays;
 132 import java.util.List;
 133 import java.util.Map;
 134 import java.util.Properties;
 135 import java.util.Set;
 136 import java.util.TimeZone;
 137 &lt;&lt;&lt;&lt;&lt;&lt;&lt; GitAnalyzerPlus_ours
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0"> 138 import java.util.ArrayList;</span>
<span style="background-color: rgba(255, 167, 0, 0.24); margin: 0"> 139 import java.util.stream.Stream;</span>
 140 ||||||| GitAnalyzerPlus_base
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 141  *  任务执行时的流程方法</span>
<span style="background-color: rgba(0, 0, 0, 0.15); margin: 0"> 142  * Date: 2020/2/17</span>
 143 =======
 144 &gt;&gt;&gt;&gt;&gt;&gt;&gt; GitAnalyzerPlus_theirs
 145 
 146 /**
 147  *  任务执行时的流程方法
 148  * Date: 2020/2/17
 149  * Company: www.dtstack.com
 150  * @author maqi
 151  */
 152 public class ExecuteProcessHelper {
 153 
 154     private static final String CLASS_FILE_NAME_FMT = &quot;class_path_%d&quot;;
 155     private static final Logger LOG = LoggerFactory.getLogger(ExecuteProcessHelper.class);
 156     private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();
 157 
 158     private static final String TIME_ZONE = &quot;timezone&quot;;
 159 
 160     public static FlinkPlanner flinkPlanner = new FlinkPlanner();
 161 
 162     public static ParamsInfo parseParams(String[] args) throws Exception {
 163         LOG.info(&quot;------------program params-------------------------&quot;);
 164         Arrays.stream(args).forEach(arg -&gt; LOG.info(&quot;{}&quot;, arg));
 165         LOG.info(&quot;-------------------------------------------&quot;);
 166 
 167         OptionParser optionParser = new OptionParser(args);
 168         Options options = optionParser.getOptions();
 169 
 170         String sql = URLDecoder.decode(options.getSql(), Charsets.UTF_8.name());
 171         String name = options.getName();
 172         String localSqlPluginPath = options.getLocalSqlPluginPath();
 173         String remoteSqlPluginPath = options.getRemoteSqlPluginPath();
 174         String pluginLoadMode = options.getPluginLoadMode();
 175         String deployMode = options.getMode();
 176 
<abbr title=" 177         Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadMode),"> 177         Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadM🔵</abbr>
 178                 &quot;Non-local mode or shipfile deployment mode, remoteSqlPluginPath is required&quot;);
 179         String confProp = URLDecoder.decode(options.getConfProp(), Charsets.UTF_8.toString());
 180         Properties confProperties = PluginUtil.jsonStrToObject(confProp, Properties.class);
 181 
 182         List&lt;URL&gt; jarUrlList = getExternalJarUrls(options.getAddjar());
 183 
 184         return ParamsInfo.builder()
 185                 .setSql(sql)
 186                 .setName(name)
 187                 .setLocalSqlPluginPath(localSqlPluginPath)
 188                 .setRemoteSqlPluginPath(remoteSqlPluginPath)
 189                 .setPluginLoadMode(pluginLoadMode)
 190                 .setDeployMode(deployMode)
 191                 .setConfProp(confProperties)
 192                 .setJarUrlList(jarUrlList)
 193                 .build();
 194 
 195     }
 196 
 197     /**
 198      *   非local模式或者shipfile部署模式，remoteSqlPluginPath必填
 199      * @param remoteSqlPluginPath
 200      * @param deployMode
 201      * @param pluginLoadMode
 202      * @return
 203      */
<abbr title=" 204     public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoadMode) {"> 204     public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String 🔵</abbr>
 205         if (StringUtils.isEmpty(remoteSqlPluginPath)) {
 206             return StringUtils.equalsIgnoreCase(pluginLoadMode, EPluginLoadMode.SHIPFILE.name())
 207                     || StringUtils.equalsIgnoreCase(deployMode, ClusterMode.local.name());
 208         }
 209         return true;
 210     }
 211 
 212 
 213     public static StreamExecutionEnvironment getStreamExecution(ParamsInfo paramsInfo) throws Exception {
<abbr title=" 214         StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo.getDeployMode());"> 214         StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), p🔵</abbr>
 215         StreamTableEnvironment tableEnv = getStreamTableEnv(env, paramsInfo.getConfProp());
 216 
 217 
 218         SqlParser.setLocalSqlPluginRoot(paramsInfo.getLocalSqlPluginPath());
 219         SqlTree sqlTree = SqlParser.parseSql(paramsInfo.getSql(), paramsInfo.getPluginLoadMode());
 220 
 221         Map&lt;String, AbstractSideTableInfo&gt; sideTableMap = Maps.newHashMap();
 222         Map&lt;String, Table&gt; registerTableCache = Maps.newHashMap();
 223 
 224         //register udf
<abbr title=" 225         ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, paramsInfo.isGetPlan());"> 225         ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, p🔵</abbr>
 226         //register table schema
<abbr title=" 227         Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSqlPluginPath(),"> 227         Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.ge🔵</abbr>
<abbr title=" 228                 paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCache);"> 228                 paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, regist🔵</abbr>
 229         // cache classPathSets
 230         ExecuteProcessHelper.registerPluginUrlToCachedFile(env, classPathSets);
 231 
<abbr title=" 232         ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(),tableEnv, sqlTree, sideTableMap, registerTableCache);"> 232         ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadM🔵</abbr>
 233 
 234         if (env instanceof MyLocalStreamEnvironment) {
 235             ((MyLocalStreamEnvironment) env).setClasspaths(ClassLoaderManager.getClassPath());
 236         }
 237         return env;
 238     }
 239 
 240 
 241     public static List&lt;URL&gt; getExternalJarUrls(String addJarListStr) throws java.io.IOException {
 242         List&lt;URL&gt; jarUrlList = Lists.newArrayList();
 243         if (Strings.isNullOrEmpty(addJarListStr)) {
 244             return jarUrlList;
 245         }
 246 
<abbr title=" 247         List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name()), List.class);"> 247         List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.U🔵</abbr>
 248         //Get External jar to load
 249         for (String addJarPath : addJarFileList) {
 250             jarUrlList.add(new File(addJarPath).toURI().toURL());
 251         }
 252         return jarUrlList;
 253     }
 254 
 255     private static void sqlTranslation(String localSqlPluginPath,
 256                                        String pluginLoadMode,
 257                                        StreamTableEnvironment tableEnv,
 258                                        SqlTree sqlTree,Map&lt;String, AbstractSideTableInfo&gt; sideTableMap,
 259                                        Map&lt;String, Table&gt; registerTableCache) throws Exception {
 260 
 261         SideSqlExec sideSqlExec = new SideSqlExec();
 262         sideSqlExec.setLocalSqlPluginPath(localSqlPluginPath);
 263         sideSqlExec.setPluginLoadMode(pluginLoadMode);
 264 
 265         int scope = 0;
 266         for (CreateTmpTableParser.SqlParserResult result : sqlTree.getTmpSqlList()) {
<abbr title=" 267             sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, scope + &quot;&quot;);"> 267             sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, sco🔵</abbr>
 268             scope++;
 269         }
 270 
 271         for (InsertSqlParser.SqlParseResult result : sqlTree.getExecSqlList()) {
 272             if (LOG.isInfoEnabled()) {
 273                 LOG.info(&quot;exe-sql:\n&quot; + result.getExecSql());
 274             }
 275             boolean isSide = false;
 276             for (String tableName : result.getTargetTableList()) {
 277                 if (sqlTree.getTmpTableMap().containsKey(tableName)) {
 278                     CreateTmpTableParser.SqlParserResult tmp = sqlTree.getTmpTableMap().get(tableName);
 279                     String realSql = DtStringUtil.replaceIgnoreQuota(result.getExecSql(), &quot;`&quot;, &quot;&quot;);
 280 
 281                     SqlNode sqlNode = flinkPlanner.getParser().parse(realSql);
 282                     String tmpSql = ((SqlInsert) sqlNode).getSource().toString();
 283                     tmp.setExecSql(tmpSql);
<abbr title=" 284                     sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;);"> 284                     sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, s🔵</abbr>
 285                 } else {
 286                     for (String sourceTable : result.getSourceTableList()) {
 287                         if (sideTableMap.containsKey(sourceTable)) {
 288                             isSide = true;
 289                             break;
 290                         }
 291                     }
 292                     if (isSide) {
 293                         //sql-dimensional table contains the dimension table of execution
<abbr title=" 294                         sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, String.valueOf(scope));"> 294                         sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache,🔵</abbr>
 295                     } else {
 296                         LOG.info(&quot;----------exec sql without dimension join-----------&quot;);
<abbr title=" 297                         LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getExecSql());"> 297                         LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getEx🔵</abbr>
 298                         FlinkSQLExec.sqlUpdate(tableEnv, result.getExecSql());
 299                         if (LOG.isInfoEnabled()) {
 300                             LOG.info(&quot;exec sql: &quot; + result.getExecSql());
 301                         }
 302                     }
 303                 }
 304 
 305                 scope++;
 306             }
 307         }
 308     }
 309 
<abbr title=" 310     public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEnv, boolean getPlan)"> 310     public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironmen🔵</abbr>
 311             throws IllegalAccessException, InvocationTargetException {
 312         // udf和tableEnv须由同一个类加载器加载
 313         ClassLoader levelClassLoader = tableEnv.getClass().getClassLoader();
 314         URLClassLoader classLoader = null;
 315         List&lt;CreateFuncParser.SqlParserResult&gt; funcList = sqlTree.getFunctionList();
 316         for (CreateFuncParser.SqlParserResult funcInfo : funcList) {
 317             // 构建plan的情况下，udf和tableEnv不需要是同一个类加载器
 318             if (getPlan) {
 319                 URL[] urls = jarUrlList.toArray(new URL[0]);
 320                 classLoader = URLClassLoader.newInstance(urls);
 321             }
 322 
 323             //classloader
 324             if (classLoader == null) {
<abbr title=" 325                 classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, (URLClassLoader) levelClassLoader);"> 325                 classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, (URLClassLoader) levelClassLoad🔵</abbr>
 326             }
<abbr title=" 327             FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv, classLoader);"> 327             FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), 🔵</abbr>
 328         }
 329     }
 330 
 331     /**
 332      *    向Flink注册源表和结果表，返回执行时插件包的全路径
 333      * @param sqlTree
 334      * @param env
 335      * @param tableEnv
 336      * @param localSqlPluginPath
 337      * @param remoteSqlPluginPath
 338      * @param pluginLoadMode   插件加载模式 classpath or shipfile
 339      * @param sideTableMap
 340      * @param registerTableCache
 341      * @return
 342      * @throws Exception
 343      */
<abbr title=" 344     public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment tableEnv, String localSqlPluginPath,"> 344     public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvi🔵</abbr>
<abbr title=" 345                                          String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSideTableInfo&gt; sideTableMap, Map&lt;String, Table&gt; registerTableCache) throws Exception {"> 345                                          String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, A🔵</abbr>
 346         Set&lt;URL&gt; pluginClassPathSets = Sets.newHashSet();
 347         WaterMarkerAssigner waterMarkerAssigner = new WaterMarkerAssigner();
 348         for (AbstractTableInfo tableInfo : sqlTree.getTableInfoMap().values()) {
 349 
 350             if (tableInfo instanceof AbstractSourceTableInfo) {
 351 
 352                 AbstractSourceTableInfo sourceTableInfo = (AbstractSourceTableInfo) tableInfo;
<abbr title=" 353                 Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPath, pluginLoadMode);"> 353                 Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSq🔵</abbr>
 354                 tableEnv.registerTable(sourceTableInfo.getAdaptName(), table);
<abbr title=" 355                 //Note --- parameter conversion function can not be used inside a function of the type of polymerization"> 355                 //Note --- parameter conversion function can not be used inside a function of the type of🔵</abbr>
 356                 //Create table in which the function is arranged only need adaptation sql
 357                 String adaptSql = sourceTableInfo.getAdaptSelectSql();
 358                 Table adaptTable = adaptSql == null ? table : tableEnv.sqlQuery(adaptSql);
 359 
<abbr title=" 360                 RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().getFieldDataTypes()), adaptTable.getSchema().getFieldNames());"> 360                 RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().ge🔵</abbr>
 361                 DataStream adaptStream = tableEnv.toAppendStream(adaptTable, typeInfo);
 362 
 363                 String fields = String.join(&quot;,&quot;, typeInfo.getFieldNames());
 364 
 365                 if (waterMarkerAssigner.checkNeedAssignWaterMarker(sourceTableInfo)) {
<abbr title=" 366                     adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTableInfo);"> 366                     adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTabl🔵</abbr>
 367                     fields += &quot;,ROWTIME.ROWTIME&quot;;
 368                 } else {
 369                     fields += &quot;,PROCTIME.PROCTIME&quot;;
 370                 }
 371 
 372                 Table regTable = tableEnv.fromDataStream(adaptStream, fields);
 373                 tableEnv.registerTable(tableInfo.getName(), regTable);
 374                 if (LOG.isInfoEnabled()) {
 375                     LOG.info(&quot;registe table {} success.&quot;, tableInfo.getName());
 376                 }
 377                 registerTableCache.put(tableInfo.getName(), regTable);
 378 
<abbr title=" 379                 URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractSourceTableInfo.SOURCE_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 379                 URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(),🔵</abbr>
 380                 pluginClassPathSets.add(sourceTablePathUrl);
 381             } else if (tableInfo instanceof AbstractTargetTableInfo) {
 382 
<abbr title=" 383                 TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo, localSqlPluginPath, pluginLoadMode);"> 383                 TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo,🔵</abbr>
<abbr title=" 384                 TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses());"> 384                 TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses()🔵</abbr>
<abbr title=" 385                 tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableSink);"> 385                 tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableS🔵</abbr>
 386 
<abbr title=" 387                 URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTargetTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 387                 URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), A🔵</abbr>
 388                 pluginClassPathSets.add(sinkTablePathUrl);
 389             } else if (tableInfo instanceof AbstractSideTableInfo) {
<abbr title=" 390                 String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tableInfo).getCacheType()) ? &quot;all&quot; : &quot;async&quot;;"> 390                 String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tab🔵</abbr>
 391                 sideTableMap.put(tableInfo.getName(), (AbstractSideTableInfo) tableInfo);
 392 
<abbr title=" 393                 URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, AbstractSideTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 393                 URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperat🔵</abbr>
 394                 pluginClassPathSets.add(sideTablePathUrl);
 395             } else {
 396                 throw new RuntimeException(&quot;not support table type:&quot; + tableInfo.getType());
 397             }
 398         }
 399         if (localSqlPluginPath == null || localSqlPluginPath.isEmpty()) {
 400             return Sets.newHashSet();
 401         }
 402         return pluginClassPathSets;
 403     }
 404 
 405     /**
 406      *   perjob模式将job依赖的插件包路径存储到cacheFile，在外围将插件包路径传递给jobgraph
 407      * @param env
 408      * @param classPathSet
 409      */
<abbr title=" 410     public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSet) {"> 410     public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSe🔵</abbr>
 411         int i = 0;
 412         for (URL url : classPathSet) {
 413             String classFileName = String.format(CLASS_FILE_NAME_FMT, i);
 414             env.registerCachedFile(url.getPath(), classFileName, true);
 415             i++;
 416         }
 417     }
 418 
<abbr title=" 419     public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws Exception {"> 419     public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode🔵</abbr>
 420         StreamExecutionEnvironment env = !ClusterMode.local.name().equals(deployMode) ?
 421                 StreamExecutionEnvironment.getExecutionEnvironment() :
 422                 new MyLocalStreamEnvironment();
 423 
 424         StreamEnvConfigManager.streamExecutionEnvironmentConfig(env, confProperties);
 425         return env;
 426     }
 427 
 428 
<abbr title=" 429     public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperties) {"> 429     public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties con🔵</abbr>
 430         // use blink and streammode
 431         EnvironmentSettings settings = EnvironmentSettings.newInstance()
 432                 .useBlinkPlanner()
 433                 .inStreamingMode()
 434                 .build();
 435 
 436         TableConfig tableConfig = new TableConfig();
 437 
 438         timeZoneCheck(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()));
 439 
<abbr title=" 440         tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID())));"> 440         tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault(🔵</abbr>
 441 
 442         StreamTableEnvironment tableEnv = StreamTableEnvironmentImpl.create(env, settings, tableConfig);
 443         StreamEnvConfigManager.streamTableEnvironmentStateTTLConfig(tableEnv, confProperties);
 444         return tableEnv;
 445     }
 446 
 447     private static void timeZoneCheck(String timeZone) {
 448         ArrayList&lt;String&gt; zones = Lists.newArrayList(TimeZone.getAvailableIDs());
 449         if (!zones.contains(timeZone)){
 450             throw new IllegalArgumentException(String.format(&quot; timezone of %s is Incorrect!&quot;, timeZone));
 451         }
 452     }
 453 
 454     private static TypeInformation&lt;?&gt;[] fromDataTypeToLegacyInfo(DataType[] dataType) {
 455         return Stream.of(dataType)
 456                 .map(TypeInfoDataTypeConverter::toLegacyTypeInfo)
 457                 .toArray(TypeInformation[]::new);
 458     }
 459 }</pre></td>
                            <td><pre>   1 /*
   2  * Licensed to the Apache Software Foundation (ASF) under one
   3  * or more contributor license agreements.  See the NOTICE file
   4  * distributed with this work for additional information
   5  * regarding copyright ownership.  The ASF licenses this file
   6  * to you under the Apache License, Version 2.0 (the
   7  * &quot;License&quot;); you may not use this file except in compliance
   8  * with the License.  You may obtain a copy of the License at
   9  *
  10  *     http://www.apache.org/licenses/LICENSE-2.0
  11  *
  12  * Unless required by applicable law or agreed to in writing, software
  13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15  * See the License for the specific language governing permissions and
  16  * limitations under the License.
  17  */
  18 
  19 package com.dtstack.flink.sql.exec;
  20 
  21 import com.dtstack.flink.sql.classloader.ClassLoaderManager;
  22 import com.dtstack.flink.sql.enums.ClusterMode;
  23 import com.dtstack.flink.sql.enums.ECacheType;
  24 import com.dtstack.flink.sql.enums.EPluginLoadMode;
  25 import com.dtstack.flink.sql.environment.MyLocalStreamEnvironment;
  26 import com.dtstack.flink.sql.environment.StreamEnvConfigManager;
  27 import com.dtstack.flink.sql.function.FunctionManager;
  28 import com.dtstack.flink.sql.option.OptionParser;
  29 import com.dtstack.flink.sql.option.Options;
  30 import com.dtstack.flink.sql.parser.CreateFuncParser;
  31 import com.dtstack.flink.sql.parser.CreateTmpTableParser;
  32 import com.dtstack.flink.sql.parser.FlinkPlanner;
  33 import com.dtstack.flink.sql.parser.InsertSqlParser;
  34 import com.dtstack.flink.sql.parser.SqlParser;
  35 import com.dtstack.flink.sql.parser.SqlTree;
  36 import com.dtstack.flink.sql.util.TypeInfoDataTypeConverter;
  37 import com.dtstack.flink.sql.side.AbstractSideTableInfo;
  38 import com.dtstack.flink.sql.side.SideSqlExec;
  39 import com.dtstack.flink.sql.sink.StreamSinkFactory;
  40 import com.dtstack.flink.sql.source.StreamSourceFactory;
  41 import com.dtstack.flink.sql.table.AbstractSourceTableInfo;
  42 import com.dtstack.flink.sql.table.AbstractTableInfo;
  43 import com.dtstack.flink.sql.table.AbstractTargetTableInfo;
  44 import com.dtstack.flink.sql.util.DtStringUtil;
  45 import com.dtstack.flink.sql.util.PluginUtil;
  46 import com.dtstack.flink.sql.watermarker.WaterMarkerAssigner;
  47 import com.fasterxml.jackson.databind.ObjectMapper;
  48 import com.google.common.base.Preconditions;
  49 import com.google.common.base.Strings;
  50 import com.google.common.collect.Lists;
  51 import com.google.common.collect.Maps;
  52 import com.google.common.collect.Sets;
  53 import org.apache.calcite.sql.SqlInsert;
  54 import org.apache.calcite.sql.SqlNode;
  55 import org.apache.commons.io.Charsets;
  56 import org.apache.commons.lang3.StringUtils;
  57 import org.apache.flink.table.types.DataType;
  58 import org.apache.flink.table.types.utils.TypeConversions;
  59 import org.apache.flink.api.common.typeinfo.TypeInformation;
  60 import org.apache.flink.api.java.typeutils.RowTypeInfo;
  61 import org.apache.flink.streaming.api.datastream.DataStream;
  62 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
  63 import org.apache.flink.table.api.EnvironmentSettings;
  64 import org.apache.flink.table.api.Table;
  65 import org.apache.flink.table.api.TableConfig;
  66 import org.apache.flink.table.api.TableEnvironment;
  67 import org.apache.flink.table.api.java.StreamTableEnvironment;
  68 import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;
  69 import org.apache.flink.table.sinks.TableSink;
  70 import org.slf4j.Logger;
  71 import org.slf4j.LoggerFactory;
  72 
  73 import java.io.File;
  74 import java.lang.reflect.InvocationTargetException;
  75 import java.net.URL;
  76 import java.net.URLClassLoader;
  77 import java.net.URLDecoder;
  78 import java.time.ZoneId;
  79 import java.util.ArrayList;
  80 import java.util.stream.Stream;
  81 import java.util.Arrays;
  82 import java.util.List;
  83 import java.util.Map;
  84 import java.util.Properties;
  85 import java.util.Set;
  86 import java.util.TimeZone;
  87 
  88 /**
  89  *  任务执行时的流程方法
  90  * Date: 2020/2/17
  91  * Company: www.dtstack.com
  92  * @author maqi
  93  */
  94 public class ExecuteProcessHelper {
  95 
  96     private static final String CLASS_FILE_NAME_FMT = &quot;class_path_%d&quot;;
  97     private static final Logger LOG = LoggerFactory.getLogger(ExecuteProcessHelper.class);
  98     private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();
  99 
 100     private static final String TIME_ZONE = &quot;timezone&quot;;
 101 
 102     public static FlinkPlanner flinkPlanner = new FlinkPlanner();
 103 
 104     public static ParamsInfo parseParams(String[] args) throws Exception {
 105         LOG.info(&quot;------------program params-------------------------&quot;);
 106         Arrays.stream(args).forEach(arg -&gt; LOG.info(&quot;{}&quot;, arg));
 107         LOG.info(&quot;-------------------------------------------&quot;);
 108 
 109         OptionParser optionParser = new OptionParser(args);
 110         Options options = optionParser.getOptions();
 111 
 112         String sql = URLDecoder.decode(options.getSql(), Charsets.UTF_8.name());
 113         String name = options.getName();
 114         String localSqlPluginPath = options.getLocalSqlPluginPath();
 115         String remoteSqlPluginPath = options.getRemoteSqlPluginPath();
 116         String pluginLoadMode = options.getPluginLoadMode();
 117         String deployMode = options.getMode();
 118 
<abbr title=" 119         Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadMode),"> 119         Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadM🔵</abbr>
 120                 &quot;Non-local mode or shipfile deployment mode, remoteSqlPluginPath is required&quot;);
 121         String confProp = URLDecoder.decode(options.getConfProp(), Charsets.UTF_8.toString());
 122         Properties confProperties = PluginUtil.jsonStrToObject(confProp, Properties.class);
 123 
 124         List&lt;URL&gt; jarUrlList = getExternalJarUrls(options.getAddjar());
 125 
 126         return ParamsInfo.builder()
 127                 .setSql(sql)
 128                 .setName(name)
 129                 .setLocalSqlPluginPath(localSqlPluginPath)
 130                 .setRemoteSqlPluginPath(remoteSqlPluginPath)
 131                 .setPluginLoadMode(pluginLoadMode)
 132                 .setDeployMode(deployMode)
 133                 .setConfProp(confProperties)
 134                 .setJarUrlList(jarUrlList)
 135                 .build();
 136 
 137     }
 138 
 139     /**
 140      *   非local模式或者shipfile部署模式，remoteSqlPluginPath必填
 141      * @param remoteSqlPluginPath
 142      * @param deployMode
 143      * @param pluginLoadMode
 144      * @return
 145      */
<abbr title=" 146     public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoadMode) {"> 146     public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String 🔵</abbr>
 147         if (StringUtils.isEmpty(remoteSqlPluginPath)) {
 148             return StringUtils.equalsIgnoreCase(pluginLoadMode, EPluginLoadMode.SHIPFILE.name())
 149                     || StringUtils.equalsIgnoreCase(deployMode, ClusterMode.local.name());
 150         }
 151         return true;
 152     }
 153 
 154 
 155     public static StreamExecutionEnvironment getStreamExecution(ParamsInfo paramsInfo) throws Exception {
<abbr title=" 156         StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo.getDeployMode());"> 156         StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), p🔵</abbr>
 157         StreamTableEnvironment tableEnv = getStreamTableEnv(env, paramsInfo.getConfProp());
 158 
 159 
 160         SqlParser.setLocalSqlPluginRoot(paramsInfo.getLocalSqlPluginPath());
 161         SqlTree sqlTree = SqlParser.parseSql(paramsInfo.getSql(), paramsInfo.getPluginLoadMode());
 162 
 163         Map&lt;String, AbstractSideTableInfo&gt; sideTableMap = Maps.newHashMap();
 164         Map&lt;String, Table&gt; registerTableCache = Maps.newHashMap();
 165 
 166         //register udf
<abbr title=" 167         ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, paramsInfo.isGetPlan());"> 167         ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, p🔵</abbr>
 168         //register table schema
<abbr title=" 169         Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSqlPluginPath(),"> 169         Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.ge🔵</abbr>
<abbr title=" 170                 paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCache);"> 170                 paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, regist🔵</abbr>
 171         // cache classPathSets
 172         ExecuteProcessHelper.registerPluginUrlToCachedFile(env, classPathSets);
 173 
<abbr title=" 174         ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(),tableEnv, sqlTree, sideTableMap, registerTableCache);"> 174         ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadM🔵</abbr>
 175 
 176         if (env instanceof MyLocalStreamEnvironment) {
 177             ((MyLocalStreamEnvironment) env).setClasspaths(ClassLoaderManager.getClassPath());
 178         }
 179         return env;
 180     }
 181 
 182 
 183     public static List&lt;URL&gt; getExternalJarUrls(String addJarListStr) throws java.io.IOException {
 184         List&lt;URL&gt; jarUrlList = Lists.newArrayList();
 185         if (Strings.isNullOrEmpty(addJarListStr)) {
 186             return jarUrlList;
 187         }
 188 
<abbr title=" 189         List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name()), List.class);"> 189         List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.U🔵</abbr>
 190         //Get External jar to load
 191         for (String addJarPath : addJarFileList) {
 192             jarUrlList.add(new File(addJarPath).toURI().toURL());
 193         }
 194         return jarUrlList;
 195     }
 196 
 197     private static void sqlTranslation(String localSqlPluginPath,
 198                                        String pluginLoadMode,
 199                                        StreamTableEnvironment tableEnv,
 200                                        SqlTree sqlTree,Map&lt;String, AbstractSideTableInfo&gt; sideTableMap,
 201                                        Map&lt;String, Table&gt; registerTableCache) throws Exception {
 202 
 203         SideSqlExec sideSqlExec = new SideSqlExec();
 204         sideSqlExec.setLocalSqlPluginPath(localSqlPluginPath);
 205         sideSqlExec.setPluginLoadMode(pluginLoadMode);
 206 
 207         int scope = 0;
 208         for (CreateTmpTableParser.SqlParserResult result : sqlTree.getTmpSqlList()) {
<abbr title=" 209             sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, scope + &quot;&quot;);"> 209             sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, sco🔵</abbr>
 210             scope++;
 211         }
 212 
 213         for (InsertSqlParser.SqlParseResult result : sqlTree.getExecSqlList()) {
 214             if (LOG.isInfoEnabled()) {
 215                 LOG.info(&quot;exe-sql:\n&quot; + result.getExecSql());
 216             }
 217             boolean isSide = false;
 218             for (String tableName : result.getTargetTableList()) {
 219                 if (sqlTree.getTmpTableMap().containsKey(tableName)) {
 220                     CreateTmpTableParser.SqlParserResult tmp = sqlTree.getTmpTableMap().get(tableName);
 221                     String realSql = DtStringUtil.replaceIgnoreQuota(result.getExecSql(), &quot;`&quot;, &quot;&quot;);
 222 
 223                     SqlNode sqlNode = flinkPlanner.getParser().parse(realSql);
 224                     String tmpSql = ((SqlInsert) sqlNode).getSource().toString();
 225                     tmp.setExecSql(tmpSql);
<abbr title=" 226                     sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;);"> 226                     sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, s🔵</abbr>
 227                 } else {
 228                     for (String sourceTable : result.getSourceTableList()) {
 229                         if (sideTableMap.containsKey(sourceTable)) {
 230                             isSide = true;
 231                             break;
 232                         }
 233                     }
 234                     if (isSide) {
 235                         //sql-dimensional table contains the dimension table of execution
<abbr title=" 236                         sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, String.valueOf(scope));"> 236                         sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache,🔵</abbr>
 237                     } else {
 238                         LOG.info(&quot;----------exec sql without dimension join-----------&quot;);
<abbr title=" 239                         LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getExecSql());"> 239                         LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getEx🔵</abbr>
 240                         FlinkSQLExec.sqlUpdate(tableEnv, result.getExecSql());
 241                         if (LOG.isInfoEnabled()) {
 242                             LOG.info(&quot;exec sql: &quot; + result.getExecSql());
 243                         }
 244                     }
 245                 }
 246 
 247                 scope++;
 248             }
 249         }
 250     }
 251 
<abbr title=" 252     public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEnv, boolean getPlan)"> 252     public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironmen🔵</abbr>
 253             throws IllegalAccessException, InvocationTargetException {
 254         // udf和tableEnv须由同一个类加载器加载
 255         ClassLoader levelClassLoader = tableEnv.getClass().getClassLoader();
 256         URLClassLoader classLoader = null;
 257         List&lt;CreateFuncParser.SqlParserResult&gt; funcList = sqlTree.getFunctionList();
 258         for (CreateFuncParser.SqlParserResult funcInfo : funcList) {
 259             // 构建plan的情况下，udf和tableEnv不需要是同一个类加载器
 260             if (getPlan) {
 261                 URL[] urls = jarUrlList.toArray(new URL[0]);
 262                 classLoader = URLClassLoader.newInstance(urls);
 263             }
 264 
 265             //classloader
 266             if (classLoader == null) {
<abbr title=" 267                 classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, (URLClassLoader) levelClassLoader);"> 267                 classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, (URLClassLoader) levelClassLoad🔵</abbr>
 268             }
<abbr title=" 269             FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv, classLoader);"> 269             FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), 🔵</abbr>
 270         }
 271     }
 272 
 273     /**
 274      *    向Flink注册源表和结果表，返回执行时插件包的全路径
 275      * @param sqlTree
 276      * @param env
 277      * @param tableEnv
 278      * @param localSqlPluginPath
 279      * @param remoteSqlPluginPath
 280      * @param pluginLoadMode   插件加载模式 classpath or shipfile
 281      * @param sideTableMap
 282      * @param registerTableCache
 283      * @return
 284      * @throws Exception
 285      */
<abbr title=" 286     public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment tableEnv, String localSqlPluginPath,"> 286     public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvi🔵</abbr>
<abbr title=" 287                                          String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSideTableInfo&gt; sideTableMap, Map&lt;String, Table&gt; registerTableCache) throws Exception {"> 287                                          String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, A🔵</abbr>
 288         Set&lt;URL&gt; pluginClassPathSets = Sets.newHashSet();
 289         WaterMarkerAssigner waterMarkerAssigner = new WaterMarkerAssigner();
 290         for (AbstractTableInfo tableInfo : sqlTree.getTableInfoMap().values()) {
 291 
 292             if (tableInfo instanceof AbstractSourceTableInfo) {
 293 
 294                 AbstractSourceTableInfo sourceTableInfo = (AbstractSourceTableInfo) tableInfo;
<abbr title=" 295                 Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPath, pluginLoadMode);"> 295                 Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSq🔵</abbr>
 296                 tableEnv.registerTable(sourceTableInfo.getAdaptName(), table);
<abbr title=" 297                 //Note --- parameter conversion function can not be used inside a function of the type of polymerization"> 297                 //Note --- parameter conversion function can not be used inside a function of the type of🔵</abbr>
 298                 //Create table in which the function is arranged only need adaptation sql
 299                 String adaptSql = sourceTableInfo.getAdaptSelectSql();
 300                 Table adaptTable = adaptSql == null ? table : tableEnv.sqlQuery(adaptSql);
 301 
<abbr title=" 302                 RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().getFieldDataTypes()), adaptTable.getSchema().getFieldNames());"> 302                 RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().ge🔵</abbr>
 303                 DataStream adaptStream = tableEnv.toAppendStream(adaptTable, typeInfo);
 304 
 305                 String fields = String.join(&quot;,&quot;, typeInfo.getFieldNames());
 306 
 307                 if (waterMarkerAssigner.checkNeedAssignWaterMarker(sourceTableInfo)) {
<abbr title=" 308                     adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTableInfo);"> 308                     adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTabl🔵</abbr>
 309                     fields += &quot;,ROWTIME.ROWTIME&quot;;
 310                 } else {
 311                     fields += &quot;,PROCTIME.PROCTIME&quot;;
 312                 }
 313 
 314                 Table regTable = tableEnv.fromDataStream(adaptStream, fields);
 315                 tableEnv.registerTable(tableInfo.getName(), regTable);
 316                 if (LOG.isInfoEnabled()) {
 317                     LOG.info(&quot;registe table {} success.&quot;, tableInfo.getName());
 318                 }
 319                 registerTableCache.put(tableInfo.getName(), regTable);
 320 
<abbr title=" 321                 URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractSourceTableInfo.SOURCE_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 321                 URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(),🔵</abbr>
 322                 pluginClassPathSets.add(sourceTablePathUrl);
 323             } else if (tableInfo instanceof AbstractTargetTableInfo) {
 324 
<abbr title=" 325                 TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo, localSqlPluginPath, pluginLoadMode);"> 325                 TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo,🔵</abbr>
<abbr title=" 326                 TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses());"> 326                 TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses()🔵</abbr>
<abbr title=" 327                 tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableSink);"> 327                 tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableS🔵</abbr>
 328 
<abbr title=" 329                 URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTargetTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 329                 URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), A🔵</abbr>
 330                 pluginClassPathSets.add(sinkTablePathUrl);
 331             } else if (tableInfo instanceof AbstractSideTableInfo) {
<abbr title=" 332                 String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tableInfo).getCacheType()) ? &quot;all&quot; : &quot;async&quot;;"> 332                 String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tab🔵</abbr>
 333                 sideTableMap.put(tableInfo.getName(), (AbstractSideTableInfo) tableInfo);
 334 
<abbr title=" 335                 URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, AbstractSideTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 335                 URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperat🔵</abbr>
 336                 pluginClassPathSets.add(sideTablePathUrl);
 337             } else {
 338                 throw new RuntimeException(&quot;not support table type:&quot; + tableInfo.getType());
 339             }
 340         }
 341         if (localSqlPluginPath == null || localSqlPluginPath.isEmpty()) {
 342             return Sets.newHashSet();
 343         }
 344         return pluginClassPathSets;
 345     }
 346 
 347     /**
 348      *   perjob模式将job依赖的插件包路径存储到cacheFile，在外围将插件包路径传递给jobgraph
 349      * @param env
 350      * @param classPathSet
 351      */
<abbr title=" 352     public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSet) {"> 352     public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSe🔵</abbr>
 353         int i = 0;
 354         for (URL url : classPathSet) {
 355             String classFileName = String.format(CLASS_FILE_NAME_FMT, i);
 356             env.registerCachedFile(url.getPath(), classFileName, true);
 357             i++;
 358         }
 359     }
 360 
<abbr title=" 361     public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws Exception {"> 361     public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode🔵</abbr>
 362         StreamExecutionEnvironment env = !ClusterMode.local.name().equals(deployMode) ?
 363                 StreamExecutionEnvironment.getExecutionEnvironment() :
 364                 new MyLocalStreamEnvironment();
 365 
 366         StreamEnvConfigManager.streamExecutionEnvironmentConfig(env, confProperties);
 367         return env;
 368     }
 369 
 370 
<abbr title=" 371     public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperties) {"> 371     public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties con🔵</abbr>
 372         // use blink and streammode
 373         EnvironmentSettings settings = EnvironmentSettings.newInstance()
 374                 .useBlinkPlanner()
 375                 .inStreamingMode()
 376                 .build();
 377 
 378         TableConfig tableConfig = new TableConfig();
 379 
 380         timeZoneCheck(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()));
 381 
<abbr title=" 382         tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID())));"> 382         tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault(🔵</abbr>
 383 
 384         StreamTableEnvironment tableEnv = StreamTableEnvironmentImpl.create(env, settings, tableConfig);
 385         StreamEnvConfigManager.streamTableEnvironmentStateTTLConfig(tableEnv, confProperties);
 386         return tableEnv;
 387     }
 388 
 389     private static void timeZoneCheck(String timeZone) {
 390         ArrayList&lt;String&gt; zones = Lists.newArrayList(TimeZone.getAvailableIDs());
 391         if (!zones.contains(timeZone)){
 392             throw new IllegalArgumentException(String.format(&quot; timezone of %s is Incorrect!&quot;, timeZone));
 393         }
 394     }
 395 
 396     private static TypeInformation&lt;?&gt;[] fromDataTypeToLegacyInfo(DataType[] dataType) {
 397         return Stream.of(dataType)
 398                 .map(TypeInfoDataTypeConverter::toLegacyTypeInfo)
 399                 .toArray(TypeInformation[]::new);
 400     }
 401 }
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </pre></td>
                            <td><pre>   1 /*
   2  * Licensed to the Apache Software Foundation (ASF) under one
   3  * or more contributor license agreements.  See the NOTICE file
   4  * distributed with this work for additional information
   5  * regarding copyright ownership.  The ASF licenses this file
   6  * to you under the Apache License, Version 2.0 (the
   7  * &quot;License&quot;); you may not use this file except in compliance
   8  * with the License.  You may obtain a copy of the License at
   9  *
  10  *     http://www.apache.org/licenses/LICENSE-2.0
  11  *
  12  * Unless required by applicable law or agreed to in writing, software
  13  * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15  * See the License for the specific language governing permissions and
  16  * limitations under the License.
  17  */
  18 package com.dtstack.flink.sql.exec;
  19 
  20 import com.dtstack.flink.sql.classloader.ClassLoaderManager;
  21 import com.dtstack.flink.sql.enums.ClusterMode;
  22 import com.dtstack.flink.sql.enums.ECacheType;
  23 import com.dtstack.flink.sql.enums.EPluginLoadMode;
  24 import com.dtstack.flink.sql.environment.MyLocalStreamEnvironment;
  25 import com.dtstack.flink.sql.environment.StreamEnvConfigManager;
  26 import com.dtstack.flink.sql.function.FunctionManager;
  27 import com.dtstack.flink.sql.option.OptionParser;
  28 import com.dtstack.flink.sql.option.Options;
  29 import com.dtstack.flink.sql.parser.CreateFuncParser;
  30 import com.dtstack.flink.sql.parser.CreateTmpTableParser;
  31 import com.dtstack.flink.sql.parser.FlinkPlanner;
  32 import com.dtstack.flink.sql.parser.InsertSqlParser;
  33 import com.dtstack.flink.sql.parser.SqlParser;
  34 import com.dtstack.flink.sql.parser.SqlTree;
  35 import com.dtstack.flink.sql.side.AbstractSideTableInfo;
  36 import com.dtstack.flink.sql.side.SideSqlExec;
  37 import com.dtstack.flink.sql.sink.StreamSinkFactory;
  38 import com.dtstack.flink.sql.source.StreamSourceFactory;
  39 import com.dtstack.flink.sql.table.AbstractSourceTableInfo;
  40 import com.dtstack.flink.sql.table.AbstractTableInfo;
  41 import com.dtstack.flink.sql.table.AbstractTargetTableInfo;
  42 import com.dtstack.flink.sql.util.DtStringUtil;
  43 import com.dtstack.flink.sql.util.PluginUtil;
  44 import com.dtstack.flink.sql.util.TypeInfoDataTypeConverter;
  45 import com.dtstack.flink.sql.watermarker.WaterMarkerAssigner;
  46 import com.fasterxml.jackson.databind.ObjectMapper;
  47 import com.google.common.base.Preconditions;
  48 import com.google.common.base.Strings;
  49 import com.google.common.collect.Lists;
  50 import com.google.common.collect.Maps;
  51 import com.google.common.collect.Sets;
  52 import java.io.File;
  53 import java.lang.reflect.InvocationTargetException;
  54 import java.net.URL;
  55 import java.net.URLClassLoader;
  56 import java.net.URLDecoder;
  57 import java.time.ZoneId;
  58 import java.util.ArrayList;
  59 import java.util.Arrays;
  60 import java.util.List;
  61 import java.util.Map;
  62 import java.util.Properties;
  63 import java.util.Set;
  64 import java.util.TimeZone;
  65 import java.util.stream.Stream;
  66 import org.apache.calcite.sql.SqlInsert;
  67 import org.apache.calcite.sql.SqlNode;
  68 import org.apache.commons.io.Charsets;
  69 import org.apache.commons.lang3.StringUtils;
  70 import org.apache.flink.api.common.typeinfo.TypeInformation;
  71 import org.apache.flink.api.java.typeutils.RowTypeInfo;
  72 import org.apache.flink.streaming.api.datastream.DataStream;
  73 import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
  74 import org.apache.flink.table.api.EnvironmentSettings;
  75 import org.apache.flink.table.api.Table;
  76 import org.apache.flink.table.api.TableConfig;
  77 import org.apache.flink.table.api.TableEnvironment;
  78 import org.apache.flink.table.api.java.StreamTableEnvironment;
  79 import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;
  80 import org.apache.flink.table.sinks.TableSink;
  81 import org.apache.flink.table.types.DataType;
  82 import org.apache.flink.table.types.utils.TypeConversions;
  83 import org.slf4j.Logger;
  84 import org.slf4j.LoggerFactory;
  85 
  86 
  87 /**
  88  *  任务执行时的流程方法
  89  * Date: 2020/2/17
  90  * Company: www.dtstack.com
  91  * @author maqi
  92  */
  93 public class ExecuteProcessHelper {
  94     private static final String CLASS_FILE_NAME_FMT = &quot;class_path_%d&quot;;
  95 
  96     private static final Logger LOG = LoggerFactory.getLogger(ExecuteProcessHelper.class);
  97 
  98     private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();
  99 
 100     private static final String TIME_ZONE = &quot;timezone&quot;;
 101 
 102     public static FlinkPlanner flinkPlanner = new FlinkPlanner();
 103 
 104     public static ParamsInfo parseParams(String[] args) throws Exception {
 105         LOG.info(&quot;------------program params-------------------------&quot;);
 106         Arrays.stream(args).forEach(( arg) -&gt; LOG.info(&quot;{}&quot;, arg));
 107         LOG.info(&quot;-------------------------------------------&quot;);
 108         OptionParser optionParser = new OptionParser(args);
 109         Options options = optionParser.getOptions();
 110         String sql = URLDecoder.decode(options.getSql(), Charsets.UTF_8.name());
 111         String name = options.getName();
 112         String localSqlPluginPath = options.getLocalSqlPluginPath();
 113         String remoteSqlPluginPath = options.getRemoteSqlPluginPath();
 114         String pluginLoadMode = options.getPluginLoadMode();
 115         String deployMode = options.getMode();
<abbr title=" 116         Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadMode), &quot;Non-local mode or shipfile deployment mode, remoteSqlPluginPath is required&quot;);"> 116         Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadM🔵</abbr>
 117         String confProp = URLDecoder.decode(options.getConfProp(), Charsets.UTF_8.toString());
 118         Properties confProperties = PluginUtil.jsonStrToObject(confProp, Properties.class);
 119         List&lt;URL&gt; jarUrlList = getExternalJarUrls(options.getAddjar());
<abbr title=" 120         return ParamsInfo.builder().setSql(sql).setName(name).setLocalSqlPluginPath(localSqlPluginPath).setRemoteSqlPluginPath(remoteSqlPluginPath).setPluginLoadMode(pluginLoadMode).setDeployMode(deployMode).setConfProp(confProperties).setJarUrlList(jarUrlList).build();"> 120         return ParamsInfo.builder().setSql(sql).setName(name).setLocalSqlPluginPath(localSqlPluginPath).s🔵</abbr>
 121     }
 122 
 123     /**
 124      *   非local模式或者shipfile部署模式，remoteSqlPluginPath必填
 125      * @param remoteSqlPluginPath
 126      * @param deployMode
 127      * @param pluginLoadMode
 128      * @return
 129      */
<abbr title=" 130     public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoadMode) {"> 130     public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String 🔵</abbr>
 131         if (StringUtils.isEmpty(remoteSqlPluginPath)) {
 132             return StringUtils.equalsIgnoreCase(pluginLoadMode, EPluginLoadMode.SHIPFILE.name())
 133                     || StringUtils.equalsIgnoreCase(deployMode, ClusterMode.local.name());
 134         }
 135         return true;
 136     }
 137 
 138     public static StreamExecutionEnvironment getStreamExecution(ParamsInfo paramsInfo) throws Exception {
<abbr title=" 139         StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo.getDeployMode());"> 139         StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), p🔵</abbr>
 140         StreamTableEnvironment tableEnv = getStreamTableEnv(env, paramsInfo.getConfProp());
 141         SqlParser.setLocalSqlPluginRoot(paramsInfo.getLocalSqlPluginPath());
 142         SqlTree sqlTree = SqlParser.parseSql(paramsInfo.getSql(), paramsInfo.getPluginLoadMode());
 143         Map&lt;String, AbstractSideTableInfo&gt; sideTableMap = Maps.newHashMap();
 144         Map&lt;String, Table&gt; registerTableCache = Maps.newHashMap();
 145         //register udf
<abbr title=" 146         ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, paramsInfo.isGetPlan());"> 146         ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, p🔵</abbr>
 147         //register table schema
<abbr title=" 148         Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSqlPluginPath(), paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCache);"> 148         Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.ge🔵</abbr>
 149         // cache classPathSets
 150         ExecuteProcessHelper.registerPluginUrlToCachedFile(env, classPathSets);
<abbr title=" 151         ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(), tableEnv, sqlTree, sideTableMap, registerTableCache);"> 151         ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadM🔵</abbr>
 152         if (env instanceof MyLocalStreamEnvironment) {
 153             ((MyLocalStreamEnvironment) (env)).setClasspaths(ClassLoaderManager.getClassPath());
 154         }
 155         return env;
 156     }
 157 
 158     public static List&lt;URL&gt; getExternalJarUrls(String addJarListStr) throws java.io.IOException {
 159         List&lt;URL&gt; jarUrlList = Lists.newArrayList();
 160         if (Strings.isNullOrEmpty(addJarListStr)) {
 161             return jarUrlList;
 162         }
 163 
<abbr title=" 164         List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name()), List.class);"> 164         List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.U🔵</abbr>
 165         //Get External jar to load
 166         for (String addJarPath : addJarFileList) {
 167             jarUrlList.add(new File(addJarPath).toURI().toURL());
 168         }
 169         return jarUrlList;
 170     }
 171 
<abbr title=" 172     private static void sqlTranslation(String localSqlPluginPath, String pluginLoadMode, StreamTableEnvironment tableEnv, SqlTree sqlTree, Map&lt;String, AbstractSideTableInfo&gt; sideTableMap, Map&lt;String, Table&gt; registerTableCache) throws Exception {"> 172     private static void sqlTranslation(String localSqlPluginPath, String pluginLoadMode, StreamTableEnvir🔵</abbr>
 173         SideSqlExec sideSqlExec = new SideSqlExec();
 174         sideSqlExec.setLocalSqlPluginPath(localSqlPluginPath);
 175         sideSqlExec.setPluginLoadMode(pluginLoadMode);
 176         int scope = 0;
 177         for (CreateTmpTableParser.SqlParserResult result : sqlTree.getTmpSqlList()) {
<abbr title=" 178             sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, scope + &quot;&quot;);"> 178             sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, sco🔵</abbr>
 179             scope++;
 180         }
 181         for (InsertSqlParser.SqlParseResult result : sqlTree.getExecSqlList()) {
 182             if (LOG.isInfoEnabled()) {
 183                 LOG.info(&quot;exe-sql:\n&quot; + result.getExecSql());
 184             }
 185             boolean isSide = false;
 186             for (String tableName : result.getTargetTableList()) {
 187                 if (sqlTree.getTmpTableMap().containsKey(tableName)) {
 188                     CreateTmpTableParser.SqlParserResult tmp = sqlTree.getTmpTableMap().get(tableName);
 189                     String realSql = DtStringUtil.replaceIgnoreQuota(result.getExecSql(), &quot;`&quot;, &quot;&quot;);
 190                     SqlNode sqlNode = flinkPlanner.getParser().parse(realSql);
 191                     String tmpSql = ((SqlInsert) (sqlNode)).getSource().toString();
 192                     tmp.setExecSql(tmpSql);
<abbr title=" 193                     sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;);"> 193                     sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, s🔵</abbr>
 194                 } else {
 195                     for (String sourceTable : result.getSourceTableList()) {
 196                         if (sideTableMap.containsKey(sourceTable)) {
 197                             isSide = true;
 198                             break;
 199                         }
 200                     }
 201                     if (isSide) {
 202                         //sql-dimensional table contains the dimension table of execution
<abbr title=" 203                         sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, String.valueOf(scope));"> 203                         sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache,🔵</abbr>
 204                     } else {
 205                         LOG.info(&quot;----------exec sql without dimension join-----------&quot;);
<abbr title=" 206                         LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getExecSql());"> 206                         LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getEx🔵</abbr>
 207                         FlinkSQLExec.sqlUpdate(tableEnv, result.getExecSql());
 208                         if (LOG.isInfoEnabled()) {
 209                             LOG.info(&quot;exec sql: &quot; + result.getExecSql());
 210                         }
 211                     }
 212                 }
 213                 scope++;
 214             }
 215         }
 216     }
 217 
<abbr title=" 218     public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEnv, boolean getPlan) throws IllegalAccessException, InvocationTargetException {"> 218     public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironmen🔵</abbr>
 219         // udf和tableEnv须由同一个类加载器加载
 220         ClassLoader levelClassLoader = tableEnv.getClass().getClassLoader();
 221         URLClassLoader classLoader = null;
 222         List&lt;CreateFuncParser.SqlParserResult&gt; funcList = sqlTree.getFunctionList();
 223         for (CreateFuncParser.SqlParserResult funcInfo : funcList) {
 224             // 构建plan的情况下，udf和tableEnv不需要是同一个类加载器
 225             if (getPlan) {
 226                 URL[] urls = jarUrlList.toArray(new URL[0]);
 227                 classLoader = URLClassLoader.newInstance(urls);
 228             }
 229             // classloader
 230             if (classLoader == null) {
<abbr title=" 231                 classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, ((URLClassLoader) (levelClassLoader)));"> 231                 classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, ((URLClassLoader) (levelClassLo🔵</abbr>
 232             }
<abbr title=" 233             FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv, classLoader);"> 233             FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), 🔵</abbr>
 234         }
 235     }
 236 
 237     /**
 238      *    向Flink注册源表和结果表，返回执行时插件包的全路径
 239      * @param sqlTree
 240      * @param env
 241      * @param tableEnv
 242      * @param localSqlPluginPath
 243      * @param remoteSqlPluginPath
 244      * @param pluginLoadMode   插件加载模式 classpath or shipfile
 245      * @param sideTableMap
 246      * @param registerTableCache
 247      * @return
 248      * @throws Exception
 249      */
<abbr title=" 250     public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment tableEnv, String localSqlPluginPath, String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSideTableInfo&gt; sideTableMap, Map&lt;String, Table&gt; registerTableCache) throws Exception {"> 250     public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvi🔵</abbr>
 251         Set&lt;URL&gt; pluginClassPathSets = Sets.newHashSet();
 252         WaterMarkerAssigner waterMarkerAssigner = new WaterMarkerAssigner();
 253         for (AbstractTableInfo tableInfo : sqlTree.getTableInfoMap().values()) {
 254             if (tableInfo instanceof AbstractSourceTableInfo) {
 255                 AbstractSourceTableInfo sourceTableInfo = ((AbstractSourceTableInfo) (tableInfo));
<abbr title=" 256                 Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPath, pluginLoadMode);"> 256                 Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSq🔵</abbr>
 257                 tableEnv.registerTable(sourceTableInfo.getAdaptName(), table);
<abbr title=" 258                 // Note --- parameter conversion function can not be used inside a function of the type of polymerization"> 258                 // Note --- parameter conversion function can not be used inside a function of the type o🔵</abbr>
 259                 // Create table in which the function is arranged only need adaptation sql
 260                 String adaptSql = sourceTableInfo.getAdaptSelectSql();
 261                 Table adaptTable = (adaptSql == null) ? table : tableEnv.sqlQuery(adaptSql);
<abbr title=" 262                 RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().getFieldDataTypes()), adaptTable.getSchema().getFieldNames());"> 262                 RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().ge🔵</abbr>
 263                 DataStream adaptStream = tableEnv.toAppendStream(adaptTable, typeInfo);
 264                 String fields = String.join(&quot;,&quot;, typeInfo.getFieldNames());
 265                 if (waterMarkerAssigner.checkNeedAssignWaterMarker(sourceTableInfo)) {
<abbr title=" 266                     adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTableInfo);"> 266                     adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTabl🔵</abbr>
 267                     fields += &quot;,ROWTIME.ROWTIME&quot;;
 268                 } else {
 269                     fields += &quot;,PROCTIME.PROCTIME&quot;;
 270                 }
 271                 Table regTable = tableEnv.fromDataStream(adaptStream, fields);
 272                 tableEnv.registerTable(tableInfo.getName(), regTable);
 273                 if (LOG.isInfoEnabled()) {
 274                     LOG.info(&quot;registe table {} success.&quot;, tableInfo.getName());
 275                 }
 276                 registerTableCache.put(tableInfo.getName(), regTable);
<abbr title=" 277                 URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractSourceTableInfo.SOURCE_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 277                 URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(),🔵</abbr>
 278                 pluginClassPathSets.add(sourceTablePathUrl);
 279             } else if (tableInfo instanceof AbstractTargetTableInfo) {
<abbr title=" 280                 TableSink tableSink = StreamSinkFactory.getTableSink(((AbstractTargetTableInfo) (tableInfo)), localSqlPluginPath, pluginLoadMode);"> 280                 TableSink tableSink = StreamSinkFactory.getTableSink(((AbstractTargetTableInfo) (tableInf🔵</abbr>
<abbr title=" 281                 TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses());"> 281                 TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses()🔵</abbr>
<abbr title=" 282                 tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableSink);"> 282                 tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableS🔵</abbr>
<abbr title=" 283                 URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTargetTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 283                 URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), A🔵</abbr>
 284                 pluginClassPathSets.add(sinkTablePathUrl);
 285             } else if (tableInfo instanceof AbstractSideTableInfo) {
<abbr title=" 286                 String sideOperator = (ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) (tableInfo)).getCacheType())) ? &quot;all&quot; : &quot;async&quot;;"> 286                 String sideOperator = (ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) (t🔵</abbr>
 287                 sideTableMap.put(tableInfo.getName(), ((AbstractSideTableInfo) (tableInfo)));
<abbr title=" 288                 URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, AbstractSideTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 288                 URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperat🔵</abbr>
 289                 pluginClassPathSets.add(sideTablePathUrl);
 290             } else {
 291                 throw new RuntimeException(&quot;not support table type:&quot; + tableInfo.getType());
 292             }
 293         }
 294         if ((localSqlPluginPath == null) || localSqlPluginPath.isEmpty()) {
 295             return Sets.newHashSet();
 296         }
 297         return pluginClassPathSets;
 298     }
 299 
 300     /**
 301      *   perjob模式将job依赖的插件包路径存储到cacheFile，在外围将插件包路径传递给jobgraph
 302      * @param env
 303      * @param classPathSet
 304      */
<abbr title=" 305     public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSet) {"> 305     public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSe🔵</abbr>
 306         int i = 0;
 307         for (URL url : classPathSet) {
 308             String classFileName = String.format(CLASS_FILE_NAME_FMT, i);
 309             env.registerCachedFile(url.getPath(), classFileName, true);
 310             i++;
 311         }
 312     }
 313 
<abbr title=" 314     public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws Exception {"> 314     public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode🔵</abbr>
 315         StreamExecutionEnvironment env = !ClusterMode.local.name().equals(deployMode) ?
 316                 StreamExecutionEnvironment.getExecutionEnvironment() :
 317                 new MyLocalStreamEnvironment();
 318 
 319         StreamEnvConfigManager.streamExecutionEnvironmentConfig(env, confProperties);
 320         return env;
 321     }
 322 
<abbr title=" 323     public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperties) {"> 323     public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties con🔵</abbr>
 324         // use blink and streammode
<abbr title=" 325         EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMode().build();"> 325         EnvironmentSettings settings = EnvironmentSettings.newInstance().useBlinkPlanner().inStreamingMod🔵</abbr>
 326         TableConfig tableConfig = new TableConfig();
 327         timeZoneCheck(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()));
<abbr title=" 328         tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID())));"> 328         tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault(🔵</abbr>
 329         StreamTableEnvironment tableEnv = StreamTableEnvironmentImpl.create(env, settings, tableConfig);
 330         StreamEnvConfigManager.streamTableEnvironmentStateTTLConfig(tableEnv, confProperties);
 331         return tableEnv;
 332     }
 333 
 334     private static void timeZoneCheck(String timeZone) {
 335         ArrayList&lt;String&gt; zones = Lists.newArrayList(TimeZone.getAvailableIDs());
 336         if (!zones.contains(timeZone)){
 337             throw new IllegalArgumentException(String.format(&quot; timezone of %s is Incorrect!&quot;, timeZone));
 338         }
 339     }
 340 
 341     private static TypeInformation&lt;?&gt;[] fromDataTypeToLegacyInfo(DataType[] dataType) {
 342         return Stream.of(dataType)
 343                 .map(TypeInfoDataTypeConverter::toLegacyTypeInfo)
 344                 .toArray(TypeInformation[]::new);
 345     }
 346 }
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 </pre></td>
                        </tr>
                    </table>
                </div>
                <div id="bottom">
                    <table style="margin:auto">
                        <tr>
                            <th>ours vs. base</th>
                            <th>theirs vs. base</th>
                        </tr>
                        <tr>
                            <td><pre>   1  /*
   2   * Licensed to the Apache Software Foundation (ASF) under one
   3   * or more contributor license agreements.  See the NOTICE file
   4   * distributed with this work for additional information
   5   * regarding copyright ownership.  The ASF licenses this file
   6   * to you under the Apache License, Version 2.0 (the
   7   * &quot;License&quot;); you may not use this file except in compliance
   8   * with the License.  You may obtain a copy of the License at
   9   *
  10   *     http://www.apache.org/licenses/LICENSE-2.0
  11   *
  12   * Unless required by applicable law or agreed to in writing, software
  13   * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15   * See the License for the specific language governing permissions and
  16   * limitations under the License.
  17   */
  18  
  19  package com.dtstack.flink.sql.exec;
  20  
  21  import com.dtstack.flink.sql.parser.CreateFuncParser;
  22  import com.dtstack.flink.sql.parser.CreateTmpTableParser;
  23  import com.dtstack.flink.sql.parser.FlinkPlanner;
  24  import com.dtstack.flink.sql.parser.InsertSqlParser;
  25  import com.dtstack.flink.sql.parser.SqlParser;
  26  import com.dtstack.flink.sql.parser.SqlTree;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  27 +import com.dtstack.flink.sql.util.TypeInfoDataTypeConverter;</span>
  28  import org.apache.flink.api.common.typeinfo.TypeInformation;
  29  import org.apache.flink.api.java.typeutils.RowTypeInfo;
  30  import org.apache.flink.streaming.api.datastream.DataStream;
  31  import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
  32  import org.apache.flink.table.api.*;
  33  import org.apache.flink.table.api.java.StreamTableEnvironment;
  34  import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;
  35  import org.apache.flink.table.sinks.TableSink;
  36  
  37  import com.dtstack.flink.sql.classloader.ClassLoaderManager;
  38  import com.dtstack.flink.sql.enums.ClusterMode;
  39  import com.dtstack.flink.sql.enums.ECacheType;
  40  import com.dtstack.flink.sql.enums.EPluginLoadMode;
  41  import com.dtstack.flink.sql.environment.MyLocalStreamEnvironment;
  42  import com.dtstack.flink.sql.environment.StreamEnvConfigManager;
  43  import com.dtstack.flink.sql.function.FunctionManager;
  44  import com.dtstack.flink.sql.option.OptionParser;
  45  import com.dtstack.flink.sql.option.Options;







  46  import com.dtstack.flink.sql.side.SideSqlExec;
  47  import com.dtstack.flink.sql.side.AbstractSideTableInfo;
  48  import com.dtstack.flink.sql.sink.StreamSinkFactory;
  49  import com.dtstack.flink.sql.source.StreamSourceFactory;
  50  import com.dtstack.flink.sql.table.AbstractSourceTableInfo;
  51  import com.dtstack.flink.sql.table.AbstractTableInfo;
  52  import com.dtstack.flink.sql.table.AbstractTargetTableInfo;
  53  import com.dtstack.flink.sql.util.DtStringUtil;
  54  import com.dtstack.flink.sql.util.PluginUtil;
  55  import com.dtstack.flink.sql.watermarker.WaterMarkerAssigner;
  56  import com.fasterxml.jackson.databind.ObjectMapper;
  57  import com.google.common.base.Preconditions;
  58  import com.google.common.base.Strings;
  59  import com.google.common.collect.Lists;
  60  import com.google.common.collect.Maps;
  61  import com.google.common.collect.Sets;
  62  import org.apache.calcite.sql.SqlInsert;
  63  import org.apache.calcite.sql.SqlNode;
  64  import org.apache.commons.io.Charsets;
  65  import org.apache.commons.lang3.StringUtils;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  66 +import org.apache.flink.table.types.DataType;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  67 +import org.apache.flink.table.types.utils.TypeConversions;</span>









  68  import org.slf4j.Logger;
  69  import org.slf4j.LoggerFactory;
  70  
  71  import java.io.File;
  72  import java.lang.reflect.InvocationTargetException;
  73  import java.net.URL;
  74  import java.net.URLClassLoader;
  75  import java.net.URLDecoder;
  76  import java.time.ZoneId;

  77  import java.util.Arrays;
  78  import java.util.List;
  79  import java.util.Map;
  80  import java.util.Properties;
  81  import java.util.Set;
  82  import java.util.TimeZone;
  83  import java.util.ArrayList;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  84 +import java.util.stream.Stream;</span>
  85  
  86  /**
  87   *  任务执行时的流程方法
  88   * Date: 2020/2/17
  89   * Company: www.dtstack.com
  90   * @author maqi
  91   */
  92  public class ExecuteProcessHelper {
  93  
  94      private static final String CLASS_FILE_NAME_FMT = &quot;class_path_%d&quot;;
  95      private static final Logger LOG = LoggerFactory.getLogger(ExecuteProcessHelper.class);
  96      private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();
  97  
  98      private static final String TIME_ZONE = &quot;timezone&quot;;
  99  
 100      public static FlinkPlanner flinkPlanner = new FlinkPlanner();
 101  
 102      public static ParamsInfo parseParams(String[] args) throws Exception {
 103          LOG.info(&quot;------------program params-------------------------&quot;);
 104          Arrays.stream(args).forEach(arg -&gt; LOG.info(&quot;{}&quot;, arg));
 105          LOG.info(&quot;-------------------------------------------&quot;);
 106  
 107          OptionParser optionParser = new OptionParser(args);
 108          Options options = optionParser.getOptions();
 109  
 110          String sql = URLDecoder.decode(options.getSql(), Charsets.UTF_8.name());
 111          String name = options.getName();
 112          String localSqlPluginPath = options.getLocalSqlPluginPath();
 113          String remoteSqlPluginPath = options.getRemoteSqlPluginPath();
 114          String pluginLoadMode = options.getPluginLoadMode();
 115          String deployMode = options.getMode();
 116  
 117          Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadMode),
 118                  &quot;Non-local mode or shipfile deployment mode, remoteSqlPluginPath is required&quot;);
 119          String confProp = URLDecoder.decode(options.getConfProp(), Charsets.UTF_8.toString());
 120          Properties confProperties = PluginUtil.jsonStrToObject(confProp, Properties.class);
 121  
 122          List&lt;URL&gt; jarUrlList = getExternalJarUrls(options.getAddjar());
 123  
 124          return ParamsInfo.builder()
 125                  .setSql(sql)
 126                  .setName(name)
 127                  .setLocalSqlPluginPath(localSqlPluginPath)
 128                  .setRemoteSqlPluginPath(remoteSqlPluginPath)
 129                  .setPluginLoadMode(pluginLoadMode)
 130                  .setDeployMode(deployMode)
 131                  .setConfProp(confProperties)
 132                  .setJarUrlList(jarUrlList)
 133                  .build();
 134  
 135      }
 136  
 137      /**
 138       *   非local模式或者shipfile部署模式，remoteSqlPluginPath必填
 139       * @param remoteSqlPluginPath
 140       * @param deployMode
 141       * @param pluginLoadMode
 142       * @return
 143       */
<abbr title=" 144      public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoadMode) {"> 144      public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoa🔵</abbr>
 145          if (StringUtils.isEmpty(remoteSqlPluginPath)) {
 146              return StringUtils.equalsIgnoreCase(pluginLoadMode, EPluginLoadMode.SHIPFILE.name())
 147                      || StringUtils.equalsIgnoreCase(deployMode, ClusterMode.local.name());
 148          }
 149          return true;
 150      }
 151  
 152  
 153      public static StreamExecutionEnvironment getStreamExecution(ParamsInfo paramsInfo) throws Exception {
<abbr title=" 154          StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo.getDeployMode());"> 154          StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo🔵</abbr>
 155          StreamTableEnvironment tableEnv = getStreamTableEnv(env, paramsInfo.getConfProp());
 156  
 157  
 158          SqlParser.setLocalSqlPluginRoot(paramsInfo.getLocalSqlPluginPath());
 159          SqlTree sqlTree = SqlParser.parseSql(paramsInfo.getSql(), paramsInfo.getPluginLoadMode());
 160  
 161          Map&lt;String, AbstractSideTableInfo&gt; sideTableMap = Maps.newHashMap();
 162          Map&lt;String, Table&gt; registerTableCache = Maps.newHashMap();
 163  
 164          //register udf
 165          ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv);

 166          //register table schema
<abbr title=" 167          Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSqlPluginPath(),"> 167          Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSql🔵</abbr>
<abbr title=" 168                  paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCache);"> 168                  paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCa🔵</abbr>
 169          // cache classPathSets
 170          ExecuteProcessHelper.registerPluginUrlToCachedFile(env, classPathSets);
 171  
<abbr title=" 172          ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(),tableEnv, sqlTree, sideTableMap, registerTableCache);"> 172          ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(),tab🔵</abbr>
 173  
 174          if (env instanceof MyLocalStreamEnvironment) {
 175              ((MyLocalStreamEnvironment) env).setClasspaths(ClassLoaderManager.getClassPath());
 176          }
 177          return env;
 178      }
 179  
 180  
 181      public static List&lt;URL&gt; getExternalJarUrls(String addJarListStr) throws java.io.IOException {
 182          List&lt;URL&gt; jarUrlList = Lists.newArrayList();
 183          if (Strings.isNullOrEmpty(addJarListStr)) {
 184              return jarUrlList;
 185          }
 186  
<abbr title=" 187          List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name()), List.class);"> 187          List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name🔵</abbr>
 188          //Get External jar to load
 189          for (String addJarPath : addJarFileList) {
 190              jarUrlList.add(new File(addJarPath).toURI().toURL());
 191          }
 192          return jarUrlList;
 193      }
 194  
 195      private static void sqlTranslation(String localSqlPluginPath,
 196                                         String pluginLoadMode,
 197                                         StreamTableEnvironment tableEnv,
 198                                         SqlTree sqlTree,Map&lt;String, AbstractSideTableInfo&gt; sideTableMap,
 199                                         Map&lt;String, Table&gt; registerTableCache) throws Exception {
 200  
 201          SideSqlExec sideSqlExec = new SideSqlExec();
 202          sideSqlExec.setLocalSqlPluginPath(localSqlPluginPath);
 203          sideSqlExec.setPluginLoadMode(pluginLoadMode);
 204  
 205          int scope = 0;
 206          for (CreateTmpTableParser.SqlParserResult result : sqlTree.getTmpSqlList()) {
 207              sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, scope + &quot;&quot;);
 208              scope++;
 209          }
 210  
 211          for (InsertSqlParser.SqlParseResult result : sqlTree.getExecSqlList()) {
 212              if (LOG.isInfoEnabled()) {
 213                  LOG.info(&quot;exe-sql:\n&quot; + result.getExecSql());
 214              }
 215              boolean isSide = false;
 216              for (String tableName : result.getTargetTableList()) {
 217                  if (sqlTree.getTmpTableMap().containsKey(tableName)) {
 218                      CreateTmpTableParser.SqlParserResult tmp = sqlTree.getTmpTableMap().get(tableName);
 219                      String realSql = DtStringUtil.replaceIgnoreQuota(result.getExecSql(), &quot;`&quot;, &quot;&quot;);
 220  
 221                      SqlNode sqlNode = flinkPlanner.getParser().parse(realSql);
 222                      String tmpSql = ((SqlInsert) sqlNode).getSource().toString();
 223                      tmp.setExecSql(tmpSql);
<abbr title=" 224                      sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;);"> 224                      sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;🔵</abbr>
 225                  } else {
 226                      for (String sourceTable : result.getSourceTableList()) {
 227                          if (sideTableMap.containsKey(sourceTable)) {
 228                              isSide = true;
 229                              break;
 230                          }
 231                      }
 232                      if (isSide) {
 233                          //sql-dimensional table contains the dimension table of execution
<abbr title=" 234                          sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, String.valueOf(scope));"> 234                          sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, St🔵</abbr>
 235                      } else {
 236                          LOG.info(&quot;----------exec sql without dimension join-----------&quot;);
 237                          LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getExecSql());
 238                          FlinkSQLExec.sqlUpdate(tableEnv, result.getExecSql());
 239                          if (LOG.isInfoEnabled()) {
 240                              LOG.info(&quot;exec sql: &quot; + result.getExecSql());
 241                          }
 242                      }
 243                  }
 244  
 245                  scope++;
 246              }
 247          }
 248      }
 249  
<abbr title=" 250      public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEnv)"> 250      public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEn🔵</abbr>

 251              throws IllegalAccessException, InvocationTargetException {
 252          // udf和tableEnv须由同一个类加载器加载
 253          ClassLoader levelClassLoader = tableEnv.getClass().getClassLoader();
 254          URLClassLoader classLoader = null;
 255          List&lt;CreateFuncParser.SqlParserResult&gt; funcList = sqlTree.getFunctionList();
 256          for (CreateFuncParser.SqlParserResult funcInfo : funcList) {






 257              //classloader
 258              if (classLoader == null) {
 259                  classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, (URLClassLoader) levelClassLoader);
 260              }
<abbr title=" 261              FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv, classLoader);"> 261              FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv,🔵</abbr>
 262          }
 263      }
 264  
 265      /**
 266       *    向Flink注册源表和结果表，返回执行时插件包的全路径
 267       * @param sqlTree
 268       * @param env
 269       * @param tableEnv
 270       * @param localSqlPluginPath
 271       * @param remoteSqlPluginPath
 272       * @param pluginLoadMode   插件加载模式 classpath or shipfile
 273       * @param sideTableMap
 274       * @param registerTableCache
 275       * @return
 276       * @throws Exception
 277       */
<abbr title=" 278      public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment tableEnv, String localSqlPluginPath,"> 278      public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment t🔵</abbr>
<abbr title=" 279                                           String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSideTableInfo&gt; sideTableMap, Map&lt;String, Table&gt; registerTableCache) throws Exception {"> 279                                           String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSi🔵</abbr>
 280          Set&lt;URL&gt; pluginClassPathSets = Sets.newHashSet();
 281          WaterMarkerAssigner waterMarkerAssigner = new WaterMarkerAssigner();
 282          for (AbstractTableInfo tableInfo : sqlTree.getTableInfoMap().values()) {
 283  
 284              if (tableInfo instanceof AbstractSourceTableInfo) {
 285  
 286                  AbstractSourceTableInfo sourceTableInfo = (AbstractSourceTableInfo) tableInfo;
<abbr title=" 287                  Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPath, pluginLoadMode);"> 287                  Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPa🔵</abbr>
 288                  tableEnv.registerTable(sourceTableInfo.getAdaptName(), table);
<abbr title=" 289                  //Note --- parameter conversion function can not be used inside a function of the type of polymerization"> 289                  //Note --- parameter conversion function can not be used inside a function of the type of polymeri🔵</abbr>
 290                  //Create table in which the function is arranged only need adaptation sql
 291                  String adaptSql = sourceTableInfo.getAdaptSelectSql();
 292                  Table adaptTable = adaptSql == null ? table : tableEnv.sqlQuery(adaptSql);
 293  
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"><abbr title=" 294 -                RowTypeInfo typeInfo = new RowTypeInfo(adaptTable.getSchema().getFieldTypes(), adaptTable.getSchema().getFieldNames());"> 294 -                RowTypeInfo typeInfo = new RowTypeInfo(adaptTable.getSchema().getFieldTypes(), adaptTable.getSchem🔵</abbr></span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"><abbr title=" 295 +                RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().getFieldDataTypes()), adaptTable.getSchema().getFieldNames());"> 295 +                RowTypeInfo typeInfo = new RowTypeInfo(fromDataTypeToLegacyInfo(adaptTable.getSchema().getFieldDat🔵</abbr></span>
 296                  DataStream adaptStream = tableEnv.toAppendStream(adaptTable, typeInfo);
 297  
 298                  String fields = String.join(&quot;,&quot;, typeInfo.getFieldNames());
 299  
 300                  if (waterMarkerAssigner.checkNeedAssignWaterMarker(sourceTableInfo)) {
 301                      adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTableInfo);
 302                      fields += &quot;,ROWTIME.ROWTIME&quot;;
 303                  } else {
 304                      fields += &quot;,PROCTIME.PROCTIME&quot;;
 305                  }
 306  
 307                  Table regTable = tableEnv.fromDataStream(adaptStream, fields);
 308                  tableEnv.registerTable(tableInfo.getName(), regTable);
 309                  if (LOG.isInfoEnabled()) {
 310                      LOG.info(&quot;registe table {} success.&quot;, tableInfo.getName());
 311                  }
 312                  registerTableCache.put(tableInfo.getName(), regTable);
 313  
<abbr title=" 314                  URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractSourceTableInfo.SOURCE_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 314                  URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), Abstract🔵</abbr>
 315                  pluginClassPathSets.add(sourceTablePathUrl);
 316              } else if (tableInfo instanceof AbstractTargetTableInfo) {
 317  
<abbr title=" 318                  TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo, localSqlPluginPath, pluginLoadMode);"> 318                  TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo, localSql🔵</abbr>
 319                  TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses());
 320                  tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableSink);
 321  
<abbr title=" 322                  URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTargetTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 322                  URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTa🔵</abbr>
 323                  pluginClassPathSets.add(sinkTablePathUrl);
 324              } else if (tableInfo instanceof AbstractSideTableInfo) {
<abbr title=" 325                  String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tableInfo).getCacheType()) ? &quot;all&quot; : &quot;async&quot;;"> 325                  String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tableInfo).g🔵</abbr>
 326                  sideTableMap.put(tableInfo.getName(), (AbstractSideTableInfo) tableInfo);
 327  
<abbr title=" 328                  URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, AbstractSideTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 328                  URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, Abstr🔵</abbr>
 329                  pluginClassPathSets.add(sideTablePathUrl);
 330              } else {
 331                  throw new RuntimeException(&quot;not support table type:&quot; + tableInfo.getType());
 332              }
 333          }
 334          if (localSqlPluginPath == null || localSqlPluginPath.isEmpty()) {
 335              return Sets.newHashSet();
 336          }
 337          return pluginClassPathSets;
 338      }
 339  
 340      /**
 341       *   perjob模式将job依赖的插件包路径存储到cacheFile，在外围将插件包路径传递给jobgraph
 342       * @param env
 343       * @param classPathSet
 344       */
 345      public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSet) {
 346          int i = 0;
 347          for (URL url : classPathSet) {
 348              String classFileName = String.format(CLASS_FILE_NAME_FMT, i);
 349              env.registerCachedFile(url.getPath(), classFileName, true);
 350              i++;
 351          }
 352      }
 353  
<abbr title=" 354      public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws Exception {"> 354      public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws 🔵</abbr>
 355          StreamExecutionEnvironment env = !ClusterMode.local.name().equals(deployMode) ?
 356                  StreamExecutionEnvironment.getExecutionEnvironment() :
 357                  new MyLocalStreamEnvironment();
 358  
 359          StreamEnvConfigManager.streamExecutionEnvironmentConfig(env, confProperties);
 360          return env;
 361      }
 362  
 363  
<abbr title=" 364      public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperties) {"> 364      public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperti🔵</abbr>
 365          // use blink and streammode
 366          EnvironmentSettings settings = EnvironmentSettings.newInstance()
 367                  .useBlinkPlanner()
 368                  .inStreamingMode()
 369                  .build();
 370  
 371          TableConfig tableConfig = new TableConfig();
 372  
 373          timeZoneCheck(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()));
 374  
<abbr title=" 375          tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID())));"> 375          tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()🔵</abbr>
 376  
 377          StreamTableEnvironment tableEnv = StreamTableEnvironmentImpl.create(env, settings, tableConfig);
 378          StreamEnvConfigManager.streamTableEnvironmentStateTTLConfig(tableEnv, confProperties);
 379          return tableEnv;
 380      }
 381  
 382      private static void timeZoneCheck(String timeZone) {
 383          ArrayList&lt;String&gt; zones = Lists.newArrayList(TimeZone.getAvailableIDs());
 384          if (!zones.contains(timeZone)){
 385              throw new IllegalArgumentException(String.format(&quot; timezone of %s is Incorrect!&quot;, timeZone));
 386          }
 387      }
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 388 +</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 389 +    private static TypeInformation&lt;?&gt;[] fromDataTypeToLegacyInfo(DataType[] dataType) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 390 +        return Stream.of(dataType)</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 391 +                .map(TypeInfoDataTypeConverter::toLegacyTypeInfo)</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 392 +                .toArray(TypeInformation[]::new);</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 393 +    }</span>
 394  }</pre></td>
                            <td><pre>   1  /*
   2   * Licensed to the Apache Software Foundation (ASF) under one
   3   * or more contributor license agreements.  See the NOTICE file
   4   * distributed with this work for additional information
   5   * regarding copyright ownership.  The ASF licenses this file
   6   * to you under the Apache License, Version 2.0 (the
   7   * &quot;License&quot;); you may not use this file except in compliance
   8   * with the License.  You may obtain a copy of the License at
   9   *
  10   *     http://www.apache.org/licenses/LICENSE-2.0
  11   *
  12   * Unless required by applicable law or agreed to in writing, software
  13   * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
  14   * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  15   * See the License for the specific language governing permissions and
  16   * limitations under the License.
  17   */
  18  
  19  package com.dtstack.flink.sql.exec;
  20  
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  21 -import com.dtstack.flink.sql.parser.CreateFuncParser;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  22 -import com.dtstack.flink.sql.parser.CreateTmpTableParser;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  23 -import com.dtstack.flink.sql.parser.FlinkPlanner;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  24 -import com.dtstack.flink.sql.parser.InsertSqlParser;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  25 -import com.dtstack.flink.sql.parser.SqlParser;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  26 -import com.dtstack.flink.sql.parser.SqlTree;</span>

<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  27 -import org.apache.flink.api.common.typeinfo.TypeInformation;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  28 -import org.apache.flink.api.java.typeutils.RowTypeInfo;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  29 -import org.apache.flink.streaming.api.datastream.DataStream;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  30 -import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  31 -import org.apache.flink.table.api.*;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  32 -import org.apache.flink.table.api.java.StreamTableEnvironment;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  33 -import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  34 -import org.apache.flink.table.sinks.TableSink;</span>
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  35 -</span>
  36  import com.dtstack.flink.sql.classloader.ClassLoaderManager;
  37  import com.dtstack.flink.sql.enums.ClusterMode;
  38  import com.dtstack.flink.sql.enums.ECacheType;
  39  import com.dtstack.flink.sql.enums.EPluginLoadMode;
  40  import com.dtstack.flink.sql.environment.MyLocalStreamEnvironment;
  41  import com.dtstack.flink.sql.environment.StreamEnvConfigManager;
  42  import com.dtstack.flink.sql.function.FunctionManager;
  43  import com.dtstack.flink.sql.option.OptionParser;
  44  import com.dtstack.flink.sql.option.Options;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  45 +import com.dtstack.flink.sql.parser.CreateFuncParser;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  46 +import com.dtstack.flink.sql.parser.CreateTmpTableParser;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  47 +import com.dtstack.flink.sql.parser.FlinkPlanner;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  48 +import com.dtstack.flink.sql.parser.InsertSqlParser;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  49 +import com.dtstack.flink.sql.parser.SqlParser;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  50 +import com.dtstack.flink.sql.parser.SqlTree;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  51 +import com.dtstack.flink.sql.side.AbstractSideTableInfo;</span>
  52  import com.dtstack.flink.sql.side.SideSqlExec;
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  53 -import com.dtstack.flink.sql.side.AbstractSideTableInfo;</span>
  54  import com.dtstack.flink.sql.sink.StreamSinkFactory;
  55  import com.dtstack.flink.sql.source.StreamSourceFactory;
  56  import com.dtstack.flink.sql.table.AbstractSourceTableInfo;
  57  import com.dtstack.flink.sql.table.AbstractTableInfo;
  58  import com.dtstack.flink.sql.table.AbstractTargetTableInfo;
  59  import com.dtstack.flink.sql.util.DtStringUtil;
  60  import com.dtstack.flink.sql.util.PluginUtil;
  61  import com.dtstack.flink.sql.watermarker.WaterMarkerAssigner;
  62  import com.fasterxml.jackson.databind.ObjectMapper;
  63  import com.google.common.base.Preconditions;
  64  import com.google.common.base.Strings;
  65  import com.google.common.collect.Lists;
  66  import com.google.common.collect.Maps;
  67  import com.google.common.collect.Sets;
  68  import org.apache.calcite.sql.SqlInsert;
  69  import org.apache.calcite.sql.SqlNode;
  70  import org.apache.commons.io.Charsets;
  71  import org.apache.commons.lang3.StringUtils;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  72 +import org.apache.flink.api.common.typeinfo.TypeInformation;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  73 +import org.apache.flink.api.java.typeutils.RowTypeInfo;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  74 +import org.apache.flink.streaming.api.datastream.DataStream;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  75 +import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  76 +import org.apache.flink.table.api.EnvironmentSettings;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  77 +import org.apache.flink.table.api.Table;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  78 +import org.apache.flink.table.api.TableConfig;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  79 +import org.apache.flink.table.api.TableEnvironment;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  80 +import org.apache.flink.table.api.java.StreamTableEnvironment;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  81 +import org.apache.flink.table.api.java.internal.StreamTableEnvironmentImpl;</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  82 +import org.apache.flink.table.sinks.TableSink;</span>
  83  import org.slf4j.Logger;
  84  import org.slf4j.LoggerFactory;
  85  
  86  import java.io.File;
  87  import java.lang.reflect.InvocationTargetException;
  88  import java.net.URL;
  89  import java.net.URLClassLoader;
  90  import java.net.URLDecoder;
  91  import java.time.ZoneId;
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0">  92 +import java.util.ArrayList;</span>
  93  import java.util.Arrays;
  94  import java.util.List;
  95  import java.util.Map;
  96  import java.util.Properties;
  97  import java.util.Set;
  98  import java.util.TimeZone;
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0">  99 -import java.util.ArrayList;</span>

 100  
 101  /**
 102   *  任务执行时的流程方法
 103   * Date: 2020/2/17
 104   * Company: www.dtstack.com
 105   * @author maqi
 106   */
 107  public class ExecuteProcessHelper {
 108  
 109      private static final String CLASS_FILE_NAME_FMT = &quot;class_path_%d&quot;;
 110      private static final Logger LOG = LoggerFactory.getLogger(ExecuteProcessHelper.class);
 111      private static final ObjectMapper OBJECT_MAPPER = new ObjectMapper();
 112  
 113      private static final String TIME_ZONE = &quot;timezone&quot;;
 114  
 115      public static FlinkPlanner flinkPlanner = new FlinkPlanner();
 116  
 117      public static ParamsInfo parseParams(String[] args) throws Exception {
 118          LOG.info(&quot;------------program params-------------------------&quot;);
 119          Arrays.stream(args).forEach(arg -&gt; LOG.info(&quot;{}&quot;, arg));
 120          LOG.info(&quot;-------------------------------------------&quot;);
 121  
 122          OptionParser optionParser = new OptionParser(args);
 123          Options options = optionParser.getOptions();
 124  
 125          String sql = URLDecoder.decode(options.getSql(), Charsets.UTF_8.name());
 126          String name = options.getName();
 127          String localSqlPluginPath = options.getLocalSqlPluginPath();
 128          String remoteSqlPluginPath = options.getRemoteSqlPluginPath();
 129          String pluginLoadMode = options.getPluginLoadMode();
 130          String deployMode = options.getMode();
 131  
 132          Preconditions.checkArgument(checkRemoteSqlPluginPath(remoteSqlPluginPath, deployMode, pluginLoadMode),
 133                  &quot;Non-local mode or shipfile deployment mode, remoteSqlPluginPath is required&quot;);
 134          String confProp = URLDecoder.decode(options.getConfProp(), Charsets.UTF_8.toString());
 135          Properties confProperties = PluginUtil.jsonStrToObject(confProp, Properties.class);
 136  
 137          List&lt;URL&gt; jarUrlList = getExternalJarUrls(options.getAddjar());
 138  
 139          return ParamsInfo.builder()
 140                  .setSql(sql)
 141                  .setName(name)
 142                  .setLocalSqlPluginPath(localSqlPluginPath)
 143                  .setRemoteSqlPluginPath(remoteSqlPluginPath)
 144                  .setPluginLoadMode(pluginLoadMode)
 145                  .setDeployMode(deployMode)
 146                  .setConfProp(confProperties)
 147                  .setJarUrlList(jarUrlList)
 148                  .build();
 149  
 150      }
 151  
 152      /**
 153       *   非local模式或者shipfile部署模式，remoteSqlPluginPath必填
 154       * @param remoteSqlPluginPath
 155       * @param deployMode
 156       * @param pluginLoadMode
 157       * @return
 158       */
<abbr title=" 159      public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoadMode) {"> 159      public static boolean checkRemoteSqlPluginPath(String remoteSqlPluginPath, String deployMode, String pluginLoa🔵</abbr>
 160          if (StringUtils.isEmpty(remoteSqlPluginPath)) {
 161              return StringUtils.equalsIgnoreCase(pluginLoadMode, EPluginLoadMode.SHIPFILE.name())
 162                      || StringUtils.equalsIgnoreCase(deployMode, ClusterMode.local.name());
 163          }
 164          return true;
 165      }
 166  
 167  
 168      public static StreamExecutionEnvironment getStreamExecution(ParamsInfo paramsInfo) throws Exception {
<abbr title=" 169          StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo.getDeployMode());"> 169          StreamExecutionEnvironment env = ExecuteProcessHelper.getStreamExeEnv(paramsInfo.getConfProp(), paramsInfo🔵</abbr>
 170          StreamTableEnvironment tableEnv = getStreamTableEnv(env, paramsInfo.getConfProp());
 171  
 172  
 173          SqlParser.setLocalSqlPluginRoot(paramsInfo.getLocalSqlPluginPath());
 174          SqlTree sqlTree = SqlParser.parseSql(paramsInfo.getSql(), paramsInfo.getPluginLoadMode());
 175  
 176          Map&lt;String, AbstractSideTableInfo&gt; sideTableMap = Maps.newHashMap();
 177          Map&lt;String, Table&gt; registerTableCache = Maps.newHashMap();
 178  
 179          //register udf
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"> 180 -        ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv);</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"><abbr title=" 181 +        ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, paramsInfo.isGetPlan());"> 181 +        ExecuteProcessHelper.registerUserDefinedFunction(sqlTree, paramsInfo.getJarUrlList(), tableEnv, paramsInfo🔵</abbr></span>
 182          //register table schema
<abbr title=" 183          Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSqlPluginPath(),"> 183          Set&lt;URL&gt; classPathSets = ExecuteProcessHelper.registerTable(sqlTree, env, tableEnv, paramsInfo.getLocalSql🔵</abbr>
<abbr title=" 184                  paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCache);"> 184                  paramsInfo.getRemoteSqlPluginPath(), paramsInfo.getPluginLoadMode(), sideTableMap, registerTableCa🔵</abbr>
 185          // cache classPathSets
 186          ExecuteProcessHelper.registerPluginUrlToCachedFile(env, classPathSets);
 187  
<abbr title=" 188          ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(),tableEnv, sqlTree, sideTableMap, registerTableCache);"> 188          ExecuteProcessHelper.sqlTranslation(paramsInfo.getLocalSqlPluginPath(), paramsInfo.getPluginLoadMode(),tab🔵</abbr>
 189  
 190          if (env instanceof MyLocalStreamEnvironment) {
 191              ((MyLocalStreamEnvironment) env).setClasspaths(ClassLoaderManager.getClassPath());
 192          }
 193          return env;
 194      }
 195  
 196  
 197      public static List&lt;URL&gt; getExternalJarUrls(String addJarListStr) throws java.io.IOException {
 198          List&lt;URL&gt; jarUrlList = Lists.newArrayList();
 199          if (Strings.isNullOrEmpty(addJarListStr)) {
 200              return jarUrlList;
 201          }
 202  
<abbr title=" 203          List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name()), List.class);"> 203          List&lt;String&gt; addJarFileList = OBJECT_MAPPER.readValue(URLDecoder.decode(addJarListStr, Charsets.UTF_8.name🔵</abbr>
 204          //Get External jar to load
 205          for (String addJarPath : addJarFileList) {
 206              jarUrlList.add(new File(addJarPath).toURI().toURL());
 207          }
 208          return jarUrlList;
 209      }
 210  
 211      private static void sqlTranslation(String localSqlPluginPath,
 212                                         String pluginLoadMode,
 213                                         StreamTableEnvironment tableEnv,
 214                                         SqlTree sqlTree,Map&lt;String, AbstractSideTableInfo&gt; sideTableMap,
 215                                         Map&lt;String, Table&gt; registerTableCache) throws Exception {
 216  
 217          SideSqlExec sideSqlExec = new SideSqlExec();
 218          sideSqlExec.setLocalSqlPluginPath(localSqlPluginPath);
 219          sideSqlExec.setPluginLoadMode(pluginLoadMode);
 220  
 221          int scope = 0;
 222          for (CreateTmpTableParser.SqlParserResult result : sqlTree.getTmpSqlList()) {
 223              sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, result, scope + &quot;&quot;);
 224              scope++;
 225          }
 226  
 227          for (InsertSqlParser.SqlParseResult result : sqlTree.getExecSqlList()) {
 228              if (LOG.isInfoEnabled()) {
 229                  LOG.info(&quot;exe-sql:\n&quot; + result.getExecSql());
 230              }
 231              boolean isSide = false;
 232              for (String tableName : result.getTargetTableList()) {
 233                  if (sqlTree.getTmpTableMap().containsKey(tableName)) {
 234                      CreateTmpTableParser.SqlParserResult tmp = sqlTree.getTmpTableMap().get(tableName);
 235                      String realSql = DtStringUtil.replaceIgnoreQuota(result.getExecSql(), &quot;`&quot;, &quot;&quot;);
 236  
 237                      SqlNode sqlNode = flinkPlanner.getParser().parse(realSql);
 238                      String tmpSql = ((SqlInsert) sqlNode).getSource().toString();
 239                      tmp.setExecSql(tmpSql);
<abbr title=" 240                      sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;);"> 240                      sideSqlExec.exec(tmp.getExecSql(), sideTableMap, tableEnv, registerTableCache, tmp, scope + &quot;&quot;🔵</abbr>
 241                  } else {
 242                      for (String sourceTable : result.getSourceTableList()) {
 243                          if (sideTableMap.containsKey(sourceTable)) {
 244                              isSide = true;
 245                              break;
 246                          }
 247                      }
 248                      if (isSide) {
 249                          //sql-dimensional table contains the dimension table of execution
<abbr title=" 250                          sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, String.valueOf(scope));"> 250                          sideSqlExec.exec(result.getExecSql(), sideTableMap, tableEnv, registerTableCache, null, St🔵</abbr>
 251                      } else {
 252                          LOG.info(&quot;----------exec sql without dimension join-----------&quot;);
 253                          LOG.info(&quot;----------real sql exec is--------------------------\n{}&quot;, result.getExecSql());
 254                          FlinkSQLExec.sqlUpdate(tableEnv, result.getExecSql());
 255                          if (LOG.isInfoEnabled()) {
 256                              LOG.info(&quot;exec sql: &quot; + result.getExecSql());
 257                          }
 258                      }
 259                  }
 260  
 261                  scope++;
 262              }
 263          }
 264      }
 265  
<span style="background-color: rgba(255, 0, 0, 0.2);; margin: 0"><abbr title=" 266 -    public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEnv)"> 266 -    public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEn🔵</abbr></span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"><abbr title=" 267 +    public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEnv, boolean getPlan)"> 267 +    public static void registerUserDefinedFunction(SqlTree sqlTree, List&lt;URL&gt; jarUrlList, TableEnvironment tableEn🔵</abbr></span>
 268              throws IllegalAccessException, InvocationTargetException {
 269          // udf和tableEnv须由同一个类加载器加载
 270          ClassLoader levelClassLoader = tableEnv.getClass().getClassLoader();
 271          URLClassLoader classLoader = null;
 272          List&lt;CreateFuncParser.SqlParserResult&gt; funcList = sqlTree.getFunctionList();
 273          for (CreateFuncParser.SqlParserResult funcInfo : funcList) {
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 274 +            // 构建plan的情况下，udf和tableEnv不需要是同一个类加载器</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 275 +            if (getPlan) {</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 276 +                URL[] urls = jarUrlList.toArray(new URL[0]);</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 277 +                classLoader = URLClassLoader.newInstance(urls);</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 278 +            }</span>
<span style="background-color: rgba(0, 255, 0, 0.2); margin: 0"> 279 +</span>
 280              //classloader
 281              if (classLoader == null) {
 282                  classLoader = ClassLoaderManager.loadExtraJar(jarUrlList, (URLClassLoader) levelClassLoader);
 283              }
<abbr title=" 284              FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv, classLoader);"> 284              FunctionManager.registerUDF(funcInfo.getType(), funcInfo.getClassName(), funcInfo.getName(), tableEnv,🔵</abbr>
 285          }
 286      }
 287  
 288      /**
 289       *    向Flink注册源表和结果表，返回执行时插件包的全路径
 290       * @param sqlTree
 291       * @param env
 292       * @param tableEnv
 293       * @param localSqlPluginPath
 294       * @param remoteSqlPluginPath
 295       * @param pluginLoadMode   插件加载模式 classpath or shipfile
 296       * @param sideTableMap
 297       * @param registerTableCache
 298       * @return
 299       * @throws Exception
 300       */
<abbr title=" 301      public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment tableEnv, String localSqlPluginPath,"> 301      public static Set&lt;URL&gt; registerTable(SqlTree sqlTree, StreamExecutionEnvironment env, StreamTableEnvironment t🔵</abbr>
<abbr title=" 302                                           String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSideTableInfo&gt; sideTableMap, Map&lt;String, Table&gt; registerTableCache) throws Exception {"> 302                                           String remoteSqlPluginPath, String pluginLoadMode, Map&lt;String, AbstractSi🔵</abbr>
 303          Set&lt;URL&gt; pluginClassPathSets = Sets.newHashSet();
 304          WaterMarkerAssigner waterMarkerAssigner = new WaterMarkerAssigner();
 305          for (AbstractTableInfo tableInfo : sqlTree.getTableInfoMap().values()) {
 306  
 307              if (tableInfo instanceof AbstractSourceTableInfo) {
 308  
 309                  AbstractSourceTableInfo sourceTableInfo = (AbstractSourceTableInfo) tableInfo;
<abbr title=" 310                  Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPath, pluginLoadMode);"> 310                  Table table = StreamSourceFactory.getStreamSource(sourceTableInfo, env, tableEnv, localSqlPluginPa🔵</abbr>
 311                  tableEnv.registerTable(sourceTableInfo.getAdaptName(), table);
<abbr title=" 312                  //Note --- parameter conversion function can not be used inside a function of the type of polymerization"> 312                  //Note --- parameter conversion function can not be used inside a function of the type of polymeri🔵</abbr>
 313                  //Create table in which the function is arranged only need adaptation sql
 314                  String adaptSql = sourceTableInfo.getAdaptSelectSql();
 315                  Table adaptTable = adaptSql == null ? table : tableEnv.sqlQuery(adaptSql);
 316  
<abbr title=" 317                  RowTypeInfo typeInfo = new RowTypeInfo(adaptTable.getSchema().getFieldTypes(), adaptTable.getSchema().getFieldNames());"> 317                  RowTypeInfo typeInfo = new RowTypeInfo(adaptTable.getSchema().getFieldTypes(), adaptTable.getSchem🔵</abbr>

 318                  DataStream adaptStream = tableEnv.toAppendStream(adaptTable, typeInfo);
 319  
 320                  String fields = String.join(&quot;,&quot;, typeInfo.getFieldNames());
 321  
 322                  if (waterMarkerAssigner.checkNeedAssignWaterMarker(sourceTableInfo)) {
 323                      adaptStream = waterMarkerAssigner.assignWaterMarker(adaptStream, typeInfo, sourceTableInfo);
 324                      fields += &quot;,ROWTIME.ROWTIME&quot;;
 325                  } else {
 326                      fields += &quot;,PROCTIME.PROCTIME&quot;;
 327                  }
 328  
 329                  Table regTable = tableEnv.fromDataStream(adaptStream, fields);
 330                  tableEnv.registerTable(tableInfo.getName(), regTable);
 331                  if (LOG.isInfoEnabled()) {
 332                      LOG.info(&quot;registe table {} success.&quot;, tableInfo.getName());
 333                  }
 334                  registerTableCache.put(tableInfo.getName(), regTable);
 335  
<abbr title=" 336                  URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractSourceTableInfo.SOURCE_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 336                  URL sourceTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), Abstract🔵</abbr>
 337                  pluginClassPathSets.add(sourceTablePathUrl);
 338              } else if (tableInfo instanceof AbstractTargetTableInfo) {
 339  
<abbr title=" 340                  TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo, localSqlPluginPath, pluginLoadMode);"> 340                  TableSink tableSink = StreamSinkFactory.getTableSink((AbstractTargetTableInfo) tableInfo, localSql🔵</abbr>
 341                  TypeInformation[] flinkTypes = FunctionManager.transformTypes(tableInfo.getFieldClasses());
 342                  tableEnv.registerTableSink(tableInfo.getName(), tableInfo.getFields(), flinkTypes, tableSink);
 343  
<abbr title=" 344                  URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTargetTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 344                  URL sinkTablePathUrl = PluginUtil.buildSourceAndSinkPathByLoadMode(tableInfo.getType(), AbstractTa🔵</abbr>
 345                  pluginClassPathSets.add(sinkTablePathUrl);
 346              } else if (tableInfo instanceof AbstractSideTableInfo) {
<abbr title=" 347                  String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tableInfo).getCacheType()) ? &quot;all&quot; : &quot;async&quot;;"> 347                  String sideOperator = ECacheType.ALL.name().equalsIgnoreCase(((AbstractSideTableInfo) tableInfo).g🔵</abbr>
 348                  sideTableMap.put(tableInfo.getName(), (AbstractSideTableInfo) tableInfo);
 349  
<abbr title=" 350                  URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, AbstractSideTableInfo.TARGET_SUFFIX, localSqlPluginPath, remoteSqlPluginPath, pluginLoadMode);"> 350                  URL sideTablePathUrl = PluginUtil.buildSidePathByLoadMode(tableInfo.getType(), sideOperator, Abstr🔵</abbr>
 351                  pluginClassPathSets.add(sideTablePathUrl);
 352              } else {
 353                  throw new RuntimeException(&quot;not support table type:&quot; + tableInfo.getType());
 354              }
 355          }
 356          if (localSqlPluginPath == null || localSqlPluginPath.isEmpty()) {
 357              return Sets.newHashSet();
 358          }
 359          return pluginClassPathSets;
 360      }
 361  
 362      /**
 363       *   perjob模式将job依赖的插件包路径存储到cacheFile，在外围将插件包路径传递给jobgraph
 364       * @param env
 365       * @param classPathSet
 366       */
 367      public static void registerPluginUrlToCachedFile(StreamExecutionEnvironment env, Set&lt;URL&gt; classPathSet) {
 368          int i = 0;
 369          for (URL url : classPathSet) {
 370              String classFileName = String.format(CLASS_FILE_NAME_FMT, i);
 371              env.registerCachedFile(url.getPath(), classFileName, true);
 372              i++;
 373          }
 374      }
 375  
<abbr title=" 376      public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws Exception {"> 376      public static StreamExecutionEnvironment getStreamExeEnv(Properties confProperties, String deployMode) throws 🔵</abbr>
 377          StreamExecutionEnvironment env = !ClusterMode.local.name().equals(deployMode) ?
 378                  StreamExecutionEnvironment.getExecutionEnvironment() :
 379                  new MyLocalStreamEnvironment();
 380  
 381          StreamEnvConfigManager.streamExecutionEnvironmentConfig(env, confProperties);
 382          return env;
 383      }
 384  
 385  
<abbr title=" 386      public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperties) {"> 386      public static StreamTableEnvironment getStreamTableEnv(StreamExecutionEnvironment env, Properties confProperti🔵</abbr>
 387          // use blink and streammode
 388          EnvironmentSettings settings = EnvironmentSettings.newInstance()
 389                  .useBlinkPlanner()
 390                  .inStreamingMode()
 391                  .build();
 392  
 393          TableConfig tableConfig = new TableConfig();
 394  
 395          timeZoneCheck(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()));
 396  
<abbr title=" 397          tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID())));"> 397          tableConfig.setLocalTimeZone(ZoneId.of(confProperties.getProperty(TIME_ZONE, TimeZone.getDefault().getID()🔵</abbr>
 398  
 399          StreamTableEnvironment tableEnv = StreamTableEnvironmentImpl.create(env, settings, tableConfig);
 400          StreamEnvConfigManager.streamTableEnvironmentStateTTLConfig(tableEnv, confProperties);
 401          return tableEnv;
 402      }
 403  
 404      private static void timeZoneCheck(String timeZone) {
 405          ArrayList&lt;String&gt; zones = Lists.newArrayList(TimeZone.getAvailableIDs());
 406          if (!zones.contains(timeZone)){
 407              throw new IllegalArgumentException(String.format(&quot; timezone of %s is Incorrect!&quot;, timeZone));
 408          }
 409      }






 410  }</pre></td>
                        </tr>
                    </table>
                </div>
              </body>
            </html>
            